---
title: "하둡 생태계"
tags: ["hadoop"]
date_created: "2021-03-27"
date_modified: "2021-06-18"
---

하둡 생태계(Hadoop Environment)에 대해 알아보자.

참고로 이 글은 책 "하둡 완벽 가이드(4판, 한빛미디어, 톰 화이트 지음, 장형석/임상배/김훈동 옮김)" 내용을 정리한 것이다.

# 하둡 기초

- 목표 : 빅데이터 분석
- 문제점 : 디스크 용량은 매우 커졌지만, 데이터를 읽고 쓰는 시간은 매우 오래 걸림
  - 디스크 하나의 용량은 TB급까지 올라왔지만, 데이터 전송 속도는 100MB/s 수준
    - 전체 데이터를 다 읽으려면 몇 시간이 걸린다(쓰는 것은 심지어 더 느리다).
- 해결책 : 여러 개의 디스크(시스템)를 병렬적으로 사용하는 새로운 시스템을 만든다면 효율적으로 데이터를 읽고 쓸 수 있을 것
  - 즉, 클러스터 위에서 동작하는 새로운 (분산 처리) 시스템이 필요 ⇒ **하둡(Hadoop)**
  - 이 시스템은 다음과 같은 일들에 잘 대처할 수 있어야 한다.
    - 하드웨어 장애 : 여러 시스템이 동시에 동작하므로 항상 하드웨어 문제가 발생할 수 있다. 새로운 시스템은 이를 잘 다룰 수 있어야 한다. ⇒ **HDFS**
    - 데이터 병합 : 데이터를 분석하려면 어느 시점에서는 병렬적으로 수행된 쿼리 결과(데이터)를 병합해야 한다. 이때 정합성 등이 잘 지켜져야 한다. ⇒ **맵리듀스(MapReduce)**

즉 정리하자면 하둡은 **안정적이고 확장성이 높은 저장 및 분석 플렛폼**이다. 

# 맵리듀스(MapReduce)

- 하둡에서 데이터를 처리하기 위해서는 처리 작업을 맵리듀스(MapReduce) 형태로 바꿔줘야 한다.
- 맵리듀스는 **맵(Map) 단계**와 **리듀스(Reduce) 단계**로 구분된다.
- 맵 단계와 리듀스 단계는 입력과 출력으로 키-값 쌍(key-value pair)을 가진다.
  - 키, 값의 타입은 프로그래머가 자유롭게 정할 수 있다.

- **잡(Job)** : 클라이언트가 수행하는 작업의 기본 단위
  - 입력 데이터 + 맵리듀스 프로그램 + 설정 정보
  - **맵 태스크(map task)** + **리듀스 태스크(reduce task)**
- 하둡은 YARN을 이용해 각 태스크를 스캐줄링하고 클러스터의 여러 노드에서 실행한다.
  - 만약 특정 노드의 태스크 하나가 실패하면 자동으로 다른 노드를 재할당하여 다시 실행 ⇒ 프로그래머는 실패에 대해 고민하지 않아도 됨
- 하둡은 맵리듀스 잡의 입력을 **입력 스플릿(input split)** 또는 단순히 **스플릿(split)**이라 부르는 고정 크기 조각으로 분리
  - 각 스플릿마다 하나의 맵 태스크가 생성됨
  - (사용자가 정의한) 맵 함수는 스플릿의 각 레코드(record)를 처리
  - 스플릿의 크기가 작을수록 부하분산(load balance)에 더 효과적. 그러나 스플릿 크기가 너무 작으면 스플릿 관리와 맵 태스크 생성을 위한 오버헤드 때문에 잡의 실행 시간이 증가하는 단점이 있다. ⇒ 일반적으로 HDFS 블록의 기본 크기(128MB)로 설정
    - 만약 하나의 스플릿이 두 개의 HDFS 블록에 걸쳐 저장되어 있다면, 이 두 블록을 모두 저장하는 HDFS 노드는 존재 가능성이 낮다. ⇒ **데이터 지역성(data locality)**을 해침
    - 이 경우 작업을 위해서는 스플릿의 일부 데이터를 네트워크를 통해 맵 태스크가 실행되는 다른 노드로 전송해야 함
    - 따라서 스플릿의 크기는 단일 노드에 저장된다고 확신할 수 있는 가장 큰 블록의 크기인 HDFS 블록의 기본 크기와 같게 하는 것이 일반적이다.
- **데이터 지역성 최적화(data locality optimization)** : 입력 스플릿이 있는 노드에서 맵 태스크를 실행하는 것
  - 로컬에서 데이터를 가져오므로 가장 빠르게 동작함
  - 클러스터의 중요한 공유 자원인 네트워크 대역폭을 사용하지 않음
  - 하둡은 되도록이면 데이터 지역성 최적화가 지켜지도록 맵 태스크를 분배함
  - 만약 입력 스플릿에 해당하는 HDFS 블록 복제본이 저장된 모든 노드들이 다른 맵 태스크를 수행중이라면(즉, 데이터 지역성을 위한 가용 슬롯이 없다면), 잡 스케줄러는 블록 복제본이 저장된 동일 랙에 속한 다른 노드에서 가용한 맵 슬롯을 찾는다. 만약 이에도 실패하면 잡 스케줄러는 데이터 노드가 저장되어 있지 않은 제 3의 랙의 노드에 작업을 할당한다. 이 경우 데이터를 전송해야 하므로 랙 간 네트워크 대역폭을 사용하게 된다.
- 맵 태스크의 결과는 HDFS가 아닌 로컬 디스크에 저장됨
  - 맵의 결과는 리듀스의 최종 결과를 생성하기 위한 중간 결과물이기 때문
  - 잡이 완료되면 맵의 결과는 버려짐
- 리듀스 태스크는 일반적으로 모든 매퍼의 출력 결과를 입력으로 받음
  - 즉, 데이터 지역성이 없음
  - 맵의 모든 결과는 네트워크를 통해 리듀스 태스크가 실행 중인 노드로 전송됨
  - 전송된 데이터는 병합되어 (사용자가 정의한) 리듀스 함수로 전달된다.
- 리듀스 태스크의 결과는 HDFS에 저장됨
  - 안정성을 위해 첫 번째 복사본은 로컬 노드에, 나머지 복제본은 외부 노드에 저장됨
  - 즉, 리듀스 태스크의 결과를 저장하는 것은 네트워크 대역폭을 소모하지만, 일반적인 HDFS 쓰기 파이프라인에 소모되는 대역폭과 비슷한 수준
- 리듀스가 여럿이면 맵 태스크는 리듀스 수만큼 **파티션(partition)**을 생성
  - 맵의 결과는 각 파티션에 분배됨
  - 한 키(와 그 값)는 오직 한 파티션에만 배치된다.
  - 사용자 정의 파티셔닝 함수를 직접 구현할 수도 있지만, 일반적으로 해시 함수로 키를 분배하는 기본 파티셔너(partitioner)를 주로 사용한다. (매우 잘 작동한다)
- **컴바이너 함수(combiner function)**
  - 맵과 리듀스 사이에 위치하는 함수
  - 맵의 결과를 입력으로 받아 이를 처리
  - 컴바이너 함수의 출력값은 리듀스 함수의 입력값이 된다.
  - 컴바이너 함수를 적절히 잘 이용하면 맵과 리듀스 태스크 사이의 데이터 전송을 최소화할 수 있다. ⇒ 네트워크 대역폭 절약
  - 컴바이너 함수를 사용하든 사용하지 않든 리듀스의 결과는 언제나 동일해야 한다.
    - 즉, 가환성(commutative) 및 결합성(associative)이 있는 함수만 컴바이너 함수로 사용할 수 있다.
    - ex) sum은 가능해도 mean은 가능하지 않다.
- **하둡 스트리밍(hadoop streaming)** : 하둡과 사용자 프로그램 사이의 인터페이스
  - 유닉스 표준 스트림을 사용 ⇒ 사용자는 표준 입력을 읽고 표준 출력으로 쓸 수 있는 다양한 언어를 이용하여 맵리듀스 프로그램을 작성할 수 있다.
  - 맵 함수의 입력 데이터는 표준 입력으로 맵에 전달되고, 행 단위로 처리된다.
  - 맵 함수의 출력 키-값 쌍은 탭으로 구분된 하나의 행으로 출력된다.
  - 리듀스 함수의 입력 데이터는 표준 입력으로 전달된, (맵 함수의 출력 형식과 같은) 키-값 쌍이다.
  - 리듀스 함수는 표준 입력으로 각 행을 읽고, 그 결과를 표준 출력에 쓴다.
  - cf) 파이썬 프로그래머는 스트리밍의 대안으로 스트리밍 맵리듀스 인터페이스를 더욱 파이썬답고 사용하기 쉽게 해주는 [Dumbo](http://klbostee.github.io/dumbo/)를 고려할 필요가 있다.[^1]

[^1]: 현재 관리되고 있지는 않은 것 같다.

# 하둡 분산 파일시스템(HDFS)

- 분산 파일시스템(Distributed Filesystem) : 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템
- **하둡 분산 파일시스템(Hadoop Distributed FileSystem)**
  - 하둡의 대표적인 파일시스템

