---
title: "하둡 생태계"
tags: ["hadoop"]
date_created: "2021-06-17"
date_modified: "2021-07-15"
---

하둡 생태계(Hadoop Environment)에 대해 알아보자.

참고로 이 글은 책 "하둡 완벽 가이드(4판, 한빛미디어, 톰 화이트 지음, 장형석/임상배/김훈동 옮김)" 내용을 정리한 것이다.

# 하둡 기초

- 목표 : 빅데이터 분석
- 문제점 : 디스크 용량은 매우 커졌지만, 데이터를 읽고 쓰는 시간은 매우 오래 걸림
  - 디스크 하나의 용량은 TB급까지 올라왔지만, 데이터 전송 속도는 100MB/s 수준
    - 전체 데이터를 다 읽으려면 몇 시간이 걸린다(쓰는 것은 심지어 더 느리다).
- 해결책 : 여러 개의 디스크(시스템)를 병렬적으로 사용하는 새로운 시스템을 만든다면 효율적으로 데이터를 읽고 쓸 수 있을 것
  - 즉, 클러스터 위에서 동작하는 새로운 (분산 처리) 시스템이 필요 ⇒ **하둡(Hadoop)**
  - 이 시스템은 다음과 같은 일들에 잘 대처할 수 있어야 한다.
    - 하드웨어 장애 : 여러 시스템이 동시에 동작하므로 항상 하드웨어 문제가 발생할 수 있다. 새로운 시스템은 이를 잘 다룰 수 있어야 한다. ⇒ **HDFS**
    - 데이터 병합 : 데이터를 분석하려면 어느 시점에서는 병렬적으로 수행된 쿼리 결과(데이터)를 병합해야 한다. 이때 정합성 등이 잘 지켜져야 한다. ⇒ **맵리듀스(MapReduce)**

즉 정리하자면 하둡은 **안정적이고 확장성이 높은 저장 및 분석 플렛폼**이다. 

# 맵리듀스(MapReduce)

- 하둡에서 데이터를 처리하기 위해서는 처리 작업을 맵리듀스(MapReduce) 형태로 바꿔줘야 한다.
- 맵리듀스는 **맵(Map) 단계**와 **리듀스(Reduce) 단계**로 구분된다.
- 맵 단계와 리듀스 단계는 입력과 출력으로 키-값 쌍(key-value pair)을 가진다.
  - 키, 값의 타입은 프로그래머가 자유롭게 정할 수 있다.

- **잡(Job)** : 클라이언트가 수행하는 작업의 기본 단위
  - 입력 데이터 + 맵리듀스 프로그램 + 설정 정보
  - **맵 태스크(map task)** + **리듀스 태스크(reduce task)**
- 하둡은 YARN을 이용해 각 태스크를 스캐줄링하고 클러스터의 여러 노드에서 실행한다.
  - 만약 특정 노드의 태스크 하나가 실패하면 자동으로 다른 노드를 재할당하여 다시 실행 ⇒ 프로그래머는 실패에 대해 고민하지 않아도 됨
- 하둡은 맵리듀스 잡의 입력을 **입력 스플릿(input split)** 또는 단순히 **스플릿(split)**이라 부르는 고정 크기 조각으로 분리
  - 각 스플릿마다 하나의 맵 태스크가 생성됨
  - (사용자가 정의한) 맵 함수는 스플릿의 각 레코드(record)를 처리
  - 스플릿의 크기가 작을수록 부하분산(load balance)에 더 효과적. 그러나 스플릿 크기가 너무 작으면 스플릿 관리와 맵 태스크 생성을 위한 오버헤드 때문에 잡의 실행 시간이 증가하는 단점이 있다. ⇒ 일반적으로 HDFS 블록의 기본 크기(128MB)로 설정
    - 만약 하나의 스플릿이 두 개의 HDFS 블록에 걸쳐 저장되어 있다면, 이 두 블록을 모두 저장하는 HDFS 노드는 존재 가능성이 낮다. ⇒ **데이터 지역성(data locality)**을 해침
    - 이 경우 작업을 위해서는 스플릿의 일부 데이터를 네트워크를 통해 맵 태스크가 실행되는 다른 노드로 전송해야 함
    - 따라서 스플릿의 크기는 단일 노드에 저장된다고 확신할 수 있는 가장 큰 블록의 크기인 HDFS 블록의 기본 크기와 같게 하는 것이 일반적이다.
- **데이터 지역성 최적화(data locality optimization)** : 입력 스플릿이 있는 노드에서 맵 태스크를 실행하는 것
  - 로컬에서 데이터를 가져오므로 가장 빠르게 동작함
  - 클러스터의 중요한 공유 자원인 네트워크 대역폭을 사용하지 않음
  - 하둡은 되도록이면 데이터 지역성 최적화가 지켜지도록 맵 태스크를 분배함
  - 만약 입력 스플릿에 해당하는 HDFS 블록 복제본이 저장된 모든 노드들이 다른 맵 태스크를 수행중이라면(즉, 데이터 지역성을 위한 가용 슬롯이 없다면), 잡 스케줄러는 블록 복제본이 저장된 동일 랙에 속한 다른 노드에서 가용한 맵 슬롯을 찾는다. 만약 이에도 실패하면 잡 스케줄러는 데이터 노드가 저장되어 있지 않은 제 3의 랙의 노드에 작업을 할당한다. 이 경우 데이터를 전송해야 하므로 랙 간 네트워크 대역폭을 사용하게 된다.
- 맵 태스크의 결과는 HDFS가 아닌 로컬 디스크에 저장됨
  - 맵의 결과는 리듀스의 최종 결과를 생성하기 위한 중간 결과물이기 때문
  - 잡이 완료되면 맵의 결과는 버려짐
- 리듀스 태스크는 일반적으로 모든 매퍼의 출력 결과를 입력으로 받음
  - 즉, 데이터 지역성이 없음
  - 맵의 모든 결과는 네트워크를 통해 리듀스 태스크가 실행 중인 노드로 전송됨
  - 전송된 데이터는 병합되어 (사용자가 정의한) 리듀스 함수로 전달된다.
- 리듀스 태스크의 결과는 HDFS에 저장됨
  - 안정성을 위해 첫 번째 복사본은 로컬 노드에, 나머지 복제본은 외부 노드에 저장됨
  - 즉, 리듀스 태스크의 결과를 저장하는 것은 네트워크 대역폭을 소모하지만, 일반적인 HDFS 쓰기 파이프라인에 소모되는 대역폭과 비슷한 수준
- 리듀스가 여럿이면 맵 태스크는 리듀스 수만큼 **파티션(partition)**을 생성
  - 맵의 결과는 각 파티션에 분배됨
  - 한 키(와 그 값)는 오직 한 파티션에만 배치된다.
  - 사용자 정의 파티셔닝 함수를 직접 구현할 수도 있지만, 일반적으로 해시 함수로 키를 분배하는 기본 파티셔너(partitioner)를 주로 사용한다. (매우 잘 작동한다)
- **컴바이너 함수(combiner function)**
  - 맵과 리듀스 사이에 위치하는 함수
  - 맵의 결과를 입력으로 받아 이를 처리
  - 컴바이너 함수의 출력값은 리듀스 함수의 입력값이 된다.
  - 컴바이너 함수를 적절히 잘 이용하면 맵과 리듀스 태스크 사이의 데이터 전송을 최소화할 수 있다. ⇒ 네트워크 대역폭 절약
  - 컴바이너 함수를 사용하든 사용하지 않든 리듀스의 결과는 언제나 동일해야 한다.
    - 즉, 가환성(commutative) 및 결합성(associative)이 있는 함수만 컴바이너 함수로 사용할 수 있다.
    - ex) sum은 가능해도 mean은 가능하지 않다.
- **하둡 스트리밍(hadoop streaming)** : 하둡과 사용자 프로그램 사이의 인터페이스
  - 유닉스 표준 스트림을 사용 ⇒ 사용자는 표준 입력을 읽고 표준 출력으로 쓸 수 있는 다양한 언어를 이용하여 맵리듀스 프로그램을 작성할 수 있다.
  - 맵 함수의 입력 데이터는 표준 입력으로 맵에 전달되고, 행 단위로 처리된다.
  - 맵 함수의 출력 키-값 쌍은 탭으로 구분된 하나의 행으로 출력된다.
  - 리듀스 함수의 입력 데이터는 표준 입력으로 전달된, (맵 함수의 출력 형식과 같은) 키-값 쌍이다.
  - 리듀스 함수는 표준 입력으로 각 행을 읽고, 그 결과를 표준 출력에 쓴다.
  - cf) 파이썬 프로그래머는 스트리밍의 대안으로 스트리밍 맵리듀스 인터페이스를 더욱 파이썬답고 사용하기 쉽게 해주는 [Dumbo](http://klbostee.github.io/dumbo/)를 고려할 필요가 있다.[^1]

[^1]: 현재 관리되고 있지는 않은 것 같다.

# 하둡 분산 파일시스템(HDFS, Hadoop Distributed FileSystem)

- 분산 파일시스템(Distributed Filesystem) : 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템

## HDFS 설계

- HDFS의 설계 특성
  - 매우 큰 파일
    - HDFS는 수백 기가바이트, 수백 테라바이트, 심지어 페타바이트 급의 파일도 잘 다룰 수 있다.
  - 스트리밍 방식의 데이터 접근
    - IDEA : 가장 효율적인 데이터 처리 패턴은 한 번 쓰고 여러 번 읽는 것"
    - 첫 번째 레코드를 읽는 데 걸리는 지연 시간보다 전체 데이터셋을 모두 읽을 때 걸리는 시간이 더 중요
  - 범용 하드웨어
    - HDFS는 노드 장애가 발생할 확률이 높은 범용 하드웨어로 구성된 대형 클러스터에서도 문제없이 작동한다.
- HDFS를 사용하기 적절하지 않은 분야
  - 빠른 데이터 응답 시간
    - HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되어 있음
    - 이를 위해 응답시간을 희생함
    - 만약 빠른 데이터 응답 시간이 필요하다면 HBase를 고려할 수 있다.
  - 수많은 작은 파일
    - 네임노드는 파일시스템의 메타데이터를 메모리에서 관리함 ⇒ HDFS에서 다룰 수 있는 파일 수는 네임노드의 메모리 용량에 좌우됨
    - 수많은 작은 파일을 다뤄야 하면 네임노드의 메모리가 바닥나기 쉽다.
  - 다중 라이터(writer)와 파일의 임의 수정
    - HDFS는 단일 라이터로 파일을 쓴다.
    - 다중 라이터는 지원하지 않음 (하둡 3.0부터는 다중 라이터 지원)
    - 한번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능
    - 파일의 임의 위치에 있는 내용을 수정하는 것은 허용하지 않음

## HDFS 개념

### HDFS 블록

- 기본적으로 128MB
  - 디스크 블록의 크기(기본적으로 512B)보다 큼
  - 탐색 비용을 최소화하기 위함
    - 블록이 크면 블록의 시작점을 탐색하는 데 걸리는 시간을 줄일 수 있고, 데이터를 전송하는데 더 많은 시간을 할애할 수 있다. ⇒ 여러 개의 블록으로 구성된 대용량 파일을 전송하는 시간은 디스크 전송 속도에 영향을 크게 받음
- HDFS의 파일은 HDFS 블록 크기의 덩어리(chunk)로 쪼개져서 저장됨
- HDFS 블록 크기보다 작은 데이터를 저장할 때는 HDFS 블록 크기만큼 디스크를 점유하지는 않는다.
  - ex) 129MB 파일은 128MB 덩어리 하나와 1MB 덩어리로 저장됨. 즉 129MB의 디스크 공간만 사용한다.
- HDFS에 블록 추상화의 개념을 도입하면서 얻게 된 이득
  - 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다.
    - 하나의 파일을 구성하는 여러 개의 블록이 동일한 디스크에만 저장될 필요가 없음
    - 하나의 파일이 여러 클러스터에 나뉘어 저장될 수 있다.
  - 파일 단위보다 블록 단위로 추상화를 하면 스토리지의 서브시스템을 단순하게 만들 수 있다.
    - 장애 유형이 너무나도 다양한 분산 시스템에서 단순함은 중요한 특성
    - 데이터는 HDFS 블록들로, 메타데이터는 별도의 시스템에 분리해 저장할 수 있다.
  - 복제(replication)를 구현하기 쉽다.
    - 내고장성(fault tolerance)과 가용성(availability)을 보장할 수 있음
    - 블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제된다.
    - 만약 한 블록을 이용할 수 없는 상황이 되면 다른 머신에 있는 복사본을 읽음
    - 블록이 손상되거나 머신의 장애로 특정 블록을 더 이상 이용할 수 없으면 또 다른 복사본을 살아 있는 머신에 복제하여 복제 계수(replication factor)를 정상 수준으로 돌아오게 할 수 있다.
    - 특정 블록의 복제 계수를 높게 설정할 수도 있다. ⇒ 읽기 부하를 클러스터 전체에 분산시킬 수 있다.

{:.code-header}
fsck 명령어 : 파일시스템에 있는 각 파일을 구성하는 블록의 목록 출력

{% highlight bash linenos %}
hdfs fsck / -files -blocks
{% endhighlight %}

### 네임노드와 데이터노드

- HDFS 클러스터는 마스터-워크 패턴(master-worker pattern)으로 동작하는, 마스터인 하나의 **네임노드(namenode)**와 워커인 여러 개의 **데이터노드(datanode)**로 구성되어 있다.
- 네임노드
  - HDFS의 네임스페이스를 관리
  - HDFS의 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지
    - 이 정보는 네임스페이스 이미지(namespace image)와 에딧 로그(edit log)라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장된다.
  - 블록의 위치 정보(파일에 속한 모든 블록이 어느 데이터노드에 있는지) 파악
    - 이 정보는 시스템이 시작할 때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장하지 않는다.
  - 네임노드가 없으면 HDFS는 동작하지 않음
    - 네임노드를 실행하는 머신이 손상되면 파일 시스템의 어떤 파일도 찾을 수 없다. ⇒ 네임노드의 장애복구 기능이 아주 중요
- 데이터노드
  - 클라이언트나 네임노드의 요청이 있을 때 블록을 저장, 탐색
  - 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고
- **HDFS 클라이언트** : 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 HDFS에 접근
- 네임노드의 장애복구 메커니즘
  - HDFS 메타데이터를 파일로 백업
  - **보조 네임노드(secondary namenode)** 운영
    - 보조 네임노드의 역할
      - 에딧 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에딧 로그와 병합하여 새로운 네임스페이스 이미지를 만듦
        - 이 작업은 충분한 CPU와 네임노드와 비슷한 용량의 메모리가 필요 ⇒ 주 네임노드(primary namenode)가 아닌 별도의 물리 머신(보조 네임노드)에서 실행하는 것이 좋음
      - 주 네임노드에 장애가 발생할 것을 대비해서 네임스페이스 이미지의 복제본을 보관
        - 그러나 주 네임노드의 네임스페이스 이미지는 약간의 시간차를 두고 보조 네임노드로 복제됨 ⇒ 주 네임노드에 장애가 발생하면 어느 정도의 데이터 손실은 불가피

- 클라이언트는 데이터를 얻기 위해 데이터노드에 직접적으로 접촉
- 네임노드는 각 블록에 적합한 데이터노드를 안내
- 데이터 트래픽은 클러스터에 있는 모든 데이터노드에 고르게 분산됨 ⇒ HDFS는 동시에 실행되는 클라이언트 수를 크게 늘릴 수 있음
- 네임노드는 효율적인 서비스를 위해 메타데이터를 메모리에 저장하고, 단순히 불록의 위치 정보 요청만 처리
  - 데이터를 저장하거나 전송하는 역할은 맡지 않는다. ⇒ 클라이언트가 많아져도 병목현상은 거의 발생하지 않음

- HDFS 파일 권한
  - 읽기(r), 쓰기(w), 실행(x) 권한
    - POSIX와 유사
    - 읽기 권한 : 파일을 읽거나, 디렉토리 내용을 보여줄 때 필요
    - 쓰기 권한 : 파일을 쓰거나, 디렉토리를 만들거나 삭제할 때 필요
    - 실행 권한 : 파일에 대한 실행 권한은 무시됨. 디렉토리에 대한 실행 권한은 하위 디렉토리 접근을 위해 필요

- 하둡은 파일시스템의 추상화 개념을 가지고 있고, HDFS는 그 구현체 중 하나일 뿐

