---
title: "Hadoop 설치하기"
order: 1
date_created: "2021-07-28"
date_modified: "2021-07-29"
---

# 시스템 구성

{:.my-table}
|        c1        |        c2         |      c3       |      c4       |
| :--------------: | :---------------: | :-----------: | :-----------: |
|  192.168.0.101   |   192.168.0.102   | 192.168.0.103 | 192.168.0.104 |
|     NameNode     | SecondaryNameNode |               |               |
|   NodeManager    |     DataNode      |   DataNode    |   DataNode    |
| ResourceManager  |                   |               |               |
| JobHistoryServer |                   |               |               |

<style>
	.my-table {
		width: 100%;
		border-collapse: collapse;
	}
	.my-table th {
		background-color: #DCDCD1;
	}
	.my-table th, .my-table td {
		width: 25%;
		text-align: center;
		border: 1px solid #cccccc;
		padding: 0.5em;
	}

</style>

# 하둡 계정 만들기

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo addgroup hadoop
sudo adduser --ingroup hadoop --gecos "" hadoop
sudo adduser --ingroup hadoop --no-create-home --gecos "" hdfs
sudo adduser --ingroup hadoop --no-create-home --gecos "" mapred
sudo adduser --ingroup hadoop --no-create-home --gecos "" yarn
{% endhighlight %}

# 하둡 다운로드

2021년 07월 현재 최신 Hadoop 버전은 3.3.1임.

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
wget https://mirror.navercorp.com/apache/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
tar -zxf hadoop-3.3.1.tar.gz
sudo mv hadoop-3.3.1/ /usr/local
sudo chown -R hadoop:hadoop /usr/local/hadoop-3.3.1/
echo 'export HADOOP_HOME="/usr/local/hadoop-3.3.1/"' >> ~/.bashrc
source ~/.bashrc
{% endhighlight %}

# Java 설치

하둡 3.3.1 버전과 호환되는 자바 버전은 8임.

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo apt install -y openjdk-8-jre-headless
echo 'export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"' >> ~/.bashrc
source ~/.bashrc
{% endhighlight %}

# 클러스터 SSH 연결 만들기

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo bash -c 'echo "192.168.0.101 c1" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.102 c2" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.103 c3" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.104 c4" >> /etc/hosts'
{% endhighlight %}

{:.code-header}
c1

{% highlight bash %}
ssh-keygen -N '' -f ~/.ssh/hccl-cluster
ssh-copy-id -i ~/.ssh/hccl-cluster hcc@c2
ssh-copy-id -i ~/.ssh/hccl-cluster hcc@c3
ssh-copy-id -i ~/.ssh/hccl-cluster hcc@c4
{% endhighlight %}

# 하둡 환경설정하기

**c1에서만 수행**

{:.code-header}
[c1] ./etc/hadoop/hadoop-env.sh (해당 항목만 변경)

{:.no-prompt}
{% highlight bash %}
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
{% endhighlight %}

{:.code-header}
[c1] ./etc/hadoop/workers (수정)

{:.no-prompt}
{% highlight bash %}
c2
c3
c4
{% endhighlight %}

{:.code-header}
[c1] ./etc/hadoop/core-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://c1:9000</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1] ./etc/hadoop/hdfs-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/hadoop/data/dfs/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/hadoop/data/dfs/datanode</value>
	</property>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>c2:50090</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1] ./etc/hadoop/yarn-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>c1</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1] ./etc/hadoop/mapred-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
  <property>
		<name>yarn.app.mapreduce.am.env</name>
		<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
  <property>
		<name>mapreduce.map.env</name>
		<value>HDAOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
  <property>
		<name>mapreduce.reduce.env</name>
		<value>HDAOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
</configuration>
{% endhighlight %}

# 하둡 실행을 위한 디렉토리 만들기

{:.code-header}
c1

{% highlight bash %}
su hadoop
mkdir -p ~/data/dfs/namenode
{% endhighlight %}

{:.code-header}
c2, c3, c4

{% highlight bash %}
su hadoop
mkdir -p ~/data/dfs/datanode
{% endhighlight %}

# 하둡 실행

{:.code-header}
c1

{% highlight bash %}
su hadoop
mkdir -p ~/data/dfs/namenode
{% endhighlight %}