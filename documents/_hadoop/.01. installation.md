---
title: "Hadoop 설치하기"
order: 1
date_created: "2021-07-28"
date_modified: "2021-09-03"
---

# 시스템 구성

{:.my-table}
|        c1        |        c2         |      c3       |      c4       |
| :--------------: | :---------------: | :-----------: | :-----------: |
|  192.168.0.101   |   192.168.0.102   | 192.168.0.103 | 192.168.0.104 |
|     NameNode     | SecondaryNameNode |               |               |
|   NodeManager    |     DataNode      |   DataNode    |   DataNode    |
| ResourceManager  |                   |               |               |
| JobHistoryServer |                   |               |               |

<style>
	.my-table {
		width: 100%;
		border-collapse: collapse;
	}
	.my-table th {
		background-color: #DCDCD1;
	}
	.my-table th, .my-table td {
		width: 25%;
		text-align: center;
		border: 1px solid #cccccc;
		padding: 0.5em;
	}

</style>

# 하둡 계정 만들기

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo addgroup hadoop
sudo adduser --ingroup hadoop --gecos "" hadoop
sudo adduser --ingroup hadoop --no-create-home --gecos "" hdfs
sudo adduser --ingroup hadoop --no-create-home --gecos "" mapred
sudo adduser --ingroup hadoop --no-create-home --gecos "" yarn
sudo adduser hadoop sudo
{% endhighlight %}

이후 모든 명령은 `hadoop` 계정에서 수행

# 하둡 다운로드

2021년 07월 현재 최신 Hadoop 버전은 3.3.1임.

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
wget https://mirror.navercorp.com/apache/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
tar -zxf hadoop-3.3.1.tar.gz
sudo mv hadoop-3.3.1/ /usr/local
echo 'export HADOOP_HOME="/usr/local/hadoop-3.3.1/"' >> ~/.bashrc
source ~/.bashrc
{% endhighlight %}

# Java 설치

하둡 3.3.1 버전과 호환되는 자바 버전은 8임.

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo apt install -y openjdk-8-jre-headless
echo 'export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"' >> ~/.bashrc
source ~/.bashrc
{% endhighlight %}

# 클러스터 SSH 연결 만들기

{:.code-header}
c1, c2, c3, c4

{% highlight bash %}
sudo bash -c 'echo "192.168.0.101 c1" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.102 c2" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.103 c3" >> /etc/hosts'
sudo bash -c 'echo "192.168.0.104 c4" >> /etc/hosts'
{% endhighlight %}

{:.code-header}
c1

{% highlight bash %}
ssh-keygen -N '' -f ~/.ssh/id_rsa
ssh-copy-id -i ~/.ssh/hccl-cluster hadoop@c2
ssh-copy-id -i ~/.ssh/hccl-cluster hadoop@c3
ssh-copy-id -i ~/.ssh/hccl-cluster hadoop@c4
{% endhighlight %}

{:.code-header}
c2, c3, c4

{% highlight bash %}
chmod 755 ~/.ssh
chmod 644 ~/.ssh/authorized_keys
{% endhighlight %}

Public/Private Key 이름은 반드시 `id_rsa`이어야 함.

# 하둡 환경설정하기

{:.code-header}
[c1, c2, c3, c4] ./etc/hadoop/hadoop-env.sh (해당 항목만 변경)

{:.no-prompt}
{% highlight bash %}
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
{% endhighlight %}

{:.code-header}
[c1, c2] ./etc/hadoop/workers (수정)

{:.no-prompt}
{% highlight bash %}
c2
c3
c4
{% endhighlight %}

{:.code-header}
[c1, c2] ./etc/hadoop/core-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://c1:9000</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1, c2] ./etc/hadoop/hdfs-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/hadoop/data/dfs/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/hadoop/data/dfs/datanode</value>
	</property>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>c2:50090</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1, c2] ./etc/hadoop/yarn-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>c1</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
</configuration>
{% endhighlight %}

{:.code-header}
[c1, c2] ./etc/hadoop/mapred-site.xml (변경)

{% highlight xml %}
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	<property>
		<name>yarn.app.mapreduce.am.env</name>
		<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
	<property>
		<name>mapreduce.map.env</name>
		<value>HDAOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
	<property>
		<name>mapreduce.reduce.env</name>
		<value>HDAOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
</configuration>
{% endhighlight %}

# 하둡 실행을 위한 디렉토리 만들기

{:.code-header}
c1

{% highlight bash %}
mkdir -p ~/data/dfs/namenode
{% endhighlight %}

{:.code-header}
c2, c3, c4

{% highlight bash %}
mkdir -p ~/data/dfs/datanode
{% endhighlight %}

# 하둡 실행

{:.code-header}
c1

{% highlight bash %}
$HADOOP_HOME/bin/hdfs namenode -format -force
$HADOOP_HOME/sbin/start-dfs.sh  # http://c1:9870
$HADOOP_HOME/sbin/start-yarn.sh  # http://c1:8088
$HADOOP_HOME/bin/mapred --daemon start historyserver  # http://c1:19888
{% endhighlight %}

# 하둡 종료

{:.code-header}
c1

{% highlight bash %}
$HADOOP_HOME/sbin/stop-dfs.sh
$HADOOP_HOME/sbin/stop-yarn.sh
$HADOOP_HOME/bin/mapred --daemon stop historyserver
rm -rf $HADOOP_HOME/data/namenode/*
{% endhighlight %}

{:.code-header}
c2, c3, c4

{% highlight bash %}
rm -rf $HADOOP_HOME/data/datanode/*
{% endhighlight %}