---
title: "ENAS (Efficient Neural Architecture Search)"
order: 2
date: "2020-09-24"
---

ENAS(Efficient Neural Architecture Search)는 의 논문 \<Efficient Neural Architecture Search via Parameter Sharing\>에 등장한 NAS 기술이다.

# 논문 분석

H. Pham, et al., \<Efficient Neural Architecture Search via Parameter Sharing\>, PMLR 80:4095-4104, 2018

## Introduction

- NAS(Neural Architecture Search)는 이미지 분류(Image Classification)와 언어 모델(Language Model)을 위한 모델 구조 설계에 성공적으로 적용되어옴

- NAS의 "RNN Controller"는 다음을 반복하는 반복문 안에서 학습됨
  - Controller는 우선 Candidate Architecture를 뽑음 : 이를 Child Model이라 함
  - Controller는 뽑은 Child Model을 수렴(Convergence)할 때까지 학습시켜 풀고자 하는 task에 대한 성능을 확인
  - Controller는 확인한 성능을 유도 신호(Guiding Signal)로 사용하여 더 유망한 Architecture를 찾는데 사용
  
- **NAS는 놀라운 성과를 냈지만, Computation Cost가 너무 크다.**
- 저자들은 **Child Model의 성능을 확인하기 위해 수렴할 때까지 학습시키는 것이 NAS의 Computational Bottleneck**임을 관찰
  - 심지어 성능 확인 후에는 Child Model의 학습된 Weight는 모두 폐기한다. => Computational Cost 낭비

- 저자들은 **모든 Child Model들이 Weight를 공유(Share)하도록 강제**하여 NAS의 효율성을 증가시킴
  - 이렇게 하면 각 Child Model은 밑바닥(Scratch)부터 (수렴할 때까지) 학습하는 것을 피할 수 있다.
  - 누가 봐도 알 수 있는 한계점 : 각 Child Model은 각자의 Weight를 다르게 사용해야 하지 않을까?
  - Transfer Learning과 Multitask Learning에 대한 이전 연구들은, 특정 task를 위해 학습한 특정 모델의 파라미터는 다른 task를 위해 학습한 다른 모델에 거의 아무런 조작을 가하지 않고도 사용할 수 있다는 것을 보여줌
  - 이 방법을 "ENAS(Efficient Neural Architecture Search)"라 명명

- 저자들은 Child Model 간 파라미터를 "공유할 수 있다"는 사실 뿐만 아니라, 이렇게 함으로서 강력한 performance가 나옴을 실험적으로 보임
  - CIFAR-10에 대해, 저자들의 방법은 Test Error Rate 2.89%
    - 기존 NAS는 Test Error Rate 2.65% => 성능 차이가 거의 없다.
  - Penn Treebank에 대해, 저자들의 방법은 Test Perplexity 55.8
    - 기존 NAS는 Test Perplexity 62.4
    - Penn Treebank에 대해 Post-Training을 사용하지 않는 접근법 중 SOTA => 오히려 성능이 좋아짐
- Computational Cost 아주 크게 감소
  - 논문의 모든 실험은 Nvidia GTX 1080Ti GPU 한 장으로 진행됨
  - Architecture 탐색에는 16시간이 안걸림
    - 기존 NAS는 450장의 GPU를 사용해 3-4일 정도를 학습 : 1000배 이상의 GPU 사용시간 감소

## Methods

- ENAS의 핵심 아이디어 : NAS가 반복하는 모든 그래프들은 Larger Graph의 Sub-Graph로 볼 수 있다.
  - 다시말해, NAS의 탐색공간(Search Space)을 *하나의* Directed Acyclic Graph(DAG, 유향 비순환 그래프)로 나타낼 수 있다. => 이를 ENAS DAG라 한다.
- ENAS DAG는 NAS의 탐색공간 내에 있는 모든 가능한 Child Model들의 중첩(Superposition)
  - 노드(Node)는 local computation을, 엣지(Edge)는 정보의 흐름을 나타냄
  - 각 노드는 각자의 파라미터를 가지고 있고, 이는 해당 노드가 활성화됐을 때만 사용됨
  - ENAS DAG의 구조는 탐색공간에서 모든 Child Model 간 파라미터가 공유되게 만든다.

### Designing Recurrent Cells

- Objective : 주어진 task를 수행하기 위한 최적의 RNN 모델(Child Model)의 Recurrent Cell 만들기
- 이를 위해 저자들은 N개의 노드를 가진 DAG를 사용
  - 각 노드는 Local Computation을 의미, 각 엣지는 데이터의 흐름을 의미
  - 각 노드는 인덱스(index)가 있음
  - 모든 노드는 자기보다 큰 인덱스를 가진 노드 방향으로만 엣지를 가지고 있음 => directed, acyclic 달성
- ENAS의 Controller는 RNN으로서 다음 사항을 결정함
  - 어떤 DAG 안의 어떤 엣지가 활성화되었는지
  - 각 노드에서 어떤 연산이 수행되는지
- NAS와 다른 방법을 사용
  - NAS : Binary Tree를 이용하여 (Child Model의) Architecture의 topology를 고정. Tree의 각 노드가 어떤 연산(Operation)을 할 지만 학습함.
  - ENAS : ENAS가 RNN Cell의 topology와 연산(operation)을 모두 결정할 수 있게 함 => 더 유연해짐
- Recurrent Cell 생성 순서
  - Controller는 N개의 각 노드에 대해 다음을 반복
    - 첫 노드 : Activation Function을 하나 뽑는다.
    - 나머지 노드 : 이전 노드의 단계에서 뽑은 Activation Function을 입력으로 받아 이전 노드(Previous Node)의 인덱스 중 하나와 새 Activation Function을 각각 뽑음
  - ex) $N=4$일 때의 예시 (논문 Fig. 01 참조)
    - Terminology
      - $\boldsymbol{x}\_t$ : Recurrent Cell로의 입력 신호 (ex. Word Embedding)
      - $\boldsymbol{h}\_{t-1}$ : 이전 time step에서의 출력값
      - $\boldsymbol{w}^{(x)}$ : Recurrent Cell이 입력값을 받아들일 때 사용하는 가중치
      - $\boldsymbol{w}^{(h)} _{j, i}$ : Recurrent Cell 내부에서, 노드 $i$ → 노드 $j$ 방향 엣지의 가중치
      - $\boldsymbol{w}^{(h)} _{1}$ : 이전 Recurrent Cell로부터 현재 Recurrent Cell의 첫 번째 노드 방향 엣지의 가중치
    - Controller 동작 과정
      - 노드 1 : Controller는 Activation Function $\tanh$를 뽑음
        - $\boldsymbol{h}\_1 = \tanh (\boldsymbol{w}^{(x)} \cdot \boldsymbol{x}\_t + \boldsymbol{w}^{(h)} \_1 \cdot \boldsymbol{h}\_{t-1})$
      - 노드 2 : Controller는 이전 노드의 인덱스 1과 Activation Function $\textrm{ReLU}$를 뽑음
        - $\boldsymbol{h}\_2 = \textrm{ReLU} (\boldsymbol{w}^{(h)} \_{2,1} \cdot \boldsymbol{h}\_{1})$
      - 노드 3 : Controller는 이전 노드의 인덱스 2와 Activation Function $\textrm{ReLU}$를 뽑음
        - $\boldsymbol{h}\_3 = \textrm{ReLU} (\boldsymbol{w}^{(h)} \_{3,2} \cdot \boldsymbol{h}\_{2})$
      - 노드 4 : Controller는 이전 노드의 인덱스 1과 Activation Function $\tanh$를 뽑음
        - $\boldsymbol{h}\_4 = \tanh (\boldsymbol{w}^{(h)} \_{4,1} \cdot \boldsymbol{h}\_{1})$
    - 출력값 : Loose End(다른 노드들의 입력으로 선택되지 않은 노드들)들을 평균
      - $\boldsymbol{h}\_t = (\boldsymbol{h}\_3 + \boldsymbol{h}\_4)/2$
- 이전 노드의 인덱스를 뽑는 과정에서 어떤 $\boldsymbol{w}^{(h)} _{j, i}$을 사용할 지를 결정
  - ENAS에서 탐색 공간 내 모든 Recurrent Cell은 같은 파라미터 집합을 사용
- 하나의 Recurrent Cell 안에서 $N$개의 노드를 사용하고, $k$개의 Activation Function 후보를 사용한다면, 탐색 공간은 $k^N \times N!$개의 configuration 가짓수를 가짐