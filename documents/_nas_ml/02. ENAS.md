---
title: "ENAS (Efficient Neural Architecture Search)"
order: 2
date: "2020-09-24"
---

ENAS(Efficient Neural Architecture Search)는 의 논문 \<Efficient Neural Architecture Search via Parameter Sharing\>에 등장한 NAS 기술이다.

# 논문 분석

H. Pham, et al., \<Efficient Neural Architecture Search via Parameter Sharing\>, PMLR 80:4095-4104, 2018

## Introduction

- NAS(Neural Architecture Search)는 이미지 분류(Image Classification)와 언어 모델(Language Model)을 위한 모델 구조 설계에 성공적으로 적용되어옴

- NAS의 "RNN 컨트롤러"는 다음을 반복하는 반복문 안에서 학습됨
  - 컨트롤러는 우선 Candidate Architecture를 뽑음 : 이를 Child Model이라 함
  - 컨트롤러는 뽑은 Child Model을 수렴(Convergence)할 때까지 학습시켜 풀고자 하는 task에 대한 성능을 확인
  - 컨트롤러는 확인한 성능을 유도 신호(Guiding Signal)로 사용하여 더 유망한 Architecture를 찾는데 사용
  
- **문제 제기** : NAS는 놀라운 성과를 냈지만, Computation Cost가 너무 크다.
- 저자들은 **Child Model의 성능을 확인하기 위해 수렴할 때까지 학습시키는 것이 NAS의 Computational Bottleneck**임을 관찰
  - 심지어 성능 확인 후에는 Child Model의 학습된 Weight는 모두 폐기한다. => Computational Cost 낭비

- **아이디어** : 저자들은 **모든 Child Model들이 Weight를 공유(Share)하도록 강제**하여 NAS의 효율성을 증가시킴
  - 이렇게 하면 각 Child Model은 밑바닥(Scratch)부터 (수렴할 때까지) 학습하는 것을 피할 수 있다.
  - 누가 봐도 알 수 있는 한계점 : 각 Child Model은 각자의 Weight를 다르게 사용한다.
    - **Motivation** : 하지만 Transfer Learning과 Multitask Learning에 대한 이전 연구들은, 특정 task를 위해 학습한 특정 모델의 파라미터는 다른 task를 위해 학습한 다른 모델에 거의 아무런 조작을 가하지 않고도 사용할 수 있다는 것을 보여줌
  - 이 방법을 "ENAS(Efficient Neural Architecture Search)"라 명명

- 저자들은 Child Model 간 파라미터를 "공유할 수 있다"는 사실 뿐만 아니라, 이렇게 함으로서 강력한 performance가 나옴을 실험적으로 보임
  - CIFAR-10에 대해, 저자들의 방법은 Test Error Rate 2.89%
    - 기존 NAS는 Test Error Rate 2.65% => 성능 차이가 거의 없다.
  - Penn Treebank에 대해, 저자들의 방법은 Test Perplexity 55.8
    - 기존 NAS는 Test Perplexity 62.4
    - Penn Treebank에 대해 Post-Training 과정을 Utilize하지 않는 접근법 중 SOTA => 오히려 성능이 좋아짐
- Computational Cost 아주 크게 감소
  - 논문의 모든 실험은 Nvidia GTX 1080Ti GPU 한 장으로 진행됨
  - Architecture 탐색에는 16시간이 안걸림
    - 기존 NAS는 450장의 GPU를 사용해 3-4일 정도를 학습 : 1000배 이상의 GPU 사용시간 감소

## Methods

- ENAS의 핵심 아이디어 : NAS가 반복하는 모든 그래프들은 Larger Graph의 Sub-Graph로 볼 수 있다.
  - 다시말해, NAS의 탐색공간(Search Space)을 *하나의* Directed Acyclic Graph(DAG, 유향 비순환 그래프)로 나타낼 수 있다. => ENAS DAG
- ENAS DAG는 NAS의 탐색공간 내에 있는 모든 가능한 Child Model들의 중첩(Superposition)
  - 노드(Node)는 local computation을, 엣지(Edge)는 정보의 흐름을 나타냄
  - 각 노드는 각자의 파라미터를 가지고 있고, 이는 해당 노드가 활성화됐을 때만 사용됨
  - ENAS DAG의 구조는 탐색공간에서 모든 Child Model 간 파라미터가 공유되게 만든다.

### Designing Recurrent Cells

- Recurrent Cell을 설계하기 위해서, 저자들은 N개의 노드를 가진 DAG를 사용
- ENAS의 Controller는 RNN으로서 어떤 DAG 안의 어떤 엣지가 활성화되었는지, 각 노드에서 어떤 연산이 수행되는지를 결정
- 저자가 사용한 탐색 공간은 ENAS가 각 RNN Cell에서의 연산과 