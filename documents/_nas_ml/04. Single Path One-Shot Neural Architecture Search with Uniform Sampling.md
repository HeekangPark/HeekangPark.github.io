---
title: "Single Path One-Shot Neural Architecture Search with Uniform Sampling"
order: 4
date: "2020-10-08"
---

# 논문 분석

Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun, \<Single Path One-Shot Neural Architecture Search with Uniform Sampling\>, ICLR 2020

## Introduction

- NAS는 feature engineering, weight optimization problem, architecture design 문제를 풀어 architecture engineering을 자동화
- 초창기 NAS는 다수의 architecture가 sample되어 바닥부터 학습해야 해, 데이터셋의 크기가 크면 비용이 너무 비싸다는 문제점
- 최근 접근에서는 연산량을 줄이기 위해 weight sharing 전략이 사용됨
  - supernet은 모든 architecture가 단 한번 학습되도록 함
  - 각 architecture는 각자의 weight를 supernet에서부터 받음. Fine-Tuning만 하면 됨 → 연산량 감소
- 대부분의 weight sharing 접근법들은 탐색공간을 parametrize하기 위해 continuous relaxation을 사용
  - architecture distribution parameter들은 경사하강법으로 supernet이 학습되는동안 같이 최적화됨
  - 최고의 architectures는 최적화가 끝난 후 distribution에서 sample됨
  - 논란점 :
    - supernet의 가중치들은 깊게 연관(couple)되어있음
      - 왜 특정 architecture에서도 상속받은 가중치가 좋은 효과를 보이는지는 불분명함
    - architecture의 weight와 supernet의 weight는 최적화 과정에서 깊게 연관됨
      - gradient-based 방법의 greedy한 성질 때문에 최적화 과정에서 bias가 생김 → Architecture 탐색에 영향을 끼칠 수 있음
      - 이전 연구자들은 복잡한 최적화 기법들을 사용해 이 문제를 해결했음
- One-shot 패러다임은 두 번째 논란점을 해소함
  - supernet과 가중치 상속(weight inheritance)는 이전 방법들과 비슷하게 사용하지만, architecture relaxation이 없음.
  - supernet의 학습이 완료된 이후 architecture 탐색 문제를 해결 (supernet의 학습과 architecture 탐색 문제를 분리)
  - nested optimization과 joint optimization의 장점을 합친 것 → 효과적이고 유연한 방식
- 첫 번째 논란점은 여전히 문제가 됨
  - 기존 one-shot 접근법들은 여전히 supernet에 coupled weight를 가지고 있음
  - 기존 접근법들의 최적화 방법은 복잡하고 민감한 hyperparameter를 포함하고 있음
  - 큰 데이터 셋에 대해 경쟁력 있는 결과를 보여주지 못함

- 저자들은 다음을 주장
  - nested optimization과 joint optimization을 사용하는 기존 NAS 접근법들의 문제점을 지적
  - sequential optimization으로 nested optimization과 joint optimization의 장점을 결합한 one-shot paradigm에 대한 관심 재고
  - uniform sampling을 통한 single path one-shot 접근법을 제안
    - 기존 one-shot 접근법들의 문제점을 해결
    - 간단하기에, 큰 search space를 사용할 수 있음 (ex. channel 크기 변경, bit width)
    - architecture 탐색 과정이 효율적이고 유연함
    - 현실 세계의 제약(ex. low latency)을 지원하기 위해서 진화 알고리즘을 사용
  - 큰 데이터 셋(ImageNet)에 대한 실험 결과, 저자들의 접근법이 정확도, 메모리, 학습 시간, architecture 탐색 효율 및 유연성 측면에서 SOTA임을 확인

## Review of NAS Approach

- architecture의 탐색 공간 $\mathcal{A}$는 DAG(Directed Acyclic Graph)로 표현됨
  - 탐색의 결과 얻는 최종 모델 $\mathcal{N}(a, w)$
    - $a$ : $\mathcal{A}$의 부분 그래프(subgraph). 즉, $a \in \mathcal{A}$. 모델의 architecture
    - $w$ : 모델의 weight
- NAS는 다음 두 가지 최적화 문제를 푸는 것
  - weight optimization : $w\_{a} = \textrm{argmin} \_{w} \mathcal{L}\_{train}(\mathcal{N}(a, w))$
    - $\mathcal{L}\_{train}()$ : loss function of the training set
  - architecture optimization : $a^{\*} = \textrm{argmax} \_{a \in \mathcal{A}} \textrm{ACC}\_{val}(\mathcal{N}(a, w\_{a}))$
    - $\textrm{ACC}\_{val}()$ : accuracy of the validation set

- 초기 NAS 접근법들은 두 최적화 문제를 nested하게 풂 : nested optimization
  - 여러 개의 (후보) 모델들이 $\mathcal{A}$로부터 샘플링됨
  - 각 후보 모델들은 바닥부터 weight optimization을 수행 → 아주 비쌈
  - 작은 데이터 셋(ex. CIFAR-10)과 작은 탐색 공간(ex. 하나의 block 찾기)에서만 유의미하게 적용 가능
- 최근 NAS 접근법들은 weight sharing 전략을 도입
  - $\mathcal{A}$는 supernet($\mathcal{N}(\mathcal{A}, W)$)로 인코딩됨
    - $W$ : supernet의 weight
  - supernet은 단 한번만 학습함
  - 모든 (후보) architecture들은 $W$로부터 자신의 weight를 상속(inherit)받음 → 각 architecture들은 supernet(의 공통 node)에서 그들의 weight를 공유
  - 필요하다면 각 architecture마다 fine-tuning을 해야 하겠지만, 바닥부터 학습하는 일은 일어나지 않는다. → architecture 탐색은 빠르고 큰 데이터 셋(ex. ImageNet)에서도 쓸 만하다.
- 완화 (Relaxation) : discrete한 architecture 탐색 공간을 continuous하게 바꿈
  - $\mathcal{A}$ → $\mathcal{A}(\theta)$로 완화
    - $\theta$ : continuous 파라미터. 공간 상에 architecture의 분포(distribution)을 의미
    - $\mathcal{A} \subseteq \mathcal{A}(\theta)$ → $\mathcal{A}(\theta)$에서 샘플링 된 architecture는 $\mathcal{A}$에서는 맞지 않을수도 있음
  - weight sharing하는 대부분의 접근법들이 사용
  - Gradient-based Method 사용 가능 : joint optimization
    - weight와 architecture distribution이 같이(jointly) 최적화됨
    - $(\theta^{\*}, W\_{\theta^{\*}}) = \textrm{argmin}\_{\theta, W} \mathcal{L}\_{train}(\mathcal{N}(\mathcal{A}(\theta), W))$
    - 또는 bi-level optimization을 할 수도 있음 : $\theta^{\*} = \textrm{argmax} \_{\theta} \textrm{ACC}\_{val} (\mathcal{N}(\mathcal{A}(\theta), W^{\*} \_{\theta}))$ s.t. $W^{\*}\_{\theta} = \textrm{argmin}\_{W} \mathcal{L}\_{train} (\mathcal{N}(\mathcal{A}(\theta), W))$
    - 최적화 이후, 최고의 architecture $a^{\*}$가 $\mathcal{A}(\theta^{\*})$으로부터 선택됨
- $(\theta^{\*}, W\_{\theta^{\*}}) = \textrm{argmin}\_{\theta, W} \mathcal{L}\_{train}(\mathcal{N}(\mathcal{A}(\theta), W))$ 최적화는 문제가 있음
  - supernet의 각 node의 가중치는 서로 의존 관계가 생기고, 최적화 과정에서 깊게 연관됨(couple)
    - 특정 architecture를 샘플링하면, 이 architecture는 $W$에서 특정 node들의 weight를 상속받음
    - 이렇게 뽑힌 weight들은 (supernet에서 연결되어 있던) 다른 node와 연관이 끊기는데(decouple), 왜 여전히 잘 작동하는지는 아직 밝혀지지 않음
  - architecture의 파리미터 $\theta$와 supernet의 weight $W$에 대한 joint optimization은 더 큰 복잡성을 야기함
    - $(\theta^{\*}, W\_{\theta^{\*}}) = \textrm{argmin}\_{\theta, W} \mathcal{L}\_{train}(\mathcal{N}(\mathcal{A}(\theta), W))$을 푸는 것은 $\theta$ 안의 특정 영역과 $W$ 안의 특정 node들에 편향(bias)이 생기게 만듦
    - 이 편향은 supernet의 몇몇 node들은 학습이 잘 되게, 몇몇 node들은 학습이 잘 안되게 만듦