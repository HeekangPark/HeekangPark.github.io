---
title: "L09. Planning and Learning with Tabular Methods"
order: 9
date_created: "2021-06-21"
date_modified: "2021-06-25"
---

# 모델(Model)

**모델(Model)**은 에이전트(agent)가 한 상태(state)에서 환경(environment)이 특정 행동(action)에 어떻게 반응할 지 예측하기 위해 사용하는 모든 것을 뜻하는 말이다. 즉 모델은 상태와 행동을 입력으로 받아 결과(다음 상태와 보상)을 출력하는 일종의 함수라 생각하면 된다.

모델은 크게 두 종류로 구분할 수 있다.

{:.no-guide-line}
- Distribution Model : 가능한 모든 결과값들과 그 결과값이 나올 수 있는 확률을 출력하는 모델
- Sample Model : 해당 결과값이 나올 수 있는 확률에 따라 한 개의 결과(샘플)를 임의로 출력하는 모델

예를 들어 주사위를 굴리는 문제에 대해 모델을 만든다면, Sample Model의 경우 1, 2, 3, 4, 5, 6 중 랜덤하게 하나의 숫자를 출력하게 된다.[^1] 그리고 Distribution Model의 경우 `[(1, (1/6)), (2, (1/6)), (3, (1/6)), (4, (1/6)), (5, (1/6)), (6, (1/6))]`과 같이, 나올 수 있는 모든 결과값들과 그 결과값이 나올 확률을 출력하게 된다.

[^1]: 주사위의 각 눈이 나올 확률은 모두 1/6로 동일하므로 1, 2, 3, 4, 5, 6 중 한 숫자가 완전 무작위로(uniformly) 출력될 것이다.

Distribution Model이 있다면 이를 이용해 결과 샘플을 만들 수 있으므로, Distribution Model이 Sample Model보다 더 강력한 모델이다. 그러나 일반적으로 Distribution Model은 Sample Model에 비해 만들기 아주 어렵다.

모델을 이용하면 가짜 경험(simulated experience)을 만들 수 있다. 시작 상태와 행동이 주어졌을 때, Sample Model을 이용하면 완전한 샘플 에피소드 하나를 만들어낼 수 있고, Distribution Model을 이용하면 가능한 모든 에피소드들을 각 에피소드가 발생할 확률과 함께 만들어낼 수 있다.

## 모델 기반 방법(Model-based Method) vs. Model-free Method

모델을 사용하는지 안하는지를 가지고 RL을 분류할 수 있다.

{:.no-guide-line}
- 모델 기반 방법(Model-based Method) : 모델을 이용해 학습을 진행하는 RL 학습법
- Model-free Method : 모델을 이용하지 않고 학습을 진행하는 RL 학습법

모델 기반 방법의 예로는 [DP](/SNU_m3309.000200/05-dp)가 있다. DP에서는 환경에 대해 모든 지식이 주어진 경우를 가정했는데, Distribution Model이 주어졌다면 DP를 적용할 수 있다. Model-free Method의 예로는 [MC Method](/SNU_m3309.000200/06-mc), [TD Method](/SNU_m3309.000200/07-td)가 있다.

# Planning

**Planning**이란 모델이 생성한 가짜 경험(simulated experience)을 이용해 상태 공간(state space)에서 가치 함수(value function)를 계산해 가며 최적 정책(optimal policy)을 만드는 모든 종류의 방법을 의미한다.[^2]

[^2]: 엄밀히 말하면, 이 설명은 State-space Planning에 대한 설명이다. Planning은 State-space Planning과 Plan-space Planning, 이렇게 두 가지로 크게 구분할 수 있다. Plan-space Planning은 Plan 공간(Plan space)을 (일반적으로 진화적 방법(evolutionary method)을 사용해) 탐색하며 최적의 Plan을 찾는 방법이다(최적의 Plan으로 만들어지는 정책이 최적 정책이 된다). Plan-space Planning은 State-space Planning에 비해 조금 더 유연하다는 장점이 있으나, (RL에서 특히 중점적으로 다루는) 연속적 확률 결정 문제(stochastic sequential decision problem)에는 효율적으로 적용하기 어렵다는 단점이 있다. 따라서 일반적으로 RL에서는 State-space Planning을 사용한다.

사실 Planning은 거창하게 새로운 것이 아니다. 우리가 이전에 봤던 [MC Method](/SNU_m3309.000200/06-mc), [TD Method](/SNU_m3309.000200/07-td)와 같은 Model-free Method에서는 환경과 실제로 상호작용하면서 만든 실제 경험(real experience)을 토대로 학습이 진행되었는데, Planning은 모델이 생성한 가짜 경험(simulated experience)에 대해 동일한 알고리즘을 적용하는 것이다.[^3] 예를 들어 다음과 같이 모델이 생성한 가짜 경험에 대해 Q-Learning을 적용하는, Q-Planning을 생각할 수 있다.

[^3]: Model-free Method 버전의 MC Method, TD Method는 각각 MC Learning, TD Learning이라, Model-based Method 버전의 MC Method, TD Method는 MC Planning, TD Planning이라 부르기도 한다.

{:.pseudo-code-header}
Random-sample Q-Planning

<div class="pseudo-code" markdown="block">

<span class="keyword-highlight">Loop</span> forever:

<span class="indent-1"/>상태 $S$와, $S$에서 가능한 행동 $A$를 무작위로 뽑는다.

<span class="indent-1"/>모델을 이용해 상태 $S$에서 행동 $A$를 수행했을 때의 보상 $R$과 다음 상태 $S'$를 얻는다. 즉, $Model(S,\,A) = R,\,S'$

<span class="indent-1"/>Q-Learning 알고리즘을 적용한다 : $Q(S,\,A) \leftarrow Q(S,\,A) + \alpha [R + \gamma \max_a Q(S',\,a) - Q(S,\,A)]$

</div>

# Dyna

위에서 살펴본 Q-Planning은 Offline 상황에서 오직 주어진 모델과 Planning만을 이용해 에이전트를 학습하는 방법이다. 그렇다면 Online 상황에서는 어떻게 될까? 그러니까, 환경과 상호작용하면서 계속 새로운 정보를 얻는 상황에서는 어떻게 Planning을 해야 할까?

**Dyna-Q**는 Online 상황에서 사용할 수 있는, Planning, 행동(acting), 학습(Learning)을 결합한 방법이다. Dyna-Q에서 에이전트는 환경과 상호작용하며 실제 경험(real experience)을 얻고, 두 가지 방법으로 이를 사용한다.

- **Direct RL** : 실제 경험을 바로 사용해 (MC Method, TD Method 등의 방법을 이용하여) 가치 함수를 계산하고 정책을 개선
- **Indirect RL** : 실제 경험을 이용하여 모델을 학습시키고[^4], 이렇게 학습된 모델을 이용해 Planning을 수행

[^4]: 이를 **모델 학습(Model Learning)**이라 한다.

Indirect RL은 주어진 (실제) 경험을 극한까지 사용하는 방법이다. 따라서 Indirect RL을 수행하면 적은 수의 경험으로도(= 실제 환경과 최소한의 상호작용을 하고도) 에이전트를 학습시킬 수 있다. 한편 Direct RL은 훨씬 간단하고, 무엇보다 모델을 설계한 방법에 따라 발생할 수 있는 편향(bias)이 없다는 장점이 있다.

