---
title: "L09. Planning and Learning with Tabular Methods"
order: 9
date_created: "2021-06-21"
date_modified: "2021-07-02"
---

# 모델(Model)

**모델(Model)**은 에이전트(agent)가 한 상태(state)에서 환경(environment)이 특정 행동(action)에 어떻게 반응할 지 예측하기 위해 사용하는 모든 것을 뜻하는 말이다. 즉 모델은 상태와 행동을 입력으로 받아 결과(다음 상태와 보상)을 출력하는 일종의 함수라 생각하면 된다.

모델은 크게 두 종류로 구분할 수 있다.

{:.no-guide-line}
- Distribution Model : 가능한 모든 결과값들과 그 결과값이 나올 수 있는 확률을 출력하는 모델
- Sample Model : 해당 결과값이 나올 수 있는 확률에 따라 한 개의 결과(샘플)를 임의로 출력하는 모델

예를 들어 주사위를 굴리는 문제에 대해 모델을 만든다면, Sample Model의 경우 1, 2, 3, 4, 5, 6 중 랜덤하게 하나의 숫자를 출력하게 된다.[^1] 그리고 Distribution Model의 경우 `[(1, (1/6)), (2, (1/6)), (3, (1/6)), (4, (1/6)), (5, (1/6)), (6, (1/6))]`과 같이, 나올 수 있는 모든 결과값들과 그 결과값이 나올 확률을 출력하게 된다.

[^1]: 주사위의 각 눈이 나올 확률은 모두 1/6로 동일하므로 1, 2, 3, 4, 5, 6 중 한 숫자가 완전 무작위로(uniformly) 출력될 것이다.

Distribution Model이 있다면 이를 이용해 결과 샘플을 만들 수 있으므로, Distribution Model이 Sample Model보다 더 강력한 모델이다. 그러나 일반적으로 Distribution Model은 Sample Model에 비해 만들기 아주 어렵다.

모델을 이용하면 가짜 경험(simulated experience)을 만들 수 있다. 시작 상태와 행동이 주어졌을 때, Sample Model을 이용하면 완전한 샘플 에피소드 하나를 만들어낼 수 있고, Distribution Model을 이용하면 가능한 모든 에피소드들을 각 에피소드가 발생할 확률과 함께 만들어낼 수 있다.

## 모델 기반 방법(Model-based Method) vs. Model-free Method

모델을 사용하는지 안하는지를 가지고 RL을 분류할 수 있다.

{:.no-guide-line}
- 모델 기반 방법(Model-based Method) : 모델을 이용해 학습을 진행하는 RL 학습법
- Model-free Method : 모델을 이용하지 않고 학습을 진행하는 RL 학습법

모델 기반 방법의 예로는 [DP](/SNU_m3309.000200/05-dp)가 있다. DP에서는 환경에 대해 모든 지식이 주어진 경우를 가정했는데, Distribution Model이 주어졌다면 DP를 적용할 수 있다. Model-free Method의 예로는 [MC Method](/SNU_m3309.000200/06-mc), [TD Method](/SNU_m3309.000200/07-td)가 있다.

# Planning

**Planning**이란 모델이 생성한 가짜 경험(simulated experience)을 이용해 상태 공간(state space)에서 가치 함수(value function)를 계산해 가며 최적 정책(optimal policy)을 만드는 모든 종류의 방법을 의미한다.[^2]

[^2]: 엄밀히 말하면, 이 설명은 State-space Planning에 대한 설명이다. Planning은 State-space Planning과 Plan-space Planning, 이렇게 두 가지로 크게 구분할 수 있다. Plan-space Planning은 Plan 공간(Plan space)을 (일반적으로 진화적 방법(evolutionary method)을 사용해) 탐색하며 최적의 Plan을 찾는 방법이다(최적의 Plan으로 만들어지는 정책이 최적 정책이 된다). Plan-space Planning은 State-space Planning에 비해 조금 더 유연하다는 장점이 있으나, (RL에서 특히 중점적으로 다루는) 연속적 확률 결정 문제(stochastic sequential decision problem)에는 효율적으로 적용하기 어렵다는 단점이 있다. 따라서 일반적으로 RL에서는 State-space Planning을 사용한다.

사실 Planning은 거창하게 새로운 것이 아니다. 우리가 이전에 봤던 [MC Method](/SNU_m3309.000200/06-mc), [TD Method](/SNU_m3309.000200/07-td)와 같은 Model-free Method에서는 환경과 실제로 상호작용하면서 만든 실제 경험(real experience)을 토대로 학습이 진행되었는데, Planning은 모델이 생성한 가짜 경험(simulated experience)에 대해 동일한 알고리즘을 적용하는 것이다.[^3] 예를 들어 다음과 같이 모델이 생성한 가짜 경험에 대해 Q-Learning을 적용하는, Q-Planning을 생각할 수 있다.

[^3]: 모델을 사용하여, 모델이 만들어낸 가짜 경험(simulated experience)을 이용해 에이전트를 학습시키는 것을 Planning이라 한다. 한편 앞에서 배웠던 것처럼, 모델을 사용하지 않고 실제 경험(real experience)을 이용해 에이전트를 학습시키는 것을 Learning이라 한다(비록 한국어로는 둘 다 학습이라 하지만 말이다). 그래서 Model-free Method 버전의 MC Method, TD Method는 각각 MC Learning, TD Learning이라, Model-based Method 버전의 MC Method, TD Method는 MC Planning, TD Planning이라 부른다.

{:.pseudo-code-header}
Random-sample Q-Planning

<div class="pseudo-code" markdown="block">

<span class="keyword-highlight">Loop</span> forever:

<span class="indent-1"/>상태 $S$와, $S$에서 가능한 행동 $A$를 무작위로 뽑는다.

<span class="indent-1"/>모델을 이용해 상태 $S$에서 행동 $A$를 수행했을 때의 보상 $R$과 다음 상태 $S'$를 얻는다. 즉, $Model(S,\,A) = R,\,S'$

<span class="indent-1"/>Q-Learning 알고리즘을 적용한다 : $Q(S,\,A) \leftarrow Q(S,\,A) + \alpha [R + \gamma \max_a Q(S',\,a) - Q(S,\,A)]$

</div>

# Dyna

위에서 살펴본 Q-Planning은 Offline 상황에서 오직 주어진 모델과 Planning만을 이용해 에이전트를 학습하는 방법이다. 그렇다면 Online 상황에서는 어떻게 될까? 그러니까, 환경과 상호작용하면서 계속 새로운 정보를 얻는 상황에서는 어떻게 Planning을 해야 할까?

**Dyna**는 Online 상황에서 사용할 수 있는, Planning, 행동(acting), 학습(Learning)을 결합한 방법이다. Dyna에서 에이전트는 환경과 상호작용하며 실제 경험(real experience)을 얻고, 두 가지 방법으로 이를 사용한다.

{:.no-guide-line}
- **Direct RL** : 실제 경험을 바로 사용해 (MC Method, TD Method 등의 방법을 이용하여) 가치 함수를 계산하고 정책을 개선한다.
- **Indirect RL** : 실제 경험을 이용하여 모델을 학습시키고[^4], 이렇게 학습된 모델을 이용해 Planning을 수행한다(= 모델이 생성한 가짜 경험(simulated experience)을 사용해[^5] 가치 함수를 계산하고 정책을 개선한다).

[^4]: 이를 **모델 학습(Model Learning)**이라 한다.
[^5]: 참고로 가짜 경험(simulated experience) 생성을 위해, 시작 상태(starting state)와 행동(action)을 선택해 모델에 입력하는 과정을 **Search Control**이라 한다.

{% include caption-img.html src="09-dyna.png" title="Fig.01 Dyna" description="Dyna는 Online 상황에서 사용할 수 있는, Planning, 행동(acting), 학습(Learning)을 결합한 방법이다." %}

Indirect RL은 주어진 (실제) 경험을 극한까지 사용하는 방법이다. 따라서 Indirect RL을 수행하면 적은 수의 경험으로도(= 실제 환경과 최소한의 상호작용을 하고도) 에이전트를 학습시킬 수 있다. 한편 Direct RL은 훨씬 간단하고, 무엇보다 모델을 설계한 방법에 따라 발생할 수 있는 편향(bias)이 없다는 장점이 있다.

Dyna-Q는 Q-Learning으로 Direct RL을 수행하고, Q-Planning으로 Indirect RL을 수행하는 알고리즘이다.[^6] 이를 의사 코드로 나타내면 다음과 같다.[^7]

[^6]: 일반적으로 Dyna 알고리즘에서는 (Dyna-Q에서처럼) Direct RL과 Indirect RL에서 동일한 RL Method를 사용한다.
[^7]: 아래 코드에서 Acting, Learning, Planning은 순차적으로(sequentially) 실행되는 것처럼 서술되어 있다. 하지만 이는 설명의 명확성을 위한 것으로, 원래 Dyna 알고리즘에서 Acting, Learning, Planning은 동시에 시행된다. 즉, 환경과 상호작용을 하는 동시에 기존 경험들을 토대로 Learning(Direct RL)을 진행하고, 또 동시에 모델 학습 및 Planning(Indirect RL)을 진행한다. 사실 Planning을 통해 더 좋은 정책(policy)을 빠르게 학습하면 학습할수록 실제 받는 보상의 크기가 커지므로, Planning은 가능한 한 빨리 하는 것이 좋다.

{:.pseudo-code-header}
Dyna-Q

<div class="pseudo-code" markdown="block">

모든 $s \in \mathcal{S}^{+}$, $a \in \mathcal{A}(s)$에 대해, $Q(s,\,a) \in \mathbb{R}$을 임의의 값으로 초기화. 단, $Q(\text{terminal},\,\cdot) = 0$.

모든 $s \in \mathcal{S}^{+}$, $a \in \mathcal{A}(s)$에 대해, $Model(s,\,a) = [\,]$(empty list)

{:.mb-1}
<span class="comment-highlight">//$Model$에 상태-행동 쌍 $(s,\,a)$를 입력하면, 상태 $s$에서 행동 $a$를 수행했을 때 받게 되는(정확히는, 받을 것으로 예상되는) 보상과 다음 상태를 반환한다.</span>

<span class="keyword-highlight">Loop</span> forever:

<span class="indent-1"/>$S \leftarrow$ 현재 상태(nonterminal state)

<span class="indent-1"/>현재 $Q$에 대해 $\varepsilon$-greedy Policy를 사용하여 $S$에서의 행동 $A$를 선택. 즉, $A \leftarrow \varepsilon$-greedy$(S,\,Q)$

<span class="indent-1"/>$A$ 시행하고, 보상 $R$과 다음 상태 $S'$ 관측<span class="comment-highlight"> //Acting</span>

<span class="indent-1"/>$Q(S,\,A) \leftarrow Q(S,\,A) + \alpha [R + \gamma \max\_a Q(S',\,a) - Q(S,\,A)]$<span class="comment-highlight"> //Learning (Direct RL)</span>

<span class="indent-1"/>$Model(S,\,A) \leftarrow R,\,S'$<span class="comment-highlight"> //Model Learning</span>

<span class="indent-1"/><span class="keyword-highlight">Loop</span> repeat $n$ times:<span class="comment-highlight"> //Planning</span>

<span class="indent-2"/>$S \leftarrow $ 이때까지 경험했던 상태 중 하나를 무작위로 선택

<span class="indent-2"/>$A \leftarrow $ $S$에서의 이때까지 경험했던 행동 중 하나를 무작위로 선택

<span class="indent-2"/>$R,\,S' \leftarrow Model(S,\,A)$

<span class="indent-2"/>$Q(S,\,A) \leftarrow Q(S,\,A) + \alpha [R + \gamma \max\_a Q(S',\,a) - Q(S,\,A)]$

</div>

## 예제 : Maze

Dyna-Q를 이용해 Fig.02의 미로를 푸는 에이전트를 학습시켜 보자. 미로에서 에이전트는 다음과 같이 움직인다.

{:.no-guide-line}
- S에서 시작해, G에 도달하는 것이 목표이다. 에이전트가 G에 도달하면 다시 S로 이동해 새로운 에피소드를 시작한다.
- 각 칸에서 에이전트는 상, 하, 좌, 우, 4방향으로 한 칸씩 움직일 수 있다. 단, 장애물(검은 칸) 또는 미로 바깥으로는 이동할 수 없다.
- G로의 이동은 +1의 보상을 받는다. 이를 제외한 모든 이동은 0의 보상을 받는다.

{% include caption-img.html src="09-maze.png" title="Fig.02 Maze" description="S에서 시작해, G에 도달하는 것이 목표이다. 에이전트가 G에 도달하면 다시 S로 이동해 새로운 에피소드를 시작한다. 각 칸에서 에이전트는 상, 하, 좌, 우, 4방향으로 한 칸씩 움직일 수 있다. 단, 장애물(검은 칸) 또는 미로 바깥으로는 이동할 수 없다. G로의 이동은 +1의 보상을 받는다. 이를 제외한 모든 이동은 0의 보상을 받는다." %}

<div class="code-folder" markdown="block">

{:.code-header}
Maze

{% highlight python linenos %}
import numpy as np
import matplotlib.pyplot as plt

class Env:
    def __init__(self):        
        self.board = np.zeros((6, 9))
        self.board[1:4, 2] = -1
        self.board[4, 5] = -1
        self.board[0:3, 7] = -1
        
        self.state = [2, 0]
    
    def _getStateIdx(self):
        return self.state[0] * 9 + self.state[1]
    
    def initEpisode(self):
        self.state = [2, 0]
        return True, self._getStateIdx(), 0
    
    def interact(self, action):
        if action == 0: # up
            new_state = [self.state[0] - 1, self.state[1]]
        elif action == 1: # down
            new_state = [self.state[0] + 1, self.state[1]]
        elif action == 2: # left
            new_state = [self.state[0], self.state[1] - 1]
        elif action == 3: # right
            new_state = [self.state[0], self.state[1] + 1]
        else:
            raise Exception("Invalid param:action")
            
        if (new_state[0] < 0 or new_state[0] > 5 or new_state[1] < 0 or new_state[1] > 8) or (self.board[tuple(new_state)] == -1): # border
            new_state = self.state
            R = 0
            status = True
        elif new_state == [0, 8]: # goal
            R = 1
            status = False
        else:
            R = 0
            status = True
        
        self.state = new_state
        
        return status, self._getStateIdx(), R
    
class DynaQ:
    class Model:
        def __init__(self):
            self.model = dict()
        
        def add(self, state, action, reward, next_state):
            if not state in self.model.keys():
                self.model[state] = dict()

            self.model[state][action] = [reward, next_state]
        
        def getRS_(self, state, action):
            return self.model[state][action]
        
        def getRandomSA(self):
            state = np.random.choice(list(self.model.keys()))
            action = np.random.choice(list(self.model[state].keys()))
            return state, action
        
    def __init__(self, env, planning_num, alpha=0.1, gamma=0.95, epsilon=0.1):
        self.env = env
        self.planning_num = planning_num
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        
        self.Q = np.zeros((54, 4))
        self.model = self.Model()
    
    def chooseAction(self, state, greedy=False): 
        if greedy: # greedy
            max_actions = np.argwhere(self.Q[state] == np.max(self.Q[state])).flatten()
            return np.random.choice(max_actions)
        else: # ε-greedy
            if np.random.random() < self.epsilon: # exploration
                return np.random.choice(4)
            else: # exploitation
                max_actions = np.argwhere(self.Q[state] == np.max(self.Q[state])).flatten()
                return np.random.choice(max_actions)
    
    def test(self, title):
        def getState(idx):
            return [idx // 9, idx % 9]
        
        def generateEpisode():
            episode = []
            status, S, R = self.env.initEpisode()
            A = self.chooseAction(S, greedy=True)
            episode.append(S)
            episode.append(A)
            while True:
                status, S, R = self.env.interact(A)
                if status == False:
                    break

                A = self.chooseAction(S, greedy=True)
                episode.append(R)
                episode.append(S)
                episode.append(A)

            episode.append(R)
            episode.append(S)

            return episode
        
        actions = ["↑", "↓", "←", "→"]
        
        board = [[" " for x in range(9)] for x in range(6)]
        board[1][2] = "■"
        board[2][2] = "■"
        board[3][2] = "■"
        board[4][5] = "■"
        board[0][7] = "■"
        board[1][7] = "■"
        board[2][7] = "■"
        
        episode = generateEpisode()
        
        i = 0
        R_sum = 0
        while True:
            if i + 2 > len(episode) - 1:
                break
                
            S = getState(episode[i])
            A = actions[episode[i + 1]]
            R_sum += episode[i + 2]
            i += 3
            
            if board[S[0]][S[1]] != " ":
                raise Exception("something is wrong...")
                
            board[S[0]][S[1]] = A
            
        S = getState(episode[i])
        board[S[0]][S[1]] = "□"
        
        print(f"{title}")
        for i in range(6):
            print("|", end="")
            for j in range(9):
                print(board[i][j], end="|")
            print()
        print(f"rewards = {R_sum}")
        print()
        
        return episode
    
    def learn(self, episode_num):
        history = np.zeros(episode_num)
        for e in range(episode_num):
            status, cur_S, R = self.env.initEpisode()
            steps = 0
            while status:
                steps += 1
                A = self.chooseAction(cur_S)
                status, next_S, R = self.env.interact(A)
                
                self.Q[cur_S, A] = self.Q[cur_S, A] + self.alpha * (R + self.gamma * np.max(self.Q[next_S]) - self.Q[cur_S, A])
                self.model.add(state=cur_S, action=A, reward=R, next_state=next_S)
                
                cur_S = next_S
                
                for n in range(self.planning_num):
                    S, A = self.model.getRandomSA()
                    R, S_ = self.model.getRS_(S, A)
                    self.Q[S, A] = self.Q[S, A] + self.alpha * (R + self.gamma * np.max(self.Q[S_]) - self.Q[S, A])
                    
            history[e] = steps
        
        return history

def drawGraph(histories, episode_num, planning_nums):
    for i, planning_num in enumerate(planning_nums):
        plt.plot(np.arange(episode_num - 1) + 2, histories[i, 1:], label=f"[planning_num = {planning_num}]")
    
    plt.title("Dyna-Q")
    plt.xlabel("Episodes")
    plt.ylabel("Steps per episode")
    plt.xticks([2, 10, 20, 30])
    plt.ylim(10, 800)
    plt.legend()
    plt.show()
    
if __name__ == "__main__":
    np.random.seed(0)
    env = Env()
    
    agent = DynaQ(env, planning_num=30)
    agent.learn(episode_num=10)
    agent.test(title="Dyna-Q [planning=30, episodes=10]")    
    
    episode_num = 30
    repeats = 10
    planning_nums = [0, 5, 50]
    histories = np.zeros((len(planning_nums), episode_num))
    
    for i, planning_num in enumerate(planning_nums):
        for repeat in range(repeats):
            agent = DynaQ(env, planning_num=planning_num)
            history = agent.learn(episode_num=episode_num)
            histories[i] += history
    
    histories /= repeats
    
    drawGraph(histories, episode_num=episode_num, planning_nums=planning_nums)
{% endhighlight %}

- line 4 ~ 45 : `Env` 클래스
  - line 13 ~ 14 : `Env._getStateIdx()` 메소드
    - 현재 상태(state)를 반환
  - line 16 ~ 18 : `Env.initEpisode()` 메소드
    - 에이전트를 S 위치로 옮기고 새 에피소드를 시작
    - `status`, `state`, `reward` 반환
      - `status` : 현재 에피소드 진행 상태. `True`면 에피소드가 진행 중인 상태를, `False`면 에피소드가 종료되었음을 나타낸다.
      - `state` : 현재 상태(state) (`Env._getStateIdx()` 메소드 이용)
      - `reward` : 보상
  - line 20 ~ 45 : `Env.interact()` 메소드
    - 에이전트가 환경과 상호작용하는 메소드
    - `status`, `state`, `reward` 반환
      - `status` : 현재 에피소드 진행 상태. `True`면 에피소드가 진행 중인 상태를, `False`면 에피소드가 종료되었음을 나타낸다.
      - `state` : 현재 상태(state) (`Env._getStateIdx()` 메소드 이용)
      - `reward` : 보상
- line 47 ~ 177 : `DynaQ` 클래스
  - line 48 ~ 64 : `DynaQ.Model` 클래스
    - DynaQ가 사용하는 모델을 나타내는 클래스
    - line 52 ~ 56 : `DynaQ.Model.add()` 메소드
      - 상태 `state`에서 행동 `action`을 수행했을 때, 보상 `reward`를 받고 상태 `next_state`로 전이했다는 정보를 모델에 저장
    - line 58 ~ 59 : `DynaQ.Model.getRS_()` 메소드
      - 가짜 경험(simulated experience)을 생성
      - 상태 `state`에서 행동 `action`을 수행했을 때 받을(것으로 예상되는) 보상 `reward`와 다음 상태(일 것으로 예상되는) `next_state`을 반환
    - line 61 ~ 64 : `DynaQ.Model.getRandomSA()` 메소드
      - 기존에 경험했던 상태-행동 쌍 중 무작위로 하나를 반환
  - line 76 ~ 85 : `DynaQ.chooseAction()` 메소드
    - 현재 상태 `state`를 입력받아, $\varepsilon$-greedy Policy로 다음 행동을 선택하는 메소드
    - `greedy` 매개변수에 True를 주면 Greedy Policy로 동작한다(즉, 탐색(exploration)을 안한다).
  - line 87 ~ 153 : `DynaQ.test()` 메소드
    - 에이전트의 학습 결과(Fig.03)를 출력하는 메소드
  - line 155 ~ 177 : `DynaQ.learn()` 메소드
    - Dyna-Q 알고리즘대로 에이전트의 학습을 진행하는 메소드
    - 각 에피소드마다 목표(G)에 도달하기까지 몇 번의 step이 필요했는지를 저장한 배열 `history`를 반환한다.
- line 179 ~ 189 : `drawGraph()` 함수
  - Fig.04를 그리는 함수
- line 191 ~ 212 : main
  - line 192 : 랜덤 시드 고정
  - line 195 ~ 197 : Fig.03 출력
    - Planning을 30번 수행하는 에이전트를 이용해, 10개의 에피소드로 학습시킨 결과 출력
  - line 199 ~ 212 : Fig.04 출력
    - Planning을 0번(= Direct RL만 수행), 5번, 50번 수행하는 에이전트를 이용해, 30개의 에피소드로 학습하는 과정을 10번 반복 실행하여, 그 결과를 평균하여 그래프로 출력
    - Planning을 하든 안 하든 첫 번째 에피소드에서 G에 도달아끼까지 필요한 step의 수는 같으므로(약 2,000 step) 그래프에서 생략하였다.

</div>

{% include caption-img.html src="09-maze-result1.png" title="Fig.03 Maze - 결과" description="화살표는 각 칸에서 에이전트가 선택한 행동(action)을 나타낸다." %}

{% include caption-img.html src="09-maze-result2.png" title="Fig.04 Maze - Planning 횟수에 따른 필요 step 수" description="Planning을 0번, 5번, 50번 수행하는 에이전트를 이용해, 30개의 에피소드로 학습하는 과정을 10번 반복 실행하여, 그 결과를 평균하여 그래프로 출력하였다. 첫 번째 에피소드의 결과는 생략하였다." %}

Fig.04에서, Planning을 많이 하는 에이전트는 Planning을 조금 하는 에이전트에 비해 적은 수의 에피소드만 가지고도 학습이 잘 되는 것을 볼 수 있다.[^8] 구체적으로, 첫 번째 에피소드에서는 Planning을 하지 않는 에이전트(파란색)와 Planning을 하는 에이전트(주황색, 초록색) 모두에서 오직 한 개 상태(G)의 가치만 업데이트된다.[^9] 하지만 두 번째 에피소드에서 Planning을 하지 않는 에이전트에서는 여전히 오직 한 개 상태(G 바로 직전 상태)의 가치만 업데이트되는 반면, Planning을 하는 에이전트에서는 (모델이 기억하고 있는 정보를 사용해) 여러 개의 상태의 가치가 업데이트된다.

[^8]: 에이전트가 잘 학습되었다는 것은 적은 수의 step을 써서(= 최단경로로) G에 도달했다는 것이다. 
[^9]: 사실 첫 번째 에피소드에서는 모델이 비어있기 때문에, Planning을 했을 때와 안 했을 때의 차이가 없다.

이처럼 Planning은 에피소드 하나로 얻은 정보를 최대한 이용해 학습을 진행하는 것이기에, Planning을 사용하는 에이전트가 Planning을 사용하지 않는 에이전트보다 더 적은 수의 에피소드로도 학습이 잘 된다.

# Dyna+

위 Maze 예제에서, 모델이 담고 있는 정보는 항상 옳았다. 하지만 확률적인(stochastic) 환경에서 충분한 관측이 시행되지 않은 경우, 상태-행동 공간(state-action space)이 너무 커 FA(Function Approximation)를 사용하는 경우, 또는 환경이 변하는 경우 등에서는 모델이 잘못된 정보를 담고 있을 수 있다. 잘못된 정보를 담고 있는 모델을 이용해 Planning을 진행하면 학습이 제대로 이루어지지 않게 된다.

