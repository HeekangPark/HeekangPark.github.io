---
title: "L05. DP(Dynamic Programming)"
order: 5
date: "2021-04-08"
---

# DP(Dynamic Programming)란?

RL에서 DP(Dynamic Programming)란 환경(environment)에 대한 완벽한 모델(model)이 MDP로 주어졌을 때 최적 정책을 계산하기 위해 사용하는 여러 알고리즘들을 뜻하는 말이다.

# DP의 한계

그러나 안타깝게도 (전통적인) DP는 한계가 분명하다.

{:.no-guide-line}
- 환경에 대한 완벽한 모델이 MDP로 주어지지 않은 경우에는 DP를 사용할 수 없다.
- 계산 시간이 엄청나게 많이 필요하다.

이런 이유들 때문에 DP는 RL 문제에 범용적으로 적용할 수는 없다. 그러나 DP는 이론적으로는 많이 사용된다. 또한 많은 RL 풀이법들이 내부적으로 DP를 사용하기 때문에, DP를 이해하지 않고는 이들을 이해할 수 없다. 따라서 DP를 공부하는 것이 중요하다.

# Policy Evaluation

본격적으로 DP에 대해 얘기하기 앞서, 우선 다음을 생각해 보자.

> Finite MDP 문제에서, 임의의 정책 $\pi$가 주어졌을 때 상태-가치 함수(state-value function) $v\_{\pi}$를 어떻게 구할 수 있을까?

이 문제를 Policy Evaluation 또는 Prediction Problem이라 부른다.

[이전 글](/SNU_m3309.000200/04-mdp)에서 살펴보았듯이, $n$개의 상태로 구성된 Finite MDP에서 환경의 Dynamics Function이 완벽히 알려져 있다면 Bellman equation은 $n$개의 변수들[^1]로 이루어진 $n$개의 연립 선형방정식이 된다. 이를 풀기 위해 두 가지 방법을 생각해 볼 수 있다.

[^1]: $v\_{\pi} (s)$들. $s$는 상태를 의미한다(총 $n$개 있다).

## 풀이 1 : 대수학적 기법을 사용하는 방법

첫 번째 방법은 대수학적 기법을 사용하는 방법이다. 우린 선형대수학 등에서 연립 선형방정식을 푸는 다양한 기법들을 학습하였다. 이들을 적절히 사용하면 Bellman equation을 풀 수 있다. 그러나 이 방법은 상태의 수가 많아지면 많이질수록 계산 난이도가 매우 올라간다는 치명적인 단점이 있다. 그래서 Bellman equation을 풀 땐 일반적으로 이 방법보단 다음에 소개할 Iterative policy evaluation이라는 방법을 많이 쓴다.

## 풀이 2 : Iterative policy evaluation

두 번째 방법은 Iterative policy evaluation라 불리는 방법으로서, Bellman equation의 재귀성을 이용해 수치해석적으로(numerically) Bellman equation을 푸는 방법이다. Iterative policy evaluation의 알고리즘은 다음과 같다.

1\. 각 상태 $s$에 대해, $v_0 (s)$를 임의의 값으로 초기화한다.

2\. 다음 업데이트 식을 이용해 반복적으로(iterative) $v_k (s)$를 업데이트한다.[^2]

[^2]: 이런 식의 업데이트 과정을 DP에선 expected update라 부른다. "expected"라는 표현은 이 업데이트 과정이 샘플링(sampling)된 다음 상태 하나만을 보고 업데이트하는 것이 아닌, 가능한 모든 다음 상태들의 기댓값(expectation)을 구한다는 것을 강조한 표현이다.

$$v_{k+1}(s) = \sum_a \pi(a | s) \sum_{s',\,r} p(s',r | s, a)[r + \gamma v_k (s')]$$

3\. 2를 적당한 횟수만큼 반복한다.

위 알고리즘에서 볼 수 있듯이, Iterative policy evaluation을 수행하면 수열 $\{ \,v\_k (s) \, \}$가 생성된다. $k$가 커지면 커질수록 $v\_k (s)$는 $v\_{\pi}(s)$에 점점 수렴한다.

### 구현

Iterative policy evaluation을 구현하는 가장 직관적은 방법은 두 개의 배열을 사용해, 각각 옛 값($v\_k (s)$)와 최신 값($v\_{k+1}(s)$)를 번갈아가며 담는 방법일 것이다.

그런데 배열을 하나만 쓰는 방법도 있다: 배열을 하나만 쓰고, 최신 값이 옛 값을 그냥 덮어씌우게 하는 것이다. 즉 비유하자면 상태 공간(state space)을 훑으며(sweep) 업데이트를 진행하는 것이다. 이 방법 역시 $v\_{\pi}(s)$에 수렴한다. 사실, 많은 경우 이 방법이 오히려 배열을 두 개 사용하는 방법보다 더 빨리 수렴한다.[^3] 대신, 이 방법은 각 상태를 업데이트하는 순서에 따라 수렴 속도가 크게 바뀐다. 일반적으로 DP에서는 이 방법으로 Iterative policy evaluation을 구현한다.

[^3]: 얼핏 생각해 보면 이렇게 할 경우 몇몇 $v\_{k+1} (s)$는 업데이트될 때 $v\_{k}(s')$이 아닌 $v\_{k+1}(s')$을 사용하게 되어 문제가 있을 것 같다. 하지만 $v\_{k} (s)$는 참값이 아닌 근사값이다. $v\_{k+1}(s')$를 이용해 업데이트를 하면 오차가 작은 최신 값을 사용해 업데이트를 한 셈이 되어 배열을 두 개 사용할 때보다 더 빨리 수렴하게 된다.

그리고 또 하나 생각해 봐야 할 것이, 바로 반복을 멈추는 시점이다. 수학적으로는 $k \rightarrow \infty$일 때 $v\_k = v\_\pi$가 되므로 무한 번 반복을 해야겠지만, 당연히 실제 구현할 땐 이렇게 하지 않는다. 반복을 멈추는 시점을 설정하는 방법은 다양한 방법이 있지만, 그 중 하나를 소개하면 다음과 같다.

1. $v\_k (s)$를 업데이트할 때 각 상태별로 최신 값과 옛 값의 차 $\| v_{k+1} (s) - v\_k (s) \|$를 계산해, 그 중 최댓값을 찾는다.
2. 만약 그 최댓값이 특정 값(threshold) 이하로 떨어지면 그만둔다.
