---
title: "L05. DP(Dynamic Programming)"
order: 5
date: "2021-04-08"
---

# DP(Dynamic Programming)란?

알고리즘을 공부해 보았다면 DP(Dynamic Programming, 동적 계획법)에 대해 들어보았을 것이다. 알고리즘에서의 DP는 다음 두 가지 속성을 가지고 있는 문제에 적용 가능한 풀이 방법이다.

{:.no-guide-line}
- Overlapping Subproblem : 문제는 여러 개의 하위 문제(subproblem)으로 쪼갤 수 있다. 이때 하위 문제들은 서로 겹치는(overlapping) 부분들이 있어, 한 하위 문제를 풀 때 사용했던 결과를 재활용할 수 있다(이를 메모이제이션(memoization)이라 한다).[^1]
- Optimal Substructure : 하위 문제의 최적 해(optimal solution)를 이용해 상위 문제의 최적 해를 구할 수 있다.[^2]

[^1]: 예를 들어, $n$번째 피보나치 수를 구하는 문제는 $n-1$번째 피보나치 수를 구하는 하위 문제와 $n-2$번째 피보나치 수를 구하는 하위 문제로 쪼갤 수 있다. 다시 $n-1$번째 피보다치 수를 구하는 문제는 $n-2$번째 피보나치 수를 구하는 하위 문제와 $n-3$번째 피보나치 수를 구하는 하위 문제로 쪼갤 수 있다. 이런 식으로 계속 하위 문제로 쪼개어 나갈 때, 몇몇 부분들이 겹치는 것을 볼 수 있다. 예를 들어, $n-2$번째 피보나치 수를 구하는 하위 문제는 $n$번째 피보나치 수를 구하는 문제의 하위 문제이면서 동시에 $n-1$번째 피보나치 수를 구하는 문제의 하위 문제이다. 이런 경우 동적 계획법에서는 $n-2$번째 피보나치 수를 한 번만 계산해 (어딘가에 저장해 뒀다가) 이를 다시 재활용하여 사용한다.
[^2]: 예를 들어, A 도시에서 B 도시까지 가는 길찾기 문제가 있다고 해 보자. 만약 A 도시와 B 도시 사이 C 도시가 있다면, A 도시에서 B 도시까지 가는 최적 경로에는 A 도시에서 C 도시까지 가는 최적 경로와 C 도시에서 B 도시까지 가는 최적 경로들의 조합이 반드시 포함되어 있을 것이다. 이런 식의 성질을 Optimal Substructure라 한다.

**이때 Finite MDP 문제에서 환경에 대해 완벽한 지식이 주어진 경우, DP를 이용해 최적 정책(optimal policy)을 구할 수 있다.**

그러나 안타깝게도 DP는 한계가 분명하다.

{:.no-guide-line}
- Finite MDP가 아닐 경우 DP를 사용할 수 없다.
- Finite MDP이더라도 환경에 대해 완벽한 지식을 모른다면 DP를 사용할 수 없다.
- 계산 시간이 엄청나게 많이 필요하다.

이런 이유들 때문에 DP를 RL 문제에 범용적으로 적용하긴 어렵다. 그러나 DP는 이론적으로는 많이 사용되고, 또 많은 RL 풀이법들이 내부적으로 DP를 사용하기에 DP를 이해하지 않고는 이들을 이해할 수 없다. 따라서 DP를 공부하는 것이 중요하다.

# Policy Iteration

첫 번째로 배울 DP는 Policy Iteration이라는 방법이다. 이 방법은 가치 함수(value function)을 이용해 최적 정책을 찾는다. 이 방법을 간단히 설명하면 다음과 같다.

1. Initialization : 정책 $\pi$, 상태-가치 함수 $v$를 적당한 값으로 초기화한다.
2. Policy Evaluation : 현재 정책 $\pi$를 적용했을 때 각 상태 $s$의 가치 $v\_{\pi}(s)$를 업데이트한다.
3. Policy Improvement : 업데이트된 가치 $v\_{\pi}(s)$를 바탕으로 현재 정책 $\pi$보다 조금 더 개선된 새로운 정책 $\pi'$를 만든다. 이를 현재 정책으로 삼는다.
4. 2, 3번 과정을 여러 번 반복한다.

위 과정을 수행하면 정책과 가치 함수가 번갈아가며 점점 최적 정책 $\pi\_{\*}$와 최적 가치 $v\_{\*}(s)$에 각각 다가가게 된다. 각 과정을 자세히 살펴보자.

## Policy Evaluation

Policy Evaluation[^3]은 임의의 정책 $\pi$가 주어졌을 때 상태-가치 함수(state-value function) $v\_{\pi}$를 구하는 방법이다.

[^3]: Prediction Problem이라고도 부른다.

우린 현재 환경에 대한 모든 정보를 알고 있으므로 [이전 글](/SNU_m3309.000200/04-mdp)에서 살펴본 Bellman equation을 세울 수 있다. 전체 상태들의 집합을 $\mathcal{S}$라 할 때, Bellman equation은 $\|\mathcal{S}\|$개의 변수들[^4]로 이루어진 $\|\mathcal{S}\|$개의 연립 선형방정식 형태로 세워진다.

[^4]: $v\_{\pi} (s)$들 ($s \in \mathcal{S}$)

선형대수학 등에서 학습한 연립 선형방정식을 푸는 다양한 기법들을 활용하면 이를 풀 수 있다. 하지만 이 방법은 상태의 수가 많아지면 많이질수록 계산 난이도가 매우 올라간다는 치명적인 단점이 있다. 그래서 일반적으론 Bellman equation을 풀 때 이 방법보다 **Iterative policy evaluation**이라는 방법을 많이 쓴다.

Iterative policy evaluation은 Bellman equation의 재귀성을 이용해 수치해석적으로(numerically) Bellman equation을 푸는 방법이다. Iterative policy evaluation의 알고리즘은 다음과 같다.

{:.mb-0}
1\. 각 상태 $s$에 대해, $v_0 (s)$를 임의의 값으로 초기화한다.

{:.mb-0}
2\. 다음 업데이트 식을 이용해 반복적으로(iterative) $v_k (s)$를 업데이트한다.[^5]

[^5]: 이런 식의 업데이트 과정을 DP에선 expected update라 부른다. "expected"라는 표현은 이 업데이트 과정이 샘플링(sampling)된 다음 상태 하나만을 보고 업데이트하는 것이 아닌, 가능한 모든 다음 상태들의 기댓값(expectation)을 구한다는 것에서 나온 표현이다.

$$v_{k+1}(s) = \sum_a \pi(a | s) \sum_{s',\,r} p(s',r | s, a)[r + \gamma v_k (s')]$$

3\. 2를 적당한 횟수만큼 반복한다.

Iterative policy evaluation을 수행하면 수열 $\\{ \,v\_k (s) \, \\}$가 생성된다. $k$가 커지면 커질수록 $v\_k (s)$는 $v\_{\pi}(s)$에 점점 수렴한다.

<style>
.mb-0 {
    margin-bottom: 0 !important;
}
</style>

### 여담: Iterative policy evaluation 구현하기

Iterative policy evaluation을 구현하는 가장 직관적은 방법은 두 개의 배열을 사용해, 각각 옛 값($v\_k (s)$)와 최신 값($v\_{k+1}(s)$)를 번갈아가며 담는 방법일 것이다.

그런데 배열을 하나만 쓰는 방법도 있다: 배열을 하나만 쓰고, 최신 값이 옛 값을 그냥 덮어씌우게 하는 것이다. 이렇게 하면 상태 공간(state space)을 훑으며(sweep) 업데이트가 진행된다. 이 방법 역시 $v\_{\pi}(s)$에 수렴한다. 사실, 많은 경우 이 방법이 오히려 배열을 두 개 사용하는 방법보다 더 빨리 수렴한다.[^6] 대신, 이 방법은 각 상태를 업데이트하는 순서에 따라 수렴 속도가 크게 바뀐다. 일반적으론 배열을 두 개 사용하는 방법보다 이 방법으로 Iterative policy evaluation을 구현한다.

[^6]: 얼핏 생각해 보면 이렇게 할 경우 몇몇 $v\_{k+1} (s)$는 업데이트될 때 $v\_{k}(s')$이 아닌 $v\_{k+1}(s')$을 사용하게 되어 문제가 있을 것 같다. 하지만 $v\_{k} (s)$는 참값이 아닌 근사값임을 기억하자. $v\_{k+1}(s')$를 이용해 업데이트를 하면 오차가 작은 최신 값을 사용해 업데이트를 한 셈이 되어 배열을 두 개 사용할 때보다 더 빨리 수렴하게 된다.

그리고 또 하나 생각해 봐야 할 것이, 바로 반복을 멈추는 시점이다. 수학적으로는 $k \rightarrow \infty$일 때 $v\_k = v\_\pi$가 되므로 무한 번 반복을 해야겠지만, 당연히 실제 구현할 땐 이렇게 하지 않는다. 반복을 멈추는 시점을 설정하는 다양한 방법 중 하나를 소개하면 다음과 같다.

1. $v\_k (s)$를 업데이트할 때 각 상태별로 최신 값과 옛 값의 차 $\| v_{k+1} (s) - v\_k (s) \|$를 계산해, 그 중 최댓값을 찾는다.
2. 만약 그 최댓값이 특정 값(threshold) 이하로 떨어지면 그만둔다.

## Policy Improvement

Policy Improvement는 가치 $v\_{\pi}(s)$를 바탕으로 현재 정책 $\pi$보다 조금 더 개선된 새로운 정책 $\pi'$를 만드는 방법을 뜻한다.

우리는 상태 $s$에서 현재 정책 $\pi$를 따르는 것이 얼마나 좋은지를 나타내는 값을 이미 배웠다. $v\_{\pi}$가 바로 그 값이다. 이때, 어떤 상태 $s$에서 특정 행동 $a \neq \pi(s)$에 대해

$$q_{\pi}(s, a) > v_{\pi}(s)$$

가 성립한다고 해 보자. 말로 풀어 쓰면, 상태 $s$에서 ($\pi(s)$가 아닌) 행동 $a$를 시행하고 이후 $\pi$를 따르는 것이 계속 $\pi$를 따르는 것보다 낫다는 것이다. 그렇다면 상태 $s$에서는 행동 $a$를 선택하고 나머지 상태에 대해서는 $\pi$와 똑같은, 새로운 정책 $\pi'$을 생각할 수 있다. 이 새로운 정책 $\pi'$는 기존 정책 $\pi$보다 더 나은 정책이라 할 수 있다.

이를 일반화하면 다음과 같이 된다.

{:#policy-improvement-theorem}
> {:.title}
> Policy Improvement Theorem
> 
> 정책 $\pi$와, $\pi$와 동일하지만 특정 상태 $s$에서만 다른(즉, $\pi'(s) \neq \pi(s)$) 정책 $\pi'$가 있다고 하자. 모든 상태 $s \in \mathcal{S}$에 대해
> 
> $$q_{\pi} (s,\,\pi'(s)) \ge v_{\pi}(s)$$
> 
> 가 성립하면, 정책 $\pi'$는 정책 $\pi$보다 낫거나 같은 정책이다. 다시 말해, 모든 상태 $s \in \mathcal{S}$에 대해 다음이 성립한다.
> 
> {:.mb-0}
> $$v_{\pi'}(s) \ge v_{\pi}$$

<div class="folder" style="margin-bottom: 1em;">
<p class="folder-btn folder-open-btn">증명 열기</p>
<p class="folder-btn folder-close-btn hidden">증명 닫기</p>

<div class="folder-content hidden proof mb-0" markdown="block">

pf)

$$\begin{align}
v_{\pi}(s) 
&\le q_{\pi}(s,\,\pi'(s))\\[0.5em]
&=\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s,\,A_t = \pi'(s)]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1},\,\pi'(S_{t+1}))\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) \,|\,S_{t+1},\,A_{t+1} = \pi'(S_{t+1})]\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+3})\,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+4})\,|\,S_t = s]\\[0.5em]
&\qquad\qquad\qquad\qquad\qquad\vdots
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots \,|\,S_t = s]\\[0.5em]
&= v_{\pi'}(s)
\end{align}
$$

</div>
</div>

<style>
#policy-improvement-theorem .title {
    font-size: 1.2em;
    font-weight: bold;
    margin-bottom: 1em;
}

#policy-improvement-theorem .mb-0 mjx-container {
    margin-bottom: 0 !important;
}

.proof {
    border: 1px solid #888888;
    border-radius: 0.5em;
    padding: 1em;
    margin-bottom: 1em;
}

.document .document-content .folder .hidden {
    display: none;
}

.document .document-content .folder .folder-btn {
    text-align: center;
    color: #0275d8;
    cursor: pointer;
}

.document .document-content .folder .folder-btn:hover {
    text-decoration: underline;
}
</style>

<script>
$(".document .document-content .folder .folder-btn.folder-open-btn").click(function() {
    let open_btn = $(this);
    let close_btn = $(this).siblings(".folder-close-btn");
    let content = $(this).siblings(".folder-content");

    open_btn.addClass("hidden");
    close_btn.removeClass("hidden");
    content.removeClass("hidden");
});

$(".document .document-content .folder .folder-btn.folder-close-btn").click(function() {
    let open_btn = $(this).siblings(".folder-open-btn");
    let close_btn = $(this);
    let content = $(this).siblings(".folder-content");

    open_btn.removeClass("hidden");
    close_btn.addClass("hidden");
    content.addClass("hidden");
});
</script>

이제 얘기를 조금 더 확장해, 하나의 상태에서만 개선된 정책이 아닌 전체 상태에 대해 더 개선된 정책 $\pi''(s)$을 생각해 보자. 