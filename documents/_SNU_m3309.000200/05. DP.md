---
title: "L05. DP(Dynamic Programming)"
order: 5
date: "2021-04-08"
---

# DP(Dynamic Programming)란?

알고리즘을 공부해 보았다면 DP(Dynamic Programming, 동적 계획법)에 대해 들어보았을 것이다. 알고리즘에서의 DP는 다음 두 가지 속성을 가지고 있는 문제에 적용 가능한 풀이 방법이다.

{:.no-guide-line}
- Overlapping Subproblem : 문제는 여러 개의 하위 문제(subproblem)으로 쪼갤 수 있다. 이때 하위 문제들은 서로 겹치는(overlapping) 부분들이 있어, 한 하위 문제를 풀 때 사용했던 결과를 재활용할 수 있다(이를 메모이제이션(memoization)이라 한다).[^1]
- Optimal Substructure : 하위 문제의 최적 해(optimal solution)를 이용해 상위 문제의 최적 해를 구할 수 있다.[^2]

[^1]: 예를 들어, $n$번째 피보나치 수를 구하는 문제는 $n-1$번째 피보나치 수를 구하는 하위 문제와 $n-2$번째 피보나치 수를 구하는 하위 문제로 쪼갤 수 있다. 다시 $n-1$번째 피보다치 수를 구하는 문제는 $n-2$번째 피보나치 수를 구하는 하위 문제와 $n-3$번째 피보나치 수를 구하는 하위 문제로 쪼갤 수 있다. 이런 식으로 계속 하위 문제로 쪼개어 나갈 때, 몇몇 부분들이 겹치는 것을 볼 수 있다. 예를 들어, $n-2$번째 피보나치 수를 구하는 하위 문제는 $n$번째 피보나치 수를 구하는 문제의 하위 문제이면서 동시에 $n-1$번째 피보나치 수를 구하는 문제의 하위 문제이다. 이런 경우 동적 계획법에서는 $n-2$번째 피보나치 수를 한 번만 계산해 (어딘가에 저장해 뒀다가) 이를 다시 재활용하여 사용한다.
[^2]: 예를 들어, A 도시에서 B 도시까지 가는 길찾기 문제가 있다고 해 보자. 만약 A 도시와 B 도시 사이 C 도시가 있다면, A 도시에서 B 도시까지 가는 최적 경로에는 A 도시에서 C 도시까지 가는 최적 경로와 C 도시에서 B 도시까지 가는 최적 경로들의 조합이 반드시 포함되어 있을 것이다. 이런 식의 성질을 Optimal Substructure라 한다.

**이때 Finite MDP 문제에서 환경에 대해 완벽한 지식이 주어진 경우, DP를 이용해 최적 정책(optimal policy)을 구할 수 있다.**

그러나 안타깝게도 DP는 한계가 분명하다.

{:.no-guide-line}
- Finite MDP가 아닐 경우 DP를 사용할 수 없다.
- Finite MDP이더라도 환경에 대해 완벽한 지식을 모른다면 DP를 사용할 수 없다.
- 계산 시간이 엄청나게 많이 필요하다.

이런 이유들 때문에 DP를 RL 문제에 범용적으로 적용하긴 어렵다. 그러나 DP는 이론적으로는 많이 사용되고, 또 많은 RL 풀이법들이 내부적으로 DP를 사용하기에 DP를 이해하지 않고는 이들을 이해할 수 없다. 따라서 DP를 공부하는 것이 중요하다.

# Policy Iteration

첫 번째로 배울 DP는 Policy Iteration이라는 방법이다. 이 방법은 가치 함수(value function)을 이용해 최적 정책을 찾는다. 이 방법을 간단히 설명하면 다음과 같다.

1. Initialization : 정책 $\pi$, 상태-가치 함수 $v$를 적당한 값으로 초기화한다.
2. Policy Evaluation : 현재 정책 $\pi$를 적용했을 때 각 상태 $s$의 가치 $v\_{\pi}(s)$를 업데이트한다.
3. Policy Improvement : 업데이트된 가치 $v\_{\pi}(s)$를 바탕으로 현재 정책 $\pi$보다 조금 더 개선된 새로운 정책 $\pi'$를 만든다. 이를 현재 정책으로 삼는다.
4. 2, 3번 과정을 여러 번 반복한다.

위 과정을 수행하면 정책과 가치 함수가 번갈아가며 점점 최적 정책 $\pi\_{\*}$와 최적 가치 $v\_{\*}(s)$에 각각 다가가게 된다. 즉 다음과 같은 시퀸스가 형성된다.

$$\pi_0 \textcolor{red}{\rightarrow} v_{\pi_0} \textcolor{blue}{\rightarrow}
\pi_1 \textcolor{red}{\rightarrow} v_{\pi_1} \textcolor{blue}{\rightarrow}
\pi_2 \textcolor{red}{\rightarrow} v_{\pi_2} \textcolor{blue}{\rightarrow}
\cdots \textcolor{blue}{\rightarrow}
\pi_* \textcolor{red}{\rightarrow} v_{*}
$$

여기서 빨간 화살표는 Policy Evaluation을, 파란 화살표는 Policy Improvement를 의미한다. 각 과정을 자세히 살펴보자.

## Policy Evaluation

Policy Evaluation[^3]은 임의의 정책 $\pi$가 주어졌을 때 상태-가치 함수(state-value function) $v\_{\pi}$를 구하는 방법이다.

[^3]: Prediction Problem이라고도 부른다.

우린 현재 환경에 대한 모든 정보를 알고 있으므로 [이전 글](/SNU_m3309.000200/04-mdp)에서 살펴본 Bellman equation을 세울 수 있다. 전체 상태들의 집합을 $\mathcal{S}$라 할 때, Bellman equation은 $\|\mathcal{S}\|$개의 변수들[^4]로 이루어진 $\|\mathcal{S}\|$개의 연립 선형방정식 형태로 세워진다.

[^4]: $v\_{\pi} (s)$들 ($s \in \mathcal{S}$)

선형대수학 등에서 학습한 연립 선형방정식을 푸는 다양한 기법들을 활용하면 이를 풀 수 있다. 하지만 이 방법은 상태의 수가 많아지면 많이질수록 계산 난이도가 매우 올라간다는 치명적인 단점이 있다. 그래서 일반적으론 Bellman equation을 풀 때 이 방법보다 **Iterative policy evaluation**이라는 방법을 많이 쓴다.

Iterative policy evaluation은 Bellman equation의 재귀성을 이용해 수치해석적으로(numerically) Bellman equation을 푸는 방법이다. Iterative policy evaluation의 알고리즘은 다음과 같다.

{:.mb-0}
1\. 각 상태 $s$에 대해, $v_0 (s)$를 임의의 값으로 초기화한다.

{:.mb-0}
2\. 다음 업데이트 식을 이용해 반복적으로(iterative) 각 상태 $s$마다 $v_k (s)$를 업데이트한다.[^5]

[^5]: 이런 식의 업데이트 과정을 DP에선 expected update라 부른다. "expected"라는 표현은 이 업데이트 과정이 샘플링(sampling)된 다음 상태 하나만을 보고 업데이트하는 것이 아닌, 가능한 모든 다음 상태들의 기댓값(expectation)을 구한다는 것에서 나온 표현이다.

$$v_{k+1}(s) = \sum_a \pi(a | s) \sum_{s',\,r} p(s',r | s, a)[r + \gamma v_k (s')]$$

3\. 2를 적당한 횟수만큼 반복한다.

Iterative policy evaluation을 수행하면 수열 $\\{ \,v\_k (s) \, \\}$가 생성된다. $k$가 커지면 커질수록 $v\_k (s)$는 $v\_{\pi}(s)$에 점점 수렴한다.

<style>
.mb-0 {
    margin-bottom: 0 !important;
}
</style>

### 여담: Iterative policy evaluation 구현하기

Iterative policy evaluation을 구현하는 가장 직관적은 방법은 두 개의 배열을 사용해, 각각 옛 값($v\_k (s)$)와 최신 값($v\_{k+1}(s)$)를 번갈아가며 담는 방법일 것이다.

그런데 배열을 하나만 쓰는 방법도 있다: 배열을 하나만 쓰고, 최신 값이 옛 값을 그냥 덮어씌우게 하는 것이다. 이렇게 하면 상태 공간(state space)을 훑으며(sweep) 업데이트가 진행된다. 이 방법 역시 $v\_{\pi}(s)$에 수렴한다. 사실, 많은 경우 이 방법이 오히려 배열을 두 개 사용하는 방법보다 더 빨리 수렴한다.[^6] 대신, 이 방법은 각 상태를 업데이트하는 순서에 따라 수렴 속도가 크게 바뀐다. 일반적으론 배열을 두 개 사용하는 방법보다 이 방법으로 Iterative policy evaluation을 구현한다.

[^6]: 얼핏 생각해 보면 이렇게 할 경우 몇몇 $v\_{k+1} (s)$는 업데이트될 때 $v\_{k}(s')$이 아닌 $v\_{k+1}(s')$을 사용하게 되어 문제가 있을 것 같다. 하지만 $v\_{k} (s)$는 참값이 아닌 근사값임을 기억하자. $v\_{k+1}(s')$를 이용해 업데이트를 하면 오차가 작은 최신 값을 사용해 업데이트를 한 셈이 되어 배열을 두 개 사용할 때보다 더 빨리 수렴하게 된다.

그리고 또 하나 생각해 봐야 할 것이, 바로 반복을 멈추는 시점이다. 수학적으로는 $k \rightarrow \infty$일 때 $v\_k = v\_\pi$가 되므로 무한 번 반복을 해야겠지만, 당연히 실제 구현할 땐 이렇게 하지 않는다. 반복을 멈추는 시점을 설정하는 다양한 방법 중 하나를 소개하면 다음과 같다.

1. $v\_k (s)$를 업데이트할 때 각 상태별로 최신 값과 옛 값의 차 $\| v_{k+1} (s) - v\_k (s) \|$를 계산해, 그 중 최댓값을 찾는다.
2. 만약 그 최댓값이 특정 값(threshold) 이하로 떨어지면 그만둔다.

## Policy Improvement

우리는 [이전 글](/SNU_m3309.000200/04-mdp#kramdown_정책policy과-가치-함수value-function)에서 $v\_{\pi}$가 상태 $s$에서 현재 정책 $\pi$를 따르는 것이 얼마나 좋은지를 나타내는 값임을 배웠다. 어떤 임의의 결정론적 정책(deterministic policy) $\pi$가 있다 해 보자. 이때, 어떤 상태 $s$에서 특정 행동 $a \neq \pi(s)$에 대해

$$q_{\pi}(s, a) > v_{\pi}(s)$$

가 성립한다고 해 보자.[^7] 그렇다면 상태 $s$에서는 행동 $a$를 선택하고 나머지 상태에 대해서는 $\pi$와 똑같은 선택을 하는, 새로운 정책 $\pi'$는 기존 정책 $\pi$보다 더 나은 정책이라 할 수 있다.

[^7]: 이를 말로 풀어 쓰면 다음과 같다: 상태 $s$에서 ($\pi(s)$가 아닌) 행동 $a$를 시행하고 이후 $\pi$를 따르는 것이, 계속 $\pi$를 따르는 것보다 낫다.

이를 일반화하면 다음과 같이 된다.

{:#policy-improvement-theorem}
> {:.title}
> Policy Improvement Theorem
> 
> 결정론적 정책 $\pi$와, $\pi$와 동일하지만 특정 상태 $s$에서만 다른(즉, $\pi'(s) \neq \pi(s)$) 결정론적 정책 $\pi'$가 있다고 하자. 모든 상태 $s \in \mathcal{S}$에 대해
> 
> $$q_{\pi} (s,\,\pi'(s)) \ge v_{\pi}(s)$$
> 
> 가 성립하면, 정책 $\pi'$는 정책 $\pi$보다 낫거나 같은 정책이다. 다시 말해, 모든 상태 $s \in \mathcal{S}$에 대해 다음이 성립한다.
> 
> {:.mb-0}
> $$v_{\pi'}(s) \ge v_{\pi}$$

<div class="folder" style="margin-bottom: 1em;">
<p class="folder-btn folder-open-btn">증명 열기</p>
<p class="folder-btn folder-close-btn hidden">증명 닫기</p>

<div class="folder-content hidden proof mb-0" markdown="block">

pf)

$$\begin{align}
v_{\pi}(s) 
&\le q_{\pi}(s,\,\pi'(s))\\[0.5em]
&=\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s,\,A_t = \pi'(s)]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1},\,\pi'(S_{t+1}))\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) \,|\,S_{t+1},\,A_{t+1} = \pi'(S_{t+1})]\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+3})\,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+4})\,|\,S_t = s]\\[0.5em]
&\qquad\qquad\qquad\qquad\qquad\vdots
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots \,|\,S_t = s]\\[0.5em]
&= v_{\pi'}(s)
\end{align}
$$

</div>
</div>

<style>
#policy-improvement-theorem .title {
    font-size: 1.2em;
    font-weight: bold;
    margin-bottom: 1em;
}

#policy-improvement-theorem .mb-0 mjx-container {
    margin-bottom: 0 !important;
}

.proof {
    border: 1px solid #888888;
    border-radius: 0.5em;
    padding: 1em;
    margin-bottom: 1em;
}

.document .document-content .folder .hidden {
    display: none;
}

.document .document-content .folder .folder-btn {
    text-align: center;
    color: #0275d8;
    cursor: pointer;
}

.document .document-content .folder .folder-btn:hover {
    text-decoration: underline;
}
</style>

<script>
$(".document .document-content .folder .folder-btn.folder-open-btn").click(function() {
    let open_btn = $(this);
    let close_btn = $(this).siblings(".folder-close-btn");
    let content = $(this).siblings(".folder-content");

    open_btn.addClass("hidden");
    close_btn.removeClass("hidden");
    content.removeClass("hidden");
});

$(".document .document-content .folder .folder-btn.folder-close-btn").click(function() {
    let open_btn = $(this).siblings(".folder-open-btn");
    let close_btn = $(this);
    let content = $(this).siblings(".folder-content");

    open_btn.removeClass("hidden");
    close_btn.addClass("hidden");
    content.addClass("hidden");
});
</script>

이제 얘기를 조금 더 확장해, 하나의 상태에서만 개선된 정책이 아닌 전체 상태에 대해 더 개선된 정책 $\pi''(s)$을 생각해 보자. 즉, 모든 상태에 대해 $q\_{\pi} (s,\,a)$를 참고해 가장 최고의 행동을 탐욕적으로(greedy) 뽑는 것이다. 이를 수식으로 나타내면 다음과 같다.

$$\begin{align}
\pi''(s)
&= \underset{a}{\operatorname{arg max}} q_{\pi} (s,\,a)\\[0.5em]
&= \underset{a}{\operatorname{arg max}} \mathbb{E}[R_{t+1} + \gamma v_{\pi} (S_{t+1}) \,|\,S_t = s,\,A_t = a]\\[0.5em]
&= \underset{a}{\operatorname{arg max}} \sum_{s'\,r} p(s',\,r\,|\,s,\,a) [r + \gamma v_{\pi} (s')]\\[0.5em]
\end{align}
$$

$\pi''(s)$는 Policy Improvement Theorem을 만족하므로 기존 정책 $\pi(s)$보다 항상 좋거나 같은 정책이 된다. **이처럼 가치 함수를 토대로 기존 정책보다 더 개선된 정책 $\pi''(s)$를 찾는 과정을 Policy Improvement라 한다.**

만약 Policy Improvement로 만든 새로운 정책 $\pi''(s)$가 기존 정책 $\pi(s)$와 같은 정도로 좋다고 해 보자. 즉, $v\_{\pi} = v\_{\pi''}$라 해 보자. 이 경우 $v\_{\pi''}$을 다음과 같이 전개할 수 있다.

$$\begin{align}
v\_{\pi''}(s)
&= \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{\pi''}(S_{t+1})\,|\,S_t = s,\, A_t = a]\\[0.5em]
&= \max_{a} \sum_{s'\,r} p(s',\,r\,|\,s,\,a) [r + \gamma v_{\pi''} (s')]\\[0.5em]
\end{align}
$$

이때, 위 식의 형태는 [이전 글](/SNU_m3309.000200/04-mdp#kramdown_bellman-optimality-equation)에서 다룬 Bellman optimality equation과 같음을 볼 수 있다. Finite MDP 문제에서는 Bellman optimality equation의 해가 유일하므로, $v\_{\pi} = v\_{\pi''} = v\_{\*}$가 된다. 다시 말해, Policy Improvement는 기존 정책이 최적 정책이 아닌 한 무조건 기존 정책보다 개선된 정책을 만들어 낸다.

이때까지의 우리의 논의는 모두 주어진 정책이 결정론적 정책일 때의 얘기였다. 그런데 Policy Improvement는 확률론적 정책(stochastic policy)으로도 쉽게 확장 가능하다. Policy Improvement Theorem은 확률론적 정책에 대해서도 적용된다. 만약 탐욕적인 행동($q\_{\pi}$를 최대로 만드는 행동)이 여러 개 있을 경우[^8] 확률론적 정책에서는 이들에 모두 0이 아닌 확률을 부여한다.

[^8]: 결정론적 정책에서는 이런 경우 여러 행동 중 하나를 랜덤하게 고른다(어떤 것을 골라도 최적 행동이 된다).

## 구현

Policy Iteration에 대한 의사 코드(pseudo code)는 다음과 같다.

<div class="pseudo-code-block">
<p class="title">Policy Iteration (Iterative Policy Evaluation 사용)</p>
<div class="code" markdown="block">

1\. Initialization

<span class="indent"/>모든 $s \in \mathcal{S}$에 대해 $V(s) \in \mathbb{R}$과 $\pi(s) \in \mathcal{A}(s)$를 임의의 값으로 초기화한다.

{:.mt-1}
2\. (Iterative) Policy Evaluation

<span class="indent"/>Loop:

<span class="indent"/><span class="indent"/>$\Delta \leftarrow 0$

<span class="indent"/><span class="indent"/>Loop for each $s \in \mathcal{S}$:

<span class="indent"/><span class="indent"/><span class="indent"/>$v \leftarrow V(s)$

<span class="indent"/><span class="indent"/><span class="indent"/>$V(s) \leftarrow \sum\_{s',\,r}p(s',\,r\,\|\,s,\,\pi(s))[r + \gamma V(s')]$

<span class="indent"/><span class="indent"/><span class="indent"/>$\Delta \leftarrow \max(\Delta, \|v - V(s)\|)$

<span class="indent"/><span class="indent"/>until $\Delta < \theta$ (단, $\theta$는 추정(estimation)의 정확도를 결정하는 작은 양의 실수)

{:.mt-1}
3\. Policy Improvement

<span class="indent"/>policy_stable $\leftarrow$ true

<span class="indent"/>For each $s \in \mathcal{S}$:

<span class="indent"/><span class="indent"/>old_action $\leftarrow \pi(s)$

<span class="indent"/><span class="indent"/>$\pi(s) \leftarrow \underset{a}{\operatorname{arg max}} \sum_{s'\,r} p(s',\,r\,\|\,s,\,a) [r + \gamma v_{\pi} (s')]$

<span class="indent"/><span class="indent"/>If old_action $\neq \pi(s)$:

<span class="indent"/><span class="indent"/><span class="indent"/>policy_stable $\leftarrow$ false

<span class="indent"/>If policy_stable:

<span class="indent"/><span class="indent"/>Return $V$ ($V \approx v\_{\*}$)

<span class="indent"/>else:

<span class="indent"/><span class="indent"/>Goto 2. Policy Evaluation

</div>
</div>

<style>
.pseudo-code-block {
    /*font-family: "Ubuntu Mono", "Nanum Gothic Coding", monospace;*/
}

.pseudo-code-block p {
    margin-bottom: 0 !important;
}

.pseudo-code-block .title{
    background-color: #181a26;
    color: #f8f8f2;
    padding-top: 0.5em;
    padding-bottom: 0.5em;
    padding-left: 1em;
    padding-right: 1em;
    border-top-left-radius: 0.5em;
    border-top-right-radius: 0.5em;
    margin-bottom: 0 !important;
    border-bottom: 1px solid #f8f8f2;
}

.pseudo-code-block .code {
    width: 100%;
    box-sizing: border-box;
    overflow-x: auto;
    background-color: #282a36;
    color: #f8f8f2;
    padding: 1em;
    border-bottom-left-radius: 0.5em;
    border-bottom-right-radius: 0.5em;
    line-height: 1.6
    z-index: 3;
    white-space: nowrap;
}

.pseudo-code-block .code .indent {
    padding-left: 2em;
}

.pseudo-code-block .code .mt-1 {
    margin-top: 1em;
}
</style>

# Value Iteration

Policy Iteration의 문제점 중 하나는 계산 비용이 너무 많이 든다는 것이다. Policy Iteration에서는 매 반복마다 Policy Evaluation에서 $v\_{\pi}$로 수렴할 때까지 시간이 오래 걸리기 때문이다. Value Iteration은 바로 여기에서 착안한 DP 기법이다. Value Iteration은 다음 업데이트 식을 이용해 각 상태마다 딱 한번만 가치 함수를 업데이트한다.[^9]

[^9]: (Iterative) Policy Evaluation에서는 아래 업데이트 식에서 $\max_a$ 연산이 없었다.

$$\begin{align}
v_{k+1}(s)
&= \max_a \mathbb{E} [R_{t+1} + \gamma v_k (S_{t+1}) \,|\,S_t = s,\, A_t = a]\\[0.5em]
&= \max_a \sum_{s',\,r} p(s',\,r \,|\,s,\,a)[r + \gamma v_k (s')]\\[0.5em]
\end{align}
$$

이렇게 해도 $\\{\, v\_k \, \\}$는 반드시 $v\_{\*}$로 수렴하게 된다.[^10] 사실 이 업데이트 식은 Bellman optimality equation을 변형한 식이다. 이 식을 이용해 상태공간을 한 번 훑을(sweep) 때마다 Policy Evaluation과 Policy Improvement을 한 번씩 수행하는 꼴이 된다.

[^10]: 물론 당연히 $v\_{\*}$가 존재해야 한다.

Value Iteration 역시 Policy Evaluation에서와 마찬가지로 이론적으로는 무한 번 반복해야 $v\_{\*}$로 수렴하게 된다. 그래서 현실적으론 [위 문단](#kramdown_여담-iterative-policy-evaluation-구현하기)에서 설명한 것처럼 가치의 변화량이 일정 값($\theta$) 이하로 떨어지면 반복을 멈춘다.
## 구현

Value Iteration을 의사 코드로 나타내면 다음과 같다.

<div class="pseudo-code-block">
<p class="title">Value Iteration</p>
<div class="code" markdown="block">

모든 $s \in \mathcal{S}$에 대해 $V(s) \in \mathbb{R}$를 적절한 값으로 초기화한다. 단, 종료 상태의 가치는 0으로 초기화한다.

{:.mt-1}
Loop:

<span class="indent"/>$\Delta \leftarrow 0$

<span class="indent"/>Loop for each $s \in \mathcal{S}$:

<span class="indent"/><span class="indent"/>$v \leftarrow V(s)$

<span class="indent"/><span class="indent"/>$V(s) \leftarrow \max\_{a}\sum\_{s',\,r}p(s',\,r\,\|\,s,\,\pi(s))[r + \gamma V(s')]$

<span class="indent"/><span class="indent"/>$\Delta \leftarrow \max(\Delta, \|v - V(s)\|)$

until $\Delta < \theta$ (단, $\theta$는 추정(estimation)의 정확도를 결정하는 작은 양의 실수)

{:.mt-1}
Return $\pi(s) = \underset{a}{\operatorname{arg max}} \sum_{s'\,r} p(s',\,r\,\|\,s,\,a) [r + \gamma v_{\pi} (s')]$

</div>
</div>