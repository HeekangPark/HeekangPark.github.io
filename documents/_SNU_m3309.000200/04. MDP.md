---
title: "L04. MDP(Markov Decision Process)"
order: 4
date: "2021-03-26"
---

# 에이전트와 환경의 관계

[이전 글](/SNU_m3309.000200/03-mab)에서 보았던 k-armed Bandit Problem의 경우 에이전트가 행동을 할 때마다 보상이 발생했다. 그러나 그 과정에서 에이전트를 둘러싸고 있는 환경(environment)의 상태(state)가 변하지는 않았다.

하지만 일반적인 RL 문제의 경우 에이전트가 행동을 수행하면 보상이 발생할 뿐만 아니라 환경의 상태가 변한다. 그래서 에이전트는 상태를 고려하여 다음 행동을 결정해야 한다.[^1] 일반적으로, 매 시간 간격(time step) $t=0,\,1,\,2,\,\cdots$마다[^2] 에이전트는 환경의 상태를 나타내는 $S\_t \in \mathcal{S}$를 토대로 행동 $A\_t \in \mathcal{A}(s)$[^3]를 결정해 수행하고, 그 결과 보상 $R\_{t+1} \in \mathcal{R} \subset \mathbb{R}$과 새로운 상태값 $S\_{t+1}$을 받는다[^4]. 이를 반복하면 다음과 같은 시퀸스를 얻을 수 있다.[^5]

[^1]: k-armed Bandit Problem에서는 $t=1$, $t=2$, …일 때 선택할 수 있는 행동의 종류가 모두 같았다(항상 $k$개의 옵션 중 하나를 선택할 수 있었다). 즉 k-armed Bandit Problem의 경우 행동을 선택할 때 아무런 값도 고려할 필요가 없었다(이전 보상을 바탕으로 다음 행동을 선택하는 것은 '보상을 최대화한다'는 목적을 위해서였지, 무슨 제약사항이 있어서 한 것은 아니었다). 그러나 일반적인 RL 문제의 경우 환경의 상태에 따라 특정 행동은 선택 불가능해질 수도 있다. 즉 상태는 선택 가능한 행동의 집합 $\mathcal{A}(s)$를 결정한다고 할 수 있다.
[^2]: 이 글에서 시간 간격을 이산적으로(discrete) 놓은 이유는 논의를 단순화하기 위함이다. 당연히 RL에서는 이산적인 시간뿐만 아니라 연속적인 시간도 다룬다. 그러나 연속적인 시간에서의 RL 문제에도 이산적인 시간에서의 RL을 다룰 떄의 아이디어를 확장해 사용할 수 있는 경우가 많다.
[^3]: 만약 모든 상태에서 선택할 수 있는 행동의 집합이 동일하다면(즉 $\mathcal{A}(s)$가 항등함수이면) 단순히 $\mathcal{A}$라 쓰기도 한다.
[^4]: 행동 $A\_t$의 결과가 $R\_{t+1}$과 $S\_{t+1}$임에 주목하라. 시간 $t$에서의 행동의 결과가 시간 $t+1$에서의 보상과 상태로 나온다. 이 표기법을 사용하면 보상과 상태가 상호 결정적인(jointly determined) 값임을 강조할 수 있기에 많이 사용된다.
[^5]: 보상 $R$과 상태 $S$의 순서는 바뀌어도 된다.

$$S_0,\,A_0,\,R_1,\,S_1,\,A_1,\,R_2,\,S_2,\,A_2,\,\cdots$$

보면 알겠지만, $t=0$일 때를 제외하곤 "RSA" 순서로 나온다는 것을 기억하면 헷갈리지 않을 것이다.

# MDP (Markov Decision Process)

어떤 RL 문제에서, 확률변수 $R\_t$와 $S\_t$가 오직 직전 상태 $S\_{t-1}$와 행동 $A\_{t-1}$에만 의존한 이산 확률 분포(discrete probability distribution)를 따른다면 이 문제는 **Markov 속성(Markov property)**가 있다고 한다.[^6] 그리고 이렇게 Markov 속성을 가지고 있는 RL 문제를 **MDP(Markov Decision Process)**라 한다. 그리고 MDP 중 상태들의 집합 $\mathcal{S}$, 행동들의 집합 $\mathcal{A}$, 보상들의 집합 $\mathcal{R}$이 모두 유한집합인 MDP를 **Finite MDP**라 한다.

[^6]: 즉, 과거의 상태($S\_{t-2}$, $S\_{t-3}$, …) 및 행동($A\_{t-2}$, $A\_{t-3}$, …)에는 영향을 받지 않고, 오직 바로 직전 상태($S\_{t-1}$) 및 행동($A\_{t-1}$)에만 영향을 받는다는 것이다.

사실 대부분의 RL 문제는 (Finite) MDP 문제로 환원시켜 생각할 수 있다. 연속적으로 일어나는 RL 문제의 경우 가상의 시간 단계(stage)가 있어 각 단계에서 의사결정 및 행동이 이루어진다고 생각하면 된다. 또한 대부분의 문제에서 환경과 상태를 적절히 잘 정의하면[^7] Markov 속성을 가지게 할 수 있다.

[^7]: RL에서 환경은 "에이전트의 외부"라 정의된다. 이때 에이전트와 환경의 경계를 어디까지로 봐야 할까? 일단 이 경계는 물리적 경계와 다를 수 있다. 예를 들어 물컵의 위치, 모양 등을 인식하는 센서와 로봇 팔을 움직이는 모터로 구성된, 물컵을 들어올리는 로봇 팔을 생각해 보자. 물리적으로야 센서와 모터는 하나의 몸이지만, RL의 관점에서는 보통 모터를 에이전트로, 센서는 환경의 일부라 본다. 사실 이 문제에 명확한 정답은 없지만, 일반적으로는 에이전트가 마음대로 조작할 수 없는 영역을 에이전트의 외부, 즉 환경이라 본다. 이때 주의해야 할 것이, 에이전트와 환경의 경계는 에이전트가 완전히 통제(absolute control)할 수 있는 영역에 대한 경계이지, 에이전트의 지식(knowledge)에 대한 경계가 아니다. 에이전트는 환경이 어떻게 동작하는지에 대한 지식(ex. 보상은 어떻게 계산되는지 등)을 가지고 있을 수도 있다(비록 원하는 대로 통제할 수는 없지만 말이다).

# Dynamics Function

상술했듯이 MDP는 바로 직전의 상태값 $S\_{t-1}$와 행동 $A\_{t-1}$에만 의존해 현재의 상태 $S\_t$와 보상 $R\_t$가 결정된다. 즉, 시간 $t-1$에 특정 상태 $S\_{t-1} = s \in \mathcal{S}$에서 특정 행동 $A\_{t-1} = a \in \mathcal{A}$을 했을 때, (그 결과) 시간 $t$에 특정 상태 $S\_t = s' \in \mathcal{S}$가 되고 특정 보상 $r \in \mathcal{R}$을 받을 확률 $p(s',\,r \| s,\,a)$가 항상 존재한다는 것이다.[^8]

[^8]: 보면 알겠지만 $p$는 오직 바로 직전 $t-1$일 때의 상태/행동 항만 있지, $t-2$, $t-3$, …일 때의 상태/행동 항은 없다(Markov 속성).

{:.text-align-center}
$p(s',\,r \| s,\,a)$ = ($S\_{t-1}=s$, $A\_{t-1}=a$일 때 $S\_t = s'$, $R\_t=r$이 될 확률)

즉 $p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,\,1]$인 함수이다. 이 함수를 MDP의 **Dynamics Function**이라 한다. Dynamics Function은 확률이기 때문에 모든 $s \in \mathcal{S}$, $a \in \mathcal{A}(s)$에 대해 다음과 같은 성질이 성립한다.

$$\sum _{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',\,r | s,\,a) = 1$$

MDP에서 Dynamics Function는 아주 중요하다. Dynamics Function만 알면 필요한 모든 것을 계산할 수 있기 때문이다.

{:.no-guide-line}
- 상태 전이 확률(State Transition Probability) $p(s' \| s,\,a)$[^9] : 상태 $s$에서 행동 $a$를 해 상태 $s'$이 될 확률 ($p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,\,1]$)

[^9]: 원래 $p$는 Dynamics Function을 의미하는 기호이지만, 상태 전이 확률을 나타내는 기호로도 혼용된다.

$$p(s' | s,\,a) = \sum _{r \in \mathcal{R}} p(s',\,r | s,\,a)$$

{:.no-guide-line}
- (상태, 행동) 쌍에 대한 보상 기댓값(expected rewards for state-action pairs) $r(s,\,a)$ : 상태 $s$에서 행동 $a$를 할 때의 보상의 기댓값 ($r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$)

$$r(s,\,a) = \sum _{r \in \mathcal{R}} r \sum _{s' \in \mathcal{S}} p(s',\,r | s,\,a)$$

{:.no-guide-line}
- (상태, 행동, 다음 상태) 쌍에 대한 보상 기댓값(expected rewards for state-action-next-state triples) $r(s,\,a,\,s')$ : 상태 $s$에서 행동 $a$를 해 상태 $s'$이 되었을 때의 보상의 기댓값 ($r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$)

$$r(s,\,a,\,s') = \sum _{r \in \mathcal{R}} r \frac{p(s',\,r | s,\,a)}{p(s' | s,\,a)}$$

# 목표(Goal)와 보상(Reward)

RL에서 목표는 보상이라는, 환경이 에이전트에게 제공하는 일종의 시그널을 통해 구체화할 수 있다. 목표는 (장기적인 관점에서) 받을 수 있는 보상의 총합을 최대화하는 것이다.[^10] 이렇게 목표를 정의할 때 보상이라는 실수값을 쓰는 것은 RL만이 가진 특색이다.

[^10]: 이를 보상 가정(Reward Hypothesis)이라 한다.

즉, 만약 에이전트가 특정한 행동을 하길 원한다면 그 행동을 했을 때 적절한 보상을 주면 된다. 그러나 보상은 목표에 대한 사전 지식(prior knowledge)을 에이전트에게 알려주는 식으로 사용되어선 안된다. 조금 더 쉽게 말하자면, 보상은 반드시 최종 목표(goal)에 대해 주어저야지, 중간 목표(subgoal)에 대해 주어져선 안 된다.

예를 들어 체스 게임을 플레이하는 에이전트를 만들 때, 보상은 최종 목표인 "게임 승리"에 대해 주어져야 한다. 비록 우리가 "상대편 말을 잡는다" 또는 "보드의 중앙 영역에 대한 지배권을 갖는다"와 같은 중간 목표들이 게임을 승리하는데 필요하다는 사전 지식을 알고 있지만, 이런 중간 목표들에 대한 보상을 주면 에이전트는 최종 목표에 달성하는 방법을 찾기보단 중간 목표들을 달성하는 방법(ex. 게임에는 져도, 상대의 말을 최대한 많이 죽이는 방법)을 학습해 버릴 가능성이 높다.

보상은 에이전트에게 목표가 *무엇(what)*인지 알려주는 역할을 해야지, 목표를 *어떻게(how)* 달성하는지 알려주는 역할을 해선 안 된다.[^11]

[^11]: 사전 지식은 초기 정책(initial policy) 또는 가치 함수(value function)을 통해 전달해야 한다.

# Return

이때까진 RL 에이전트의 목표를 장기적인 관점에서 받을 수 있는 보상의 총합을 최대화하는 것이라 정의했다. 이때 "장기적인 관점에서 받을 수 있는 보상"이란 구체적으로 무엇일까?

RL 에이전트의 목표를 엄밀하게 정의하면 **Return을 최대화하는 것**이라 할 수 있다. 그렇다면 Return이란 무엇일까? Return $G\_t$는 시간 $t$ 이후 시간 $T$까지 받게 되는 모든 보상의 총합을 의미한다. 즉, 다음과 같이 수식으로 나타낼 수 있다.

$$G_t = R_{t+1} + R_{t+2} + \cdots + R_{T}$$

# Episodic Task vs. Continuing Task

RL 문제는 크게 두 가지로 분류할 수 있다.

{:.no-guide-line}
- Episodic Task : 에이전트와 환경 간 상호작용에서, 자연적으로 끊어지는 단위가 있는 경우
- Continuing Task : 에이전트와 환경 간 상호작용에서, 자연적으로 끊어지는 단위 없이 연속적으로 계속 이루어지는 경우

## Episodic Task

Episodic Task에서, 자연적으로 끊어지는 상호작용의 단위를 에피소드(Episode)라 한다. 예를 들어 미로를 찾는 로봇의 경우, 매 도전이 하나의 에피소드이다. 또 바둑을 두는 에이전트의 경우, 바둑 게임 한 판 한 판이 하나의 에피소드이다.

Episodic Task의 상태는 크게 종료 상태(terminal state)와 비종료 상태(nonterminal state)로 구분할 수 있다. 각 에피소드는 비종료 상태 중 하나인 시작 상태(starting state)에서 시작해 종료 상태가 되면 끝난다. 그리고 다시 시작 상태로 돌아가 다음 에피소드가 시작된다. 이때 비종료 상태들의 집합을 $\mathcal{S}$로, 모든 상태(종료 상태 + 비종료 상태)들의 집합을 $\mathcal{S}^+$로 표기하곤 한다.

각 에피소드는 다른 에피소드와 독립적이다. 예를 들어 바둑을 두는 에이전트에서, 전 에피소드에 승리했다고 바둑돌을 몇 개 더 두고 시작하는 경우는 없다. 전 에피소드의 결과와 상관없이 처음으로 돌아가 다시 독립적인 에피소드가 시작된다. 따라서 Episodic Task에서 각 에피소드는 모두 같은 종료 상태에서 끝나고, 결과에 따라 보상만 달라진다고 할 수 있다.

Episodic Task에서 Return은 각 에피소드마다 계산된다. 즉 Return의 마지막 시간 간격 $T$로 종료 상태가 됐을 때의 시간을 사용한다.

Episodic Task에서 상태를 표현할 때는 조금 다른 표기법을 사용해야 한다. 이때까진 상태를 $S\_t$와 같이 시간 $t$만 표기했는데, Episodic Task에서는 현재 상태가 몇 번째 에피소드의 몇 번째 상태인지를 나타내는 것도 중요하다. 이를 표현하기 위해 $S\_{t,\,i}$와 같이 표기한다. 여기서 $i$는 몇 번째 에피소드인지를, $t$는 현재 한 에피소드에서 몇 번째 상태인지를 나타낸다.[^12] 즉 예를 들어 $S\_{2,\,5}$라 쓰면 5번째 에피소드 시행 중 현재 세 번째 상태에 있음을 의미한다. 이 표기법은 상태뿐만 아니라 행동($A\_{t,\,i}$), 보상($R\_{t,\,i}$) 등 다른 값들에도 동일하게 사용 가능하다. 그런데 사실 Episodic Task에서는 한 에피소드만 다루는 경우가 대부분이라, 많은 경우 몇 번째 에피소드($i$)인지에 대한 정보는 필요 없어 $i$를 떼고 그냥 $S\_t$와 같이 표기하는 경우가 많다.

[^12]: $t$는 0부터 시작한다.
## Continuing Task

Continuing Task의 예로는 R2-D2나 C-3PO와 같은 안드로이드 로봇이 있다. Continuing Task에서 Return의 마지막 시간 간격 $T=\infty$이기에, 기존 Return 계산식을 그대로 사용하면 Return이 무한대가 되기 쉽다. 따라서 Continuing Task에서의 Return에는 *Discounting*이라는 개념을 추가한 Discounted Return을 사용하는 경우가 많다. Discounted Return $G\_t$는 다음과 같이 정의된다.

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum _{k=0} ^\infty \gamma^k R_{t+k+1}$$

이때 Discount되는 양을 조절하는 파라미터 $0 \le \gamma < 1$를 Discount Rate라 부른다. $\{R\_k\}$가 bound되어 있는 경우[^13] $G\_t$는 수렴한다. 

[^13]: 최대값이 정해져 있는 경우(정확히는, 상한이 있는 경우)

Discounting은 미래에 받을 보상이 현재 얼마만큼의 가치가 있는지를 나타낸다. 시간 $t=k$에 받는 미래의 보상 $R\_k$는 현재 $\gamma^{k-1}R\_k$의 가치만 있다고 보는 것이다. $\gamma$가 1에 가까워질수록 미래에 받을 보상의 가치를 크게 여기고, 0에 가까워질수록 최근에 받을 보상들에 좀 더 집중하게 된다. 만약 $\gamma = 0$이면 미래에 받을 보상들은 전혀 고려하지 않고 오직 바로 직후에 받을 보상을 최대화하는 방향으로 학습이 일어나 아주 근시안적인(myopic) 에이전트가 된다.

Discounted Return 식을 변형하면 다음과 같이 된다.

$$\begin{align}
G_t
&= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\\[0.5em]
&= R_{t+1} + \gamma G_{t+1}\\[0.5em]
\end{align}
$$

이 성질은 다양한 RL 알고리즘에서 응용된다.

## 여담 : Episodic Task와 Continuing Task 합치기

Episodic Task에서의 Return은 유한 개의 숫자들의 합으로 정의되고, Continuing Task에서의 Return은 무한 개의 숫자들의 합으로 정의된다. 이때 Episodic Task에서 한 에피소드가 종료되는 것을, 가능한 전이(transition)는 오직 자기 자신으로의 전이밖에 없고, 이때 받는 보상은 0인 특별한 상태에 진입하는 것으로 해석할 수 있다. 이 특별한 상태의 이름을 Absorbing state라 한다. 이렇게 Episodic Task에 Absorbing state를 도입하면 Episodic Task에서도 Continuing Task의 Return 식을 사용할 수 있다($\gamma=1$로 놓고 쓴다).

또는 다음 식을 이용해 Episodic Task와 Continuing Task의 Return 식을 하나로 합칠 수도 있다.

$$G_t = \sum _{k=t+1} ^{T} \gamma^{k-t-1} R_{k}$$

Episodic Task일 때는 $\gamma=1$로, Continuing Task일 때는 $0 \le \gamma < 1$, $T=\infty$로 놓고 사용하면 된다.

일반적으로는 표기의 편의성 때문에 Episodic Task와 Continuing Task의 Return을 위 두 가지 방법 중 하나를 선택하여 한 번에 나타낸다.

# 정책(policy)과 가치 함수(value function)

Return의 개념을 이용하면 [이전 글](/SNU_m3309.000200/02-introduction)에서 살펴본 가치 함수(value function)를 조금 더 정확히 정의할 수 있다. 가치 함수는 상태에 대한 함수로서, 에이전트가 해당 상태가 되었을 때 받을 것으로 수 있는 Return 값(expected return)을 알려준다.[^14] 거의 대부분의 RL 알고리즘은 이 가치 함수를 추정하는 단계를 가지고 있다. 미래에 받을 보상은 어떤 행동을 해 왔는지에 따라 달라지기 때문에, 가치 함수는 어떤 행동을 할 지 결정하는 정책(policy)에 따라 바뀐다. 

[^14]: 또는 상태-행동 쌍에 대한 함수로서, 해당 상태에서 해당 행동을 했을 때 받을 수 있는 Return 값을 알려준다.

정책 $\pi(a \| s)$는 특정 상태 $s$에서 특정 행동 $a$를 할 때의 확률을 알려주는 함수이다.


