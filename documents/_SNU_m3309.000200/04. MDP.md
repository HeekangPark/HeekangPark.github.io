---
title: "L04. MDP(Markov Decision Process)"
order: 4
date: "2021-03-26"
---

# 에이전트와 환경의 관계

[이전 글](/SNU_m3309.000200/03-mab)에서 보았던 k-armed Bandit Problem의 경우 에이전트가 행동을 할 때마다 보상이 발생했다. 그러나 그 과정에서 에이전트를 둘러싸고 있는 환경(environment)의 상태(state)가 변하지는 않았다.

하지만 일반적인 RL 문제의 경우 에이전트가 행동을 수행하면 보상이 발생할 뿐만 아니라 환경의 상태가 변한다. 그래서 에이전트는 상태를 고려하여 다음 행동을 결정해야 한다.[^1] 일반적으로, 매 시간 간격(time step) $t=0,\,1,\,2,\,\cdots$마다[^2] 에이전트는 환경의 상태를 나타내는 $S\_t \in \mathcal{S}$를 토대로 행동 $A\_t \in \mathcal{A}(s)$[^3]를 결정해 수행하고, 그 결과 보상 $R\_{t+1} \in \mathcal{R} \subset \mathbb{R}$과 새로운 상태값 $S\_{t+1}$을 받는다[^4]. 이를 반복하면 다음과 같은 시퀸스를 얻을 수 있다.[^5]

[^1]: k-armed Bandit Problem에서는 $t=1$, $t=2$, …일 때 선택할 수 있는 행동의 종류가 모두 같았다(항상 $k$개의 옵션 중 하나를 선택할 수 있었다). 즉 k-armed Bandit Problem의 경우 행동을 선택할 때 아무런 값도 고려할 필요가 없었다(이전 보상을 바탕으로 다음 행동을 선택하는 것은 '보상을 최대화한다'는 목적을 위해서였지, 무슨 제약사항이 있어서 한 것은 아니었다). 그러나 일반적인 RL 문제의 경우 환경의 상태에 따라 특정 행동은 선택 불가능해질 수도 있다. 즉 상태는 선택 가능한 행동의 집합 $\mathcal{A}(s)$를 결정한다고 할 수 있다.
[^2]: 이 글에서 시간 간격을 이산적으로(discrete) 놓은 이유는 논의를 단순화하기 위함이다. 당연히 RL에서는 이산적인 시간뿐만 아니라 연속적인 시간도 다룬다. 그러나 연속적인 시간에서의 RL 문제에도 이산적인 시간에서의 RL을 다룰 떄의 아이디어를 확장해 사용할 수 있는 경우가 많다.
[^3]: 만약 모든 상태에서 선택할 수 있는 행동의 집합이 동일하다면(즉 $\mathcal{A}(s)$가 항등함수이면) 단순히 $\mathcal{A}$라 쓰기도 한다.
[^4]: 행동 $A\_t$의 결과가 $R\_{t+1}$과 $S\_{t+1}$임에 주목하라. 시간 $t$에서의 행동의 결과가 시간 $t+1$에서의 보상과 상태로 나온다. 이 표기법을 사용하면 보상과 상태가 상호 결정적인(jointly determined) 값임을 강조할 수 있기에 많이 사용된다.
[^5]: 보상 $R$과 상태 $S$의 순서는 바뀌어도 된다.

$$S_0,\,A_0,\,R_1,\,S_1,\,A_1,\,R_2,\,S_2,\,A_2,\,\cdots$$

보면 알겠지만, $t=0$일 때를 제외하곤 "RSA" 순서로 나온다는 것을 기억하면 헷갈리지 않을 것이다.

# MDP (Markov Decision Process)

어떤 RL 문제에서, 확률변수 $R\_t$와 $S\_t$가 오직 직전 상태 $S\_{t-1}$와 행동 $A\_{t-1}$에만 의존한 이산 확률 분포(discrete probability distribution)를 따른다면 이 문제는 **Markov 속성(Markov property)**가 있다고 한다.[^6] 그리고 이렇게 Markov 속성을 가지고 있는 RL 문제를 **MDP(Markov Decision Process)**라 한다. 그리고 MDP 중 상태들의 집합 $\mathcal{S}$, 행동들의 집합 $\mathcal{A}$, 보상들의 집합 $\mathcal{R}$이 모두 유한집합인 MDP를 **Finite MDP**라 한다.

[^6]: 즉, 과거의 상태($S\_{t-2}$, $S\_{t-3}$, …) 및 행동($A\_{t-2}$, $A\_{t-3}$, …)에는 영향을 받지 않고, 오직 바로 직전 상태($S\_{t-1}$) 및 행동($A\_{t-1}$)에만 영향을 받는다는 것이다.

사실 대부분의 RL 문제는 (Finite) MDP 문제로 환원시켜 생각할 수 있다. 연속적으로 일어나는 RL 문제의 경우 가상의 시간 단계(stage)가 있어 각 단계에서 의사결정 및 행동이 이루어진다고 생각하면 된다. 또한 대부분의 문제에서 환경과 상태를 적절히 잘 정의하면[^7] Markov 속성을 가지게 할 수 있다.

[^7]: RL에서 환경은 "에이전트의 외부"라 정의된다. 이때 에이전트와 환경의 경계를 어디까지로 봐야 할까? 일단 이 경계는 물리적 경계와 다를 수 있다. 예를 들어 물컵의 위치, 모양 등을 인식하는 센서와 로봇 팔을 움직이는 모터로 구성된, 물컵을 들어올리는 로봇 팔을 생각해 보자. 물리적으로야 센서와 모터는 하나의 몸이지만, RL의 관점에서는 보통 모터를 에이전트로, 센서는 환경의 일부라 본다. 사실 이 문제에 명확한 정답은 없지만, 일반적으로는 에이전트가 마음대로 조작할 수 없는 영역을 에이전트의 외부, 즉 환경이라 본다. 이때 주의해야 할 것이, 에이전트와 환경의 경계는 에이전트가 완전히 통제(absolute control)할 수 있는 영역에 대한 경계이지, 에이전트의 지식(knowledge)에 대한 경계가 아니다. 에이전트는 환경이 어떻게 동작하는지에 대한 지식(ex. 보상은 어떻게 계산되는지 등)을 가지고 있을 수도 있다(비록 원하는 대로 통제할 수는 없지만 말이다).

# Dynamics Function

상술했듯이 MDP는 바로 직전의 상태값 $S\_{t-1}$와 행동 $A\_{t-1}$에만 의존해 현재의 상태 $S\_t$와 보상 $R\_t$가 결정된다. 즉, 시간 $t-1$에 특정 상태 $S\_{t-1} = s \in \mathcal{S}$에서 특정 행동 $A\_{t-1} = a \in \mathcal{A}$을 했을 때, (그 결과) 시간 $t$에 특정 상태 $S\_t = s' \in \mathcal{S}$가 되고 특정 보상 $r \in \mathcal{R}$을 받을 확률 $p(s',\,r \| s,\,a)$가 항상 존재한다는 것이다.[^8]

[^8]: 보면 알겠지만 $p$는 오직 바로 직전 $t-1$일 때의 상태/행동 항만 있지, $t-2$, $t-3$, …일 때의 상태/행동 항은 없다(Markov 속성).

{:.text-align-center}
$p(s',\,r \| s,\,a)$ = ($S\_{t-1}=s$, $A\_{t-1}=a$일 때 $S\_t = s'$, $R\_t=r$이 될 확률)

즉 $p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,\,1]$인 함수이다. 이 함수를 MDP의 **Dynamics Function**이라 한다. Dynamics Function은 확률이기 때문에 모든 $s \in \mathcal{S}$, $a \in \mathcal{A}(s)$에 대해 다음과 같은 성질이 성립한다.

$$\sum _{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',\,r | s,\,a) = 1$$

MDP에서 Dynamics Function는 아주 중요하다. Dynamics Function만 알면 필요한 모든 것을 계산할 수 있기 때문이다.

{:.no-guide-line}
- 상태 전이 확률(State Transition Probability) $p(s' \| s,\,a)$[^9] : 상태 $s$에서 행동 $a$를 해 상태 $s'$이 될 확률 ($p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,\,1]$)

[^9]: 원래 $p$는 Dynamics Function을 의미하는 기호이지만, 상태 전이 확률을 나타내는 기호로도 혼용된다.

$$p(s' | s,\,a) = \sum _{r \in \mathcal{R}} p(s',\,r | s,\,a)$$

{:.no-guide-line}
- (상태, 행동) 쌍에 대한 보상 기댓값(expected rewards for state-action pairs) $r(s,\,a)$ : 상태 $s$에서 행동 $a$를 할 때의 보상의 기댓값 ($r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$)

$$r(s,\,a) = \sum _{r \in \mathcal{R}} r \sum _{s' \in \mathcal{S}} p(s',\,r | s,\,a)$$

{:.no-guide-line}
- (상태, 행동, 다음 상태) 쌍에 대한 보상 기댓값(expected rewards for state-action-next-state triples) $r(s,\,a,\,s')$ : 상태 $s$에서 행동 $a$를 해 상태 $s'$이 되었을 때의 보상의 기댓값 ($r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$)

$$r(s,\,a,\,s') = \sum _{r \in \mathcal{R}} r \frac{p(s',\,r | s,\,a)}{p(s' | s,\,a)}$$

# 목표(Goal)와 보상(Reward)

