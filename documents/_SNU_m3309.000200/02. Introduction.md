---
title: "L02. Introduction"
order: 2
date: "2021-03-04"
---

# 강화학습(RL, Reinforcement Learning)이란?

강화학습(RL, Reinforcement Learning)이라는 단어는 일반적으로 다음 세 가지 뜻 중 하나로 사용된다.

- 에이전트(agent)가 특정 환경(environment) 안에서 받을 수 있는 보상(reward)를 극대화하는 문제
- 이 문제를 푸는 해결법들
- 이런 문제와 해결법을 다루는 분야

이 세가지 정의은 유사하지만 집중하는 부분이 조금씩 다르기에, 문맥을 잘 읽어 어떤 의미로 사용되었는지를 정확히 파악할 필요가 있다.

# RL의 특징

RL은 다음과 같은 특성이 있다.

- 시행착오(trial and error search) : 에이전트는 어떤 행동(action)을 취해야 하는지, 취하지 말아야 하는지를 입력받지 않는다. 에이전트는 여러 번의 시행착오를 통해 쌓은 경험(experience)을 토대로 어떤 행동을 해야 할 지 결정한다.
- 지연된 보상(delayed reward) : 에이전트의 행동은 즉각적인 보상를 만들어 낼 수도 있지만, 나중 상황(situation)을 바꾸어 앞으로의 보상(reward)에 영향을 끼칠 수도 있다. 에이전트는 이를 모두 고려해 최종적인 보상의 합이 가장 큰 방향으로 학습되어야 한다.

이들 두 특성은 RL이 다른 기계학습 방법에 비해 가지는 가장 큰 특징이다.

# 용어 정리

RL에서 사용하는 주요한 용어들을 알아보자.

- 에이전트(agent) : 의사결정(decision-making)을 하는 주체. RL은 에이전트가 환경(environment) 안에서 환경의 불확실성(uncertainty)에도 불구하고 목표(goal)를 위해 적절한 행동(action)을 하도록 만드는 것이 목표이다.
- 행동(action) : 에이전트가 특정 동작을 수행하는 것. 에이전트의 행동은 환경에 영향을 미치고, 변화한 환경은 에이전트의 가능한 미래 행동들의 종류에 영향을 미친다. 따라서 에이전트는 수시로 환경의 변화를 감시해야 하고 이에 적절이 대응해야 한다. 이렇게 에이전트와 환경이 서로 영향을 주고받는 행위 및 현상을 상호작용(interaction)이라 한다.
- 상태(state) : 에이전트한테 전달된, 지금 현재 환경(environment)이 어떤 상태인지를 알려주는 신호. 에이전트는 상태를 토대로 다음 행동을 결정한다. 상태는 정책(policy)과 가치 함수(value function)의 입력값이자, 모델(model)의 입/출력값으로 사용된다.

# RL vs. 지도학습(Supervised Learning) vs. 비지도학습(Unsupervised Learning) 

RL과 지도학습, 비지도학습을 비교하면 다음과 같다.

## 학습 데이터

- 지도학습 : 레이블링(labeling)된 예제(example)들로 이루어진 학습 데이터가 주어진다.
- 비지도학습 : 레이블링 되지 않은 예제들로 이루어진 데이터가 주어진다.
- RL : 학습 데이터가 주어지지 않는다. RL에서는 심지어 "A 조건에서 에이전트는 *반드시* B 행동을 취해야 한다"와 같은, 언제나 참인 예제가 없을 수도 있다. RL에서는 에이전트가 환경과 상호작용해 획득한 경험(experience)을 이용해 학습이 진행된다.

## 학습의 목표

- 지도학습 : 모델(model)이 학습 데이터로부터 일반화(generalize)된 정보를 습득하게 해, 학습 데이터에서 등장하지 않은 상황에서도 올바른 예측을 할 수 있게 하는 것이 목표이다.
- 비지도학습 : 데이터에서 숨겨진 구조(structure) 및 특징(feature)을 찾아내는 것이 목표이다.
- RL : 에이전트가 주어진 환경을 잘 파악하여, 최종적으로 획득하는 보상의 총합이 가장 크도록 하는 것이 학습의 목표이다.

# Exploitation vs. Exploration

환경 안에서 에이전트가 할 수 있는 행동은 Exploitation(이용), Exploration(탐색), 이렇게 두 가지로 크게 분류할 수 있다.

- Exploitation(이용) : 많은 보상을 얻기 위해, 과거에 시도해 보았던 행동 중 가장 큰 보상을 만드는 행동을 한다.
- Exploration(탐색) : 가능한 행동의 종류를 찾기 위해, 과거에 시도해보지 않았던 행동을 한다.

예를 들어 음식점을 선택하는 문제에서, 평소에 자주 가던 맛집을 가는 것은 Exploitation이다. 반면 새로운 맛집을 찾아보는 것은 Exploration이다(이 경우 보상은 "맛있는 식사"이다). 또한 환자에게 약을 투여하는 상황에서, 이미 검증된 약을 사용하는 것은 Exploitation이다. 반면 새로운 신약을 시험해보는 것은 Exploration이다(이 경우 보상은 "환자의 회복"이다).

Exploitation과 Exploration 중 어떤 것을 선택해야 할까? 이 문제를 Exploitation-Exploration Dilemma라 부른다. 두 종류의 행동은 모두 중요하고, 동시에 두 가지 행동을 할 수는 없으므로 한 종류의 행동을 하면 (해당 행동을 하는 동안) 다른 행동을 할 수 없다. 그러나 계속 한 가지 행동만 하면 별로 큰 보상을 얻지 못할 것이다. 지도학습이나 비지도학습에서는 이 문제가 발생하지 않는다. 이 문제를 해결하는 것이 RL의 목표 중 하나이다.

# RL의 핵심 요소

RL 시스템을 구성하는 핵심 요소는 정책(policy), 보상 시그널(reward signal), 가치 함수(value function), 모델(model), 이렇게 4가지가 있다.

{% include caption-img.html src="02-main-element-of-RL.png" title="Fig.01 Main Element of RL" %}

## 정책(policy)

정책(policy)은 특정 시간에 에이전트가 행동하는 방식을 결정한다. 즉 정책이란 현재 환경의 상태(state)를 입력값으로 받아 에이전트의 행동을 반환하는 일종의 함수라 할 수 있다. 많은 경우 정책은 확률적으로(stochastic) 주어진다(선택 가능한 행동 하나하나의 확률(probability)이 주어진다).

정책은 두 가지 종류로 구분할 수 있다.

- 결정론적 정책(deterministic policy) : 특정 상태(state)에서는 반드시 특정 행동(action)을 하도록 정해져 있는 정책
- 확률적 정책(stochastic policy) : 특정 상태(state)에서 어떤 행동(action)을 할 지 확률적으로 결정하는 정책

정책은 심리학에서의 자극-반응 규칙(stimulus-response rule)에 대응된다.

## 보상 시그널(reward signal)

보상 시그널(reward signal) 또는 보상(reward)은 RL 문제의 목표를 정의한다. 매 시간 간격마다(에이전트가 행동을 할 때마다) 환경은 에이전트에게 보상(reward)이라 부르는 숫자 하나를 제공한다. 에이전트의 유일한 목표는 장기적인 관점에서 이들 보상의 총합이 최대가 되게 하는 것이다. 즉 보상은 에이전트에게 무엇이 좋은지 무엇이 안 좋은지를 알려주는 값이다. 일반적으로 보상 시그널은 환경 상태와 실행한 행동을 입력값으로 받고 확률적으로(stochastic) 보상을 출력하는 함수 형태로 주어진다.

보상 시그널은 정책을 바꾸는 가장 중요한 지표 중 하나이다. 만약 현재의 정책으로 결정된 행동이 계속해서 낮은 보상을 만든다면, 미래에는 동일한 상황에 다른 정책을 적용하는 것이 좋을 것이다.

보상은 생명체가 받는 행복(pleasure) 또는 고통(pain)에 대응된다.

## 가치 함수(value function)

가치(value)란 현재 상태로부터 앞으로 받을 것으로 예상되는 모든 보상들의 총합을 의미한다. 보상이 현재 환경 상태가 좋은지 안좋은지에 대해 즉각적이고 본능적인 선호도(desirability)를 알려준다면, 가치는 장기적인 관점에서 앞으로 일어날 가능성이 높은 상태들과 이들로부터의 보상들에 대한 선호도를 의미한다고 할 수 있다. 

어떤 의미에서 보상은 1차적인 값이고, 보상의 예측값인 가치는 2차적인 값이라 할 수 있다. 보상이 없다면 가치도 없고, 가치를 예측하는 유일한 이유는 더 많은 보상을 받기 위함이다.

그러나 의사를 결정하거나 결정한 의사를 평가할 때는 가치를 기준으로 해야 한다. 에이전트는 가장 높은 보상을 주는 행동이 아닌, 가장 높은 가치를 주는 행동을 선택해야 한다. 그래야 장기적인 관점에서 받을 수 있는 보상을 최대화할 수 있기 때문이다.

하지만 물론 가치를 정하는 일은 보상을 정하는 일보다 훨씬 어려운 일이다. 보상은 환경으로부터 곧바로 주어지는 값이지만, 가치는 에이전트가 생애 내내 관측한 결과들을 토대로 만든 추정값이기 때문이다. 사실 우리가 앞으로 다룰 거의 대부분의 RL 알고리즘에서 가장 핵심적으로 다루는 부분이 어떻게 하면 효율적으로 가치를 추정하는지에 대해서이다.

## 모델(model)

모델(model)이란 환경의 동작을 모사하거나, 환경이 어떻게 동작할지를 추정한 것이다. 모델은 상태와 행동을 입력값으로 받아 앞으로의 상태와 보상을 예측해 출력하는 일종의 함수이다. RL 알고리즘에 따라 모델을 사용할 수도, 사용하지 않을 수도 있다.

모델은 계획(planning)에 사용된다. 계획이란 미래에 일어날 상황들을 고려하여 행동을 결정하는, 시행착오의 반댓말이다. 모델과 계획을 이용해 RL 문제를 푸는 방법을 모델 기반 방법(model-based method)이라 한다. 모델을 사용하지 않고 시행착오만을 통해 학습하는 방법은 model-free method라 한다.

# RL vs. 진화적 방법(evolutionary method)

RL 문제를 풀기 위한 방법에는 가치 함수를 추정해 사용하는 RL 방법뿐만 아니라, 유전 알고리즘(genetic algorithm), 유전 프로그래밍(genetic programming), simulated annealing 등과 같이 가치 함수를 사용하지 않는 다양한 최적화 방법도 있다. 이 방법들은 마치 생명체가 진화하는 것과 닮아 진화적 방법(evolutionary method)이라 부른다. 진화적 방법은 일반적으로 다음과 같은 방식으로 동작한다.

1. 환경과 각각 상호작용하는 독립적인 인스턴스(instance)들을 여러 개 만든다.
2. 여러 개의 정적인(static) 정책들을 각 인스턴스에 장기간 적용한다.
3. 각 인스턴스가 최종적으로 받게 되는 보상을 확인한다.
4. 이들 중 가장 큰 보상을 받은 정책과 이를 무작위로 변형한 정책들(random variants)을 가지고 2번부터 반복한다. 참고로 이렇게 한 바퀴 도는 것을 한 세대(generation)라 부른다.

진화적 방법은 다음 경우에 사용하기 좋다.

- 선택 가능한 정책의 종류가 적다(= 정책 공간(space of policies)이 작다).
- 좋은 정책이 흔하고 찾기 쉽다.
- 탐색에 많은 시간을 들일 수 있다.
- 에이전트가 환경에 대한 완전한 정보(state)를 알 수 없다.

RL과 진화적 방법은 일견 유사해 보이지만 많은 차이가 있다. RL에서는 환경과 상호작용을 하는 동안 학습이 이루어지는 반면, 진화적 방법은 환경과의 상호작용을 통해 학습하지 않는다. 또한 진화적 방법은 정책이 한 행동에서 다른 행동으로 가는 함수라는 사실을 활용하지 않는다. 그리고 RL에서는 에이전트가 겪은 상태와 취한 행동 정보를 사용해 학습이 이루어지지만, 진화적 방법은 그런 정보를 사용하지 않고 오직 정책과 그로 인한 보상만을 토대로 학습이 이루어진다.

즉 진화적 방법은 RL에 비해 정보를 덜 사용하는 학습법이다. 물론 RL이 진화적 방법보다 추가로 더 쓰는 정보가 잘못되어(ex. 상태값이 잘못 인식된(misperceived) 경우 등) 잘못된 방향으로 학습이 이루어질 수도 있지만, 일반적으로는 RL을 사용하면 진화적 방법을 사용하는 것보다 더 효율적인 탐색이 가능하다.

# 예시: Tic-Tac-Toe

tic-tac-toe 게임의 인공지능을 만들어 보자.

## 진화적 방법(evolutionary method)

진화적 방법으로 tic-tac-toe 게임의 인공지능을 만드려면 다음 과정을 거치면 된다.

1\. 우선 여러 개의 초기 정책들을 구성한다. 여기서 정책이란 3×3 게임판에서 가능한 모든 경우에 대해[^1] 다음에 어떤 행동을 할지를 적어 놓은 배열이다. 

[^1]: 게임판의 한 칸에는 O, X, 공백(아직 아무도 놓지 않음), 이렇게 3가지 상태가 가능하므로 3×3 게임판에서 가능한 모든 경우의 수는 $3^9=19,683$이 된다. 물론 이 중 상당수는 실제 게임에서 도달 불가능한 상태이기에, 이들을 전략적으로 제거하면 경우의 수를 훨씬 줄일 수 있다.

{% include caption-img.html src="02-tic-tac-toe-evolutionary-method-policy1.png" title="Fig.02 진화적 방법 정책 예시 1" %}

다른 정책을 적용하면 다른 식으로 게임이 풀리게 된다.

{% include caption-img.html src="02-tic-tac-toe-evolutionary-method-policy2.png" title="Fig.03 진화적 방법 정책 예시 2" %}

2\. 각 정책들에 대해, 정책이 시키는 대로 행동하며 적(opponent)과 여러 판의 게임을 진행한다. 게임 결과를 바탕으로 해당 정책을 적용했을 때의 승률을 추정한다.

3\. 가장 승률이 높게 나온 정책과 그 변종들을 이용해 다시 2번을 반복한다. 이렇게 적과의 게임을 계속하며 가장 승률이 높은 정책을 계속 탐색해 나간다.

## RL

RL 방법으로 tic-tac-toe 게임의 인공지능을 만드려면 다음 과정을 거치면 된다.

1\. 가치 함수를 초기화한다. 가치 함수는 3×3 게임판에서 가능한 모든 경우에 대해 가치를 계산해 놓은 배열이다. 즉 만약 배열에서 상태 A의 가치가 상태 B의 가치보다 크게 되어 있다면, 상태 A가 상태 B보다 더 "좋은" 상태라는 뜻이다.

{% include caption-img.html src="02-tic-tac-toe-rl-policy.png" title="Fig.04 RL 정책 예시" %}

가치 함수의 배열에서, X가 이기는 경우[^2]의 가치를 1, X가 지는 경우의 가치를 0, 나머지 경우(아직 승부가 나지 않은 경우)의 가치를 0.5[^3]로 초기화한다.

[^2]: 인공지능이 항상 X를 놓는다고 가정하자.
[^3]: 아직 이길지 질지 모르므로 승률이 반반이라 설정한 것이다.

2\. 적(opponent)과 게임을 한 판 진행한다. 에이전트가 취할 행동은 다음 방법으로 결정한다.

- Exploitation : 대부분의 경우, 현재 상태에서 이동할 수 있는 상태 중 가장 높은 가치를 가지고 있는 상태가 되도록 탐욕적으로(greedily) 행동한다.
- Exploration : 간혹, 현재 상태에서 이동할 수 있는 상태 중 무작위로 하나를 골라 그 상태가 되도록 행동한다.[^4]

[^4]: 이렇게 움직이는 것을 exploratory move라 한다.

3\. 게임 결과를 바탕으로 가치 함수를 업데이트하고[^5], 2번으로 돌아가 이 과정을 필요한 만큼 반복한다. 현재 상태를 $S\_t$, 다음 상태를 $S\_{t+1}$, 상태 $S\_t$일 때의 가치를 $V(S\_t)$라 할 때, 다음 식을 이용해 업데이트를 진행한다.

[^5]: 이를 back-up이라 부른다.

$$V(S_t) = V(S_t) + \alpha [V(S_{t+1}) - V(S_t)]$$

이때 $\alpha$는 step-size parameter라 불리는 값으로, 학습이 되는 속도를 조정하는 파라미터이다($0 \le \alpha < 1$).

적(opponent)이 고정되어 있을 때 $\alpha$의 값이 시간에 따라 작아진다면 이 방법은 수렴한다. 즉 이 방법으로 (해당 적을 상대할 때의) 각 상태의 진짜 승률(true probability of winning)을 계산할 수 있다. 만약 적이 고정되어 있지 않다면, $\alpha$의 값이 시간에 따라 감소하지 않더라도 잘 학습된다.

이 방식으로 학습하는 것을 TD Learning(Temporal-Difference Learning)이라 한다. TD Learning은 model-free method 중 하나이다.

# RL에 대한 오해들

RL을 적용한 예시로 가장 유명한 것이 알파고이다 보니, RL에 대한 많은 오해들이 있다.

## RL은 적이 있을 때만 쓸 수 있다?

아니다. RL은 적이 없어도 쓸 수 있다.

## RL은 episodic task에만 쓸 수 있다?

episodic task란 위에서 살펴 본 tic-tac-toe, 알파고가 한 바둑과 같이 끝이 있는 과제를 뜻한다. 이의 반댓말은 continuing task로, 로봇 청소기의 청소(끊임없이 오염이 진행되므로 로봇 청소기는 충전할 때를 제외하고는 계속 청소를 한다) 등이 그 예이다.

많은 경우 RL은 episodic task에만 쓸 수 있다고 오해하는데, RL은 continuing task에서도 쓸 수 있다.

## RL은 이산적인 상황(discrete case)에서만 쓸 수 있다?

아니다. RL은 연속적인 상황에서도 쓸 수 있다.

## RL은 상태 집합(state set)이 작은 경우에만 쓸 수 있다?

아니다. 알파고가 보여주었다시피 상태 집합이 매우 큰 경우에도 RL은 성공적으로 적용된다. 참고로 상태 공간이 너무 큰 경우 함수 근사(function approximation) 기술을 사용할 수 있다.

## RL은 사전 지식(prior knowledge)과 함께 쓸 수 없다?

아니다. RL은 반드시 제로베이스에서 시작하지 않아도 된다.

## RL은 상태가 명확한 경우에만 쓸 수 있다?

아니다. RL은 몇몇 상태가 숨겨져 있더라도 사용할 수 있다.

## RL은 행동의 결과를 예상하는 단기간 모델(short-term model)이 있어야만 쓸 수 있다?

아니다. RL은 행동의 결과를 예상하는 단기간 모델(short-term model)이 없어도 쓸 수 있다.















<style>
.table-wrapper {
    overflow-x: auto;
}

table.my-table {
    border-collapse: collapse;
    margin-bottom: 1em;
    min-width: 400px;
}

table.my-table tbody tr td {
    padding-top: 0.5em;
    padding-bottom: 0.5em;
    padding-left: 0.5em;
    padding-right: 0.5em;
    border-top: 1px solid #888888;
    border-left: 1px solid #888888;
    border-right: 1px solid #888888;
    border-bottom: 1px solid #888888;
}

table.my-table tbody tr td:first-child {
    white-space: nowrap;
    text-align: center;
    background-color: lightblue;
}

table.my-table tbody tr td:last-child {
    width: 100%;
}
</style>