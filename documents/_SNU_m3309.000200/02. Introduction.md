---
title: "L02. Introduction"
order: 2
date: "2021-03-04"
---

# 강화학습(RL, Reinforcement Learning)이란?

강화학습(RL, Reinforcement Learning)이라는 단어는 일반적으로 다음 세 가지 뜻 중 하나로 사용된다.

- 에이전트(agent)가 특정 환경(environment) 안에서 받을 수 있는 보상(reward)를 극대화하는 문제
- 이 문제를 푸는 해결법들
- 이런 문제와 해결법을 다루는 분야

이 세가지 정의은 유사하지만 집중하는 부분이 조금씩 다르기에, 문맥을 잘 읽어 어떤 의미로 사용되었는지를 정확히 파악할 필요가 있다.

# RL의 특징

RL은 다음과 같은 특성이 있다.

- 시행착오(trial and error search) : 에이전트는 어떤 행동(action)을 취해야 하는지, 취하지 말아야 하는지를 입력받지 않는다. 에이전트는 여러 번의 시행착오를 통해 쌓은 경험(experience)을 토대로 어떤 행동을 해야 할 지 결정한다.
- 지연된 보상(delayed reward) : 에이전트의 행동은 즉각적인 보상를 만들어 낼 수도 있지만, 나중 상황(situation)을 바꾸어 앞으로의 보상(reward)에 영향을 끼칠 수도 있다. 에이전트는 이를 모두 고려해 최종적인 보상의 합이 가장 큰 방향으로 학습되어야 한다.

이들 두 특성은 RL이 다른 기계학습 방법에 비해 가지는 가장 큰 특징이다.

# 용어 정리

RL에서 사용하는 주요한 용어들을 알아보자.

- 에이전트(agent) : 의사결정(decision-making)을 하는 주체. RL은 에이전트가 환경(environment) 안에서 환경의 불확실성(uncertainty)에도 불구하고 목표(goal)를 위해 적절한 행동(action)을 하도록 만드는 것이 목표이다.
- 행동(action) : 에이전트가 특정 동작을 수행하는 것. 에이전트의 행동은 환경에 영향을 미치고, 변화한 환경은 에이전트의 가능한 미래 행동들의 종류에 영향을 미친다. 따라서 에이전트는 수시로 환경의 변화를 감시해야 하고 이에 적절이 대응해야 한다. 이렇게 에이전트와 환경이 서로 영향을 주고받는 행위 및 현상을 상호작용(interaction)이라 한다.

# RL vs. 지도학습(Supervised Learning) vs. 비지도학습(Unsupervised Learning) 

RL과 지도학습, 비지도학습을 비교하면 다음과 같다.

## 학습 데이터

- 지도학습 : 레이블링(labeling)된 예제(example)들로 이루어진 학습 데이터가 주어진다.
- 비지도학습 : 레이블링 되지 않은 예제들로 이루어진 데이터가 주어진다.
- RL : 학습 데이터가 주어지지 않는다. RL에서는 심지어 "A 조건에서 에이전트는 *반드시* B 행동을 취해야 한다"와 같은, 언제나 참인 예제가 없을 수도 있다. RL에서는 에이전트가 환경과 상호작용해 획득한 경험(experience)을 이용해 학습이 진행된다.

## 학습의 목표

- 지도학습 : 모델(model)이 학습 데이터로부터 일반화(generalize)된 정보를 습득하게 해, 학습 데이터에서 등장하지 않은 상황에서도 올바른 예측을 할 수 있게 하는 것이 목표이다.
- 비지도학습 : 데이터에서 숨겨진 구조(structure) 및 특징(feature)을 찾아내는 것이 목표이다.
- RL : 에이전트가 주어진 환경을 잘 파악하여, 최종적으로 획득하는 보상의 총합이 가장 크도록 하는 것이 학습의 목표이다.

# Exploitation vs. Exploration

환경 안에서 에이전트가 할 수 있는 행동은 Exploitation(이용), Exploration(탐색), 이렇게 두 가지로 크게 분류할 수 있다.

- Exploitation(이용) : 많은 보상을 얻기 위해, 과거에 시도해 보았던 행동 중 가장 큰 보상을 만드는 행동을 한다.
- Exploration(탐색) : 가능한 행동의 종류를 찾기 위해, 과거에 시도해보지 않았던 행동을 한다.

예를 들어 음식점을 선택하는 문제에서, 평소에 자주 가던 맛집을 가는 것은 Exploitation이다. 반면 새로운 맛집을 찾아보는 것은 Exploration이다(이 경우 보상은 "맛있는 식사"이다). 또한 환자에게 약을 투여하는 상황에서, 이미 검증된 약을 사용하는 것은 Exploitation이다. 반면 새로운 신약을 시험해보는 것은 Exploration이다(이 경우 보상은 "환자의 회복"이다).

Exploitation과 Exploration 중 어떤 것을 선택해야 할까? 이 문제를 Exploitation-Exploration Dilemma라 부른다. 두 종류의 행동은 모두 중요하고, 동시에 두 가지 행동을 할 수는 없으므로 한 종류의 행동을 하면 (해당 행동을 하는 동안) 다른 행동을 할 수 없다. 그러나 계속 한 가지 행동만 하면 별로 큰 보상을 얻지 못할 것이다. 지도학습이나 비지도학습에서는 이 문제가 발생하지 않는다. 이 문제를 해결하는 것이 RL의 목표 중 하나이다.

# RL의 핵심 요소

RL 시스템을 구성하는 핵심 요소는 정책(policy), 보상 시그널(reward signal), 가치 함수(value function), 모델(model), 이렇게 4가지가 있다.

{% include caption-img.html src="02-main-element-of-RL.png" title="Fig.01 Main Element of RL" %}

## 정책(policy)

정책(policy)은 특정 시간에 에이전트가 행동하는 방식을 결정한다. 즉 정책이란 현재 환경의 상태(state)를 입력값으로 받아 에이전트의 행동을 반환하는 일종의 함수라 할 수 있다. 많은 경우 정책은 확률적으로(stochastic) 주어진다(선택 가능한 행동 하나하나의 확률(probability)이 주어진다).

정책은 두 가지 종류로 구분할 수 있다.

- 결정론적 정책(deterministic policy) : 특정 상태(state)에서는 반드시 특정 행동(action)을 하도록 정해져 있는 정책
- 확률적 정책(stochastic policy) : 특정 상태(state)에서 어떤 행동(action)을 할 지 확률적으로 결정하는 정책

정책은 심리학에서의 자극-반응 규칙(stimulus-response rule)에 대응된다.

## 보상 시그널(reward signal)

보상 시그널(reward signal) 또는 보상(reward)은 RL 문제의 목표를 정의한다. 매 시간 간격마다(에이전트가 행동을 할 때마다) 환경은 에이전트에게 보상(reward)이라 부르는 숫자 하나를 제공한다. 에이전트의 유일한 목표는 장기적인 관점에서 이들 보상의 총합이 최대가 되게 하는 것이다. 즉 보상은 에이전트에게 무엇이 좋은지 무엇이 안 좋은지를 알려주는 것이다. 일반적으로 보상 시그널은 환경 상태와 실행한 행동을 입력값으로 받고 확률적으로(stochastic) 보상을 출력하는 함수 형태로 주어진다.

보상 시그널은 정책을 바꾸는 가장 중요한 지표 중 하나이다. 만약 현재의 정책으로 결정된 행동이 계속해서 낮은 보상을 만든다면, 미래에는 동일한 상황에 다른 정책을 적용하는 것이 좋을 것이다.

보상은 생명체가 받는 행복(pleasure) 또는 고통(pain)에 대응된다.

## 가치 함수(value function)

가치(value)란 현재 상태로부터 앞으로 받을 것으로 예상되는 모든 보상들의 총합을 의미한다. 보상이 현재 환경 상태가 좋은지 안좋은지에 대해 즉각적이고 본능적인 선호도(desirability)를 알려준다면, 가치는 장기적인 관점에서 앞으로 일어날 가능성이 높은 상태들과 이들로부터의 보상들에 대한 선호도를 의미한다고 할 수 있다. 

어떤 의미에서 보상은 1차적인 값이고, 보상의 예측값인 가치는 2차적인 값이라 할 수 있다. 보상이 없다면 가치도 없고, 가치를 예측하는 유일한 이유는 더 많은 보상을 받기 위함이다.

그러나 의사를 결정하거나 결정한 의사를 평가할 때는 가치를 기준으로 해야 한다. 에이전트는 가장 높은 보상을 주는 행동이 아닌, 가장 높은 가치를 주는 행동을 선택해야 한다. 그래야 장기적인 관점에서 받을 수 있는 보상을 최대화할 수 있기 때문이다.

하지만 물론 가치를 정하는 일은 보상을 정하는 일보다 훨씬 어려운 일이다. 보상은 환경으로부터 곧바로 주어지는 값이지만, 가치는 에이전트가 생애 내내 관측한 결과들을 토대로 만든 추정값이기 때문이다. 사실 우리가 앞으로 다룰 거의 대부분의 RL 알고리즘에서 가장 핵심적으로 다루는 부분이 어떻게 하면 효율적으로 가치를 추정하는지에 대해서이다.

## 모델(model)

모델(model)이란 환경의 동작을 모사하거나, 환경이 어떻게 동작할지를 추정한 것이다. 모델은 상태와 행동을 입력값으로 받아 앞으로의 상태와 보상을 예측해 출력하는 일종의 함수이다. RL 알고리즘에 따라 모델을 사용할 수도, 사용하지 않을 수도 있다.

모델은 계획(planning)에 사용된다. 계획이란 미래에 일어날 상황들을 고려하여 행동을 결정하는, 시행착오의 반댓말이다. 모델과 계획을 이용해 RL 문제를 푸는 방법을 모델 기반 방법(model-based method)이라 한다. 모델을 사용하지 않고 시행착오만을 통해 학습하는 방법은 model-free method라 한다.

# RL vs. Evolutionary Method

Evolutionary Method

ex. 유전 알고리즘(generic algorithm)

선택 가능한 policy가 적을때 유용
좋은 policy를 쉽게 찾고 평가할 수 있을때 유용

Evolutionary Method는 interaction을 통해 배우지 않음(value function이 없음)
state의 가치를 평가하지 않음 (policy가 좋은지 아닌지만 판단?)


# Tic-Tac-Toe 예제

Back Up


episodic task : 끝이 있는 task (ex. 바둑 : 승패가 딱 결정됨)
continuing task : 끊임없이 이어지는 task (ex. 청소로봇 : 계속 청소함)















<style>
.table-wrapper {
    overflow-x: auto;
}

table.my-table {
    border-collapse: collapse;
    margin-bottom: 1em;
    min-width: 400px;
}

table.my-table tbody tr td {
    padding-top: 0.5em;
    padding-bottom: 0.5em;
    padding-left: 0.5em;
    padding-right: 0.5em;
    border-top: 1px solid #888888;
    border-left: 1px solid #888888;
    border-right: 1px solid #888888;
    border-bottom: 1px solid #888888;
}

table.my-table tbody tr td:first-child {
    white-space: nowrap;
    text-align: center;
    background-color: lightblue;
}

table.my-table tbody tr td:last-child {
    width: 100%;
}
</style>