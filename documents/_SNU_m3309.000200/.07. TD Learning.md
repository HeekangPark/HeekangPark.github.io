---
title: "L07. TD Learning"
order: 7
date_created: "2021-04-28"
date_modified: "2021-05-28"
---

# TD Method란?

[이전 글](/SNU_m3309.000200/06-monte-carlo-methods)에서 살펴보았던 MC Method를 이용하면 환경에 대한 지식 없이도 경험만으로 최적 가치 함수 및 최적 정책을 찾을 수 있었다. 예를 들어 Nonstationary한 환경에서 every-visit MC Method를 사용하는 상황을 생각해 보자. 시간 $t$에서, 적절한 상수 $0 < \alpha \le 1$에 대해 가치 함수 $V(S\_t)$는 실제로 관측된 Sample Return $G\_t$를 이용해 다음과 같이 업데이트된다.[^1]

$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$$

[^1]: 이를 Constant-$\alpha$ MC Method라 한다.

그런데 MC Method는 한 에피소드가 끝나야 학습을 진행할 수 있다는 단점이 있다. Sample Return $G\_t$를 계산하려면 한 에피소드가 끝나야 하기 때문이다. 이 때문에 에피소드의 길이가 긴 경우 MC Method를 이용하면 학습 시간이 너무 길어진다. 또한 MC Method는 실시간으로 학습할 수 없고, Continuing Task에서는 사용할 수 없다.

그런데 생각해 보면 학습을 위해 꼭 에피소드의 종료까지 기다릴 필요가 없다. 상태 $S\_t$에서 $S\_{t+1}$로 전이하면서 보상 $R\_{t+1}$을 받는 것을 관측했다고 해 보자. 그렇다면, $S\_{t}$에서의 가치 함수 $V(S\_t )$의 추정값은 관측값 $R\_{t+1}$과 또 다른 추정값 $V(S\_{t+1})$을 이용해 다음과 같이 유의미한 업데이트할 수 있다.

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

이런 식으로 **한 추정값($V(S\_t)$)을 관측값($R_{t+1}$)과 다른 추정값($V(S\_{t+1})$)을 이용해 추정하는 방법**을 **TD(Temporal-Difference) Method**라 한다. 관측값을 통해 추정값을 추정한다는 점에서 TD Method는 MC Method와 유사하다. 또한 다른 추정값을 통해 한 추정값을 추정한다는 점에서[^2] TD Method는 DP와 유사하다. 즉 TD Method는 DP와 MC Method를 결합한 방법이라 이해할 수 있다.

[^2]: 이를 bootstrapping이라 한다.

DP에서는 보상과 다음 상태의 확률 분포 등을 알려주는, 환경에 대한 모델(model)이 필요했다. 그러나 TD Method에서는 모델이 없어도 된다. TD Method는 MC Method에서처럼 경험으로부터 학습이 가능하다. 또한 TD Method에서는[^3] 한 스텝(time step)만 기다리면 되기 때문에 MC Method와 다르게 실시간 학습(online learning)이 가능하다.

[^3]: 정확히는, TD(0) Method에서는

# (Tabular) TD(0) Method

위에서 살펴본 TD Method의 업데이트 식을 조금 더 자세히 알아보자.

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

이 식은 **(Tabular) TD(0) Method** 또는 **(Tabular) one-step TD Method**라 불리는, 가장 간단한 TD Method 업데이트 식이다.

(Constant-$\alpha$) MC Method에서는 $V(S\_t)$가 $G\_t$와 가까워지도록 업데이트가 진행된다. 다시 말해, MC Method의 업데이트의 목표(target)는 $G\_t$이다. 한편, TD(0) Method에서는 $V(S\_t)$가 $R\_{t+1} + \gamma V(S\_{t+1})$와 가까워지도록 업데이트가 진행된다. 다시 말해, TD(0) Method의 업데이트의 목표는 $R\_{t+1} + \gamma V(S\_{t+1})$이다.

TD(0) Method를 일반화하면 TD($\lambda$) Method, $n$-step TD Method라는 TD Method가 된다. 이들에 대해서는 다음 글에서 조금 더 자세히 다루도록 하겠다. 이번 글에서는 TD(0) Method를 이용하여 Prediction 문제와 Control 문제를 풀어보도록 하자.

## TD(0) Prediction

TD(0) Method의 업데이트 식

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

을 이용하면 특정 정책 $\pi$를 따를 때의 가치 함수 $v\_\pi$를 추정할 수 있다.

{:.pseudo-code-header}
TD(0) Prediction

<div class="pseudo-code" markdown="block">

입력 : 정책 $\pi$

{:.mb-1 .mt-1}
모든 $s \in \mathcal{S}^{+}$에 대해, $V(s) \in \mathbb{R}$을 임의의 값으로 초기화. 단, $V(\text{terminal}) = 0$.

<span class="keyword-highlight">Loop</span> for each episode:

<span class="indent-1"/>$S$를 임의의 상태 $s \in \mathcal{S}^{+}$으로 초기화

<span class="indent-1"/><span class="keyword-highlight">Loop</span> for each step of episode:

<span class="indent-2"/>$A \leftarrow \pi(S)$

<span class="indent-2"/>$A$ 시행하고, 보상 $R$과 다음 상태 $S'$ 관측

<span class="indent-2"/>$V(S) \leftarrow V(S) + \alpha [R + \gamma V(S') - V(S)]$

<span class="indent-2"/>$S \leftarrow S'$

<span class="indent-1"/><span class="keyword-highlight">until</span> $S$ is terminal

</div>

우리는 [이전 글](/SNU_m3309.000200/04-mdp#kramdown_정책policy과-가치-함수value-function)에서 다음을 배웠다.

$$
\begin{align}
v_\pi(s)
&= \mathbb{E}_\pi [G_t \,|\,S_t = s] \tag{1}\\[0.5em]
&= \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} \,|\,S_t = s]\\[0.5em]
&= \mathbb{E}_\pi [R_{t+1} + \gamma v_\pi (S_{t+1}) \,|\,S_t = s] \tag{2}\\[0.5em]
\end{align}
$$

DP에서는 위의 (2)번 식을 이용해 최적 정책을 추정했다. DP가 추정인 이유는 계산 과정에서 현재 단계에서 정확히 알 수 없는 $v\_\pi (S\_{t+1})$ 대신 추정값인 $V(S\_{t+1})$를 쓰기 때문이다. 한편 MC Method에서는 위의 (1)번 식을 이용해 최적 정책을 추정했다. MC Method가 추정인 이유는 (환경에 대한 모든 정보를 모르므로) $\mathbb{E}[G\_t]$ 대신 샘플링된 값(Sample Return)을 사용하기 때문이다.

DP와 MC Method를 결합한 TD Method의 결과 역시 추정값이다. TD Method는 (2)번 식을 이용해 최적 정책을 추정하는데, 현재 단계에서 정확히 알 수 없는 $v\_\pi (S\_{t+1})$ 대신 추정값인 $V(S\_{t+1})$를 쓰고, 기댓값 대신 샘플링된 값을 사용하기 때문이다.

참고로 DP처럼 가능한 모든 다음 값들을 이용해 현재 값을 업데이트하는 것을 **expected update**라 부른다. 반면 MC Method나 TD Method에서처럼 샘플링된 다음 값 하나만을 이용해 현재 값을 얻베이트하는 것을 **sample update**라 부른다.

### TD error

TD(0) Method의 업데이트 식

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [\bbox[yellow]{R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})}]$$

에서 대괄호 안 색칠한 부분을 **TD error** $\delta\_t$라 한다. 

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$$

$\delta\_t$를 계산하려면 보상 $R\_{t+1}$과 상태 $S\_{t+1}$가 필요하기에, $t$번째 TD error $\delta\_t$는 시점 $t+1$에서야 계산 가능해진다.

비슷하게, (Constant-$\alpha$) MC Method의 업데이트 식

$$V(S_t) \leftarrow V(S_t) + \alpha [\bbox[yellow]{G_t - V(S_t)}]$$

에서 대괄호 안 색칠한 부분을 MC error라 부르는데, (MC Method에서 그러는 것처럼) 한 에피소드 동안 $V$가 바뀌지 않는다면 MC error는 TD error의 합 형태로 표현할 수 있다.

$$\begin{align}
G_t - V(S_t)
&= R_{t+1} + \gamma G_{t+1} - V(S_t)\\[0.5em]
&= R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})\\[0.5em]
&= [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] + [\gamma G_{t+1} - \gamma V(S_{t+1})]\\[0.5em]
&= \delta_t + \gamma[G_{t+1} - V(S_{t+1})]\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 [G_{t+2} - V(S_{t+2})]\\[0.5em]
&= \vdots\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} [G_{T} - V(S_{T})]\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} [0 - 0]\\[0.5em]
&= \sum_{k=t} ^{T-1} \gamma^{k-t} \delta_k
\end{align}
$$

TD(0) Method에서는 그러나 매 시간 간격마다 $V$가 업데이트되므로 위 성질이 완벽히 성립하진 않지만, 만약 $\alpha$의 크기가 충분히 작다면 위 성질은 (근사적으로) 성립한다. 이 성질은 TD(0) Prediction으로도 가치 함수 $v\_{\pi}$를 추정할 수 있음을 보여준다. 구체적으로, 고정된 정책 $\pi$에 대해, $\alpha$가 다음 두 가지 중 하나인 경우 TD(0) Prediction을 이용하면 100%의 확률로 $V$는 실제 가치 함수 $v\_{\pi}$로 수렴한다.

{:.no-guide-line}
- $\alpha$가 충분히 작은 상수이다.
- $\alpha\_n$은 다음 조건을 따르는, 감소하는 변수이다.

{:.mathjax-mb-0 .mathjax-mt-0}
$$\sum_{n=1} ^{\infty}\alpha_n = \infty \qquad\text{and}\qquad \sum_{n=1} ^{\infty}\alpha_n ^2  = c$$

{:.text-align-center}
(단, $c$는 상수)

MC Prediction과 TD(0) Prediction 모두 $V$를 실제 가치 함수 $v\_{\pi}$로 수렴시킬 수 있다면, 어느 방법이 더 빨리 수렴할까? 즉, 어느 방법의 학습 속도가 더 빠를까? 또는, 어느 방법이 더 적은 데이터로도 더 효율적으로 학습할 수 있을까? 확률적인 문제(stochastic task)의 경우, TD Method가 Constant-$\alpha$ Method보다 경험적으로 더 빨리 수렴한다. 그러나 사실 이 질문은 아직까진 수학적으로 답이 나오지 않았다.

### 예제 : Random Walk

간단한 MRP(Markov Reward Process)[^4] 문제에서 Constant-$\alpha$ MC Prediction과 TD(0) Prediction을 적용해 둘을 비교해보자.

[^4]: 행동(action)이 없는 MDP 문제를 MRP(Markov Reward Process)라 한다. Prediction 문제에서는 행동을 결정할 필요가 없으므로, 논의를 단순하게 하기 위해 MRP 문제를 많이 사용한다.

<blockquote markdown="block">

{:.title}
Random Walk

Fig.01과 같은 게임판 위에서, 말은 다음과 같은 규칙으로 움직인다.

{:.no-guide-line}
- 모든 에피소드는 가운데 C에서 시작한다. 왼쪽 또는 오른쪽 끝(T)에 도착하면 에피소드가 종료된다.
- A, B, C, D, E에서는 50%의 확률로 왼쪽, 50%의 확률로 오른쪽으로 갈 수 있다.
- E에서 오른쪽 끝의 T로 가면 보상 +1을 받는다(그리고 에피소드가 종료된다). 이외의 모든 이동은 보상 0을 받는다.

예를 들어, 다음과 같은 에피소드가 나올 수 있다 : C, 0, B, 0, C, 0, D, 0, E, 1, T(종료)

{% include caption-img.html src="07-random-walk.png" title="Fig.01 Random Walk" %}

</blockquote>

위와 같은 상황에서, A, B, C, D, E 각 상태에서의 가치 함수는 얼마일까? $\gamma = 1$이라 하면, 각 상태에서의 가치 함수는 각 상태에서 시작했을 때 오른쪽 끝의 T에 도착할 확률과 같다. 즉,

$$v(\text{A}) = \frac{1}{6}\qquad
v(\text{B}) = \frac{2}{6}\qquad
v(\text{C}) = \frac{3}{6}\qquad
v(\text{D}) = \frac{4}{6}\qquad
v(\text{E}) = \frac{5}{6}\qquad
$$

이다. 이 값을 Constant-$\alpha$ MC Prediction과 TD(0) Prediction을 이용해 구해보자.

<div class="code-folder" markdown="block">

{:.code-header}
Random Walk

{% highlight python linenos %}
import random
import copy
import numpy as np
import matplotlib.pyplot as plt

labels = ["A", "B", "C", "D", "E"]
true_values = np.array([1/6, 2/6, 3/6, 4/6, 5/6])

class Prediction:
    def __init__(self):
        self.initial_values = {
            "A": 0.5,
            "B": 0.5,
            "C": 0.5,
            "D": 0.5,
            "E": 0.5,
            "T": 0
        }
        
    def generateEpisode(self):
        state_names = ["T", "A", "B", "C", "D", "E", "T"]
        state = 3
        
        episode = []
        while True:
            episode.append(state_names[state])
            if state_names[state] == "T":
                break
            
            delta = random.choice([-1, 1])
            
            if state == 5 and delta == 1:
                episode.append(1)
            else:
                episode.append(0)
                
            state += delta
        
        return episode
        
    def td0_predict(self, episode_num, alpha, gamma=1):
        values = copy.deepcopy(self.initial_values)
        
        history = np.zeros((episode_num + 1, 5))
        history[0] = [values[x] for x in labels]
        
        for e in range(episode_num):
            episode = self.generateEpisode()
            idx = 0
            while idx < len(episode) - 1:
                cur_state = episode[idx]
                reward = episode[idx + 1]
                next_state = episode[idx + 2]
                
                values[cur_state] = values[cur_state] + alpha * (reward + gamma * values[next_state] - values[cur_state])
                
                idx += 2
            
            history[e + 1] = [values[x] for x in labels]
            
        return history
    
    def mc_predict(self, episode_num, alpha, gamma=1):
        values = copy.deepcopy(self.initial_values)
        
        history = np.zeros((episode_num + 1, 5))
        history[0] = [values[x] for x in labels]
        
        for e in range(episode_num):
            episode = self.generateEpisode()
            idx = len(episode) - 3
            G = 0
            while idx >= 0:
                cur_state = episode[idx]
                reward = episode[idx + 1]
                
                G = reward + gamma * G
                values[cur_state] = values[cur_state] + alpha * (G - values[cur_state])
                
                idx -= 2
                
            history[e + 1] = [values[x] for x in labels]
            
        return history
    
def drawValues(results, title):
    plt.plot(range(len(labels)), true_values, c="k")
    for result in results:
        plt.plot(range(len(labels)), [result["values"][i] for i in range(len(labels))], marker='o', label=f"{result['episode_num']}")

    plt.xticks(range(len(labels)), labels)
    plt.title(title)
    plt.legend()
    plt.xlabel("States")
    plt.ylabel("Estimated Values")
    plt.show()

def drawErrors(td_results, mc_results):
    for i, result in enumerate(td_results):
        plt.plot(range(101), result["errors"], c=f"C{i}", label=f"TD(0) Prediction [α = {result['alpha']}]")
        
    for i, result in enumerate(mc_results):
        plt.plot(range(101), result["errors"], c=f"C{i}",label=f"MC Prediction [α = {result['alpha']}]", linestyle="--")

    plt.xticks([0, 25, 50, 75, 100])
    plt.xlim(0, 100)
    plt.ylim(0, 0.25)
    plt.title("RMS error")
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.xlabel("Episodes")
    plt.ylabel("RMS Errors")
    plt.show()

if __name__ == "__main__":
    predict = Prediction()
    
    # Values : TD(0) Prediction
    title = "TD(0) Prediction"
    episode_nums = [0, 1, 10, 100]
    results = [{"episode_num": x, "values": predict.td0_predict(episode_num=x, alpha=0.1)[-1]} for x in episode_nums]
    drawValues(results, title)
    
    # Values : MC prediction
    title = "MC Prediction"
    episode_nums = [0, 1, 10, 100]
    results = [{"episode_num": x, "values": predict.mc_predict(episode_num=x, alpha=0.1)[-1]} for x in episode_nums]
    drawValues(results, title)
    
    # RMS Errors
    alphas = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15]
    td_results = [{"alpha": alpha, "errors": np.mean([np.sqrt(np.mean(np.square(predict.td0_predict(episode_num=100, alpha=alpha) - true_values), axis=1)) for x in range(100)], axis=0)} for alpha in alphas]
    mc_results = [{"alpha": alpha, "errors": np.mean([np.sqrt(np.mean(np.square(predict.mc_predict(episode_num=100, alpha=alpha) - true_values), axis=1)) for x in range(100)], axis=0)} for alpha in alphas]
    drawErrors(td_results, mc_results)
{% endhighlight %}

- line 9 ~ 84 : `Prediction` 클래스
  - line 20 ~ 39 : `Prediction.generateEpisode()`
    - Random Walk 에피소드(ex. C, 0, B, 0, C, 0, D, 0, E, 1, T)를 생성하는 메소드
  - line 41 ~ 61 : `Prediction.td0_predict()`
    - TD(0) Prediction을 수행하는 메소드
    - (`episode_num + 1`, 5) 크기의 Numpy 배열을 반환
      - $i$행은 이때까지 $i$개의 에피소드를 사용해 (TD(0) Prediction으로) 현재까지 추정한 가치 함수를 담고 있다.
        - ex) 0번 행 : 0개의 에피소드를 사용해 현재까지 추정한 가치 함수(즉, 초기값)
        - ex) 100번 행 : 99개의 에피소드를 사용해 현재까지 추정한 가치 함수
      - 각 열은 앞에서부터 각각 A, B, C, D, E에서의 가치 함수를 의미한다.
  - line 63 ~ 84 : `Prediction.mc_predict()`
    - MC Prediction을 수행하는 메소드
    - (`episode_num + 1`, 5) 크기의 Numpy 배열을 반환
      - $i$행은 이때까지 $i$개의 에피소드를 사용해 (MC Prediction으로) 현재까지 추정한 가치 함수를 담고 있다.
        - ex) 0번 행 : 0개의 에피소드를 사용해 현재까지 추정한 가치 함수(즉, 초기값)
        - ex) 100번 행 : 99개의 에피소드를 사용해 현재까지 추정한 가치 함수
      - 각 열은 앞에서부터 각각 A, B, C, D, E에서의 가치 함수를 의미한다.
- line 86 ~ 96 : `drawValues()`
  - Fig.02, Fig.03의 그래프를 그린 함수
- line 98 ~ 112 : `drawErrors()`
  - Fig.04의 그래프를 그린 함수
- line 114 ~ 133 : main
  - 에피소드의 개수를 다르게 하여 TD(0) Prediction과 MC Prediction을 적용 (Fig.02, Fig.03)
  - $alpha$를 다르게 하여 TD(0) Prediction과 MC Prediction을 적용 (Fig.04)
    - Fig.04는 다음과 같은 방법으로 그려졌다:
      1. 현재 추정하고 있는 가치 함수가 `[0.5, 0.5, 0.5, 0.5, 0.5]`라 하자.
      2. 오차(error)를 구한다. 즉, 1에서 참값(`[1/6, 2/6, 3/6, 4/6, 5/6]`)을 뺀다 : `[0.333, 0.167, 0.000, -0.167, -0.333]`.
      3. 2를 제곱(Square)한다 : `[0.111, 0.028, 0.000, 0.028, 0.111]`.
      4. 3의 평균(Mean)을 구한다 : `0.056`.
      5. 4의 제곱근(Root)을 구한다. 이 값이 RMS Error이다 : `0.236`.
      6. 각 $\alpha$에 대해, 0개의 에피소드를 보고 추정한 가치 함수의 RMS Error, 1개의 에피소드를 보고 추정한 가치 함수의 RMS Error, ..., 100개의 에피소드를 보고 추정한 가치 함수의 RMS Error를 계산한다.
      7. 6의 과정을 100번 반복한다. 즉, 100개의 에피소드를 보는 학습을 처음부터 100번 반복한다.
      8. 각 $\alpha$에 대해, 7의 결과를 평균한다. 즉, 0개의 에피소드로 계산한 RMS Error 100개의 평균, 1개의 에피소드로 계산한 RMS Error 100개의 평균, ... 100개의 에피소드로 계산한 RMS Error 100개의 평균을 구한다. 이를 그래프로 나타낸 것이 Fig.04이다.

</div>

{% include caption-img.html src="07-random-walk-td-prediction.png" title="Fig.02 Random Walk - TD(0) Prediction" description="TD(0) Prediction을 이용해 가치 함수를 추정한 결과. 검은 선은 참값(true values)을 나타낸다." %}

{% include caption-img.html src="07-random-walk-mc-prediction.png" title="Fig.03 Random Walk - MC Prediction" description="MC Prediction을 이용해 가치 함수를 추정한 결과. 검은 선은 참값(true values)을 나타낸다." %}

{% include caption-img.html src="07-random-walk-rms-errors.png" title="Fig.04 Random Walk - RMS Errors" description="각 $\alpha$에 대해, TD(0) Prediction과 MC Prediction로 추정한 가치 함수에 대해 RMS Error를 계산한 결과. x축은 사용한 에피소드의 개수를, y축은 RMS Error를 의미한다. 실선은 TD(0) Prediction, 점선은 MC Prediction을 의미한다. 같은 색은 같은 $\alpha$를 의미한다." %}

Fig.02, Fig.03에서 볼 수 있듯이, 에피소드의 개수가 많아짐에 따라 가치 함수의 추정값은 점점 더 참값(검은 실선)에 가까워진다.[^5] 이때 Fig.04에서 볼 수 있듯이, TD(0) Prediction이 MC Prediction에 비해 더 적은 오류(RMS Error)를 가진다.[^6]

[^5]: 단, $\alpha$가 상수이기 때문에 두 방법 모두 완벽히 참값에 수렴하지는 않는다. $\alpha$로 상수를 사용하는 경우 가장 최근에 관측한 에피소드의 결과에 따라 추정값이 계속 요동친다.
[^6]: TD(0) Prediction의 RMS Error(실선)가 MC Prediction의 RMS Error(점선)보다 항상 작다(= 아래에 있다).

## TD(0) Control

TD(0) Method를 이용하여 Control 문제를 풀어보자. TD(0) Control 문제를 푸는 기본적인 아이디어는 [GPI](/SNU_m3309.000200/05-dp#kramdown_여담--gpi-generalized-policy-iteration)이다. 다만 이번엔 TD Prediction을 이용해 Evaluation 과정을 수행한다.

TD(0) Control의 해법에는 On-policy[^7] TD(0) Control 방법인 **SARSA**, 그리고 Off-policy[^8] TD(0) Control 방법인 **Q-Learning**, 이렇게 두 가지가 있다.

[^7]: 학습을 위한 정책(목표 정책, target policy)과 탐색을 위한 정책(행동 정책, behavior policy)이 동일한 학습법
[^8]: 학습을 위한 정책(목표 정책, target policy)과 탐색을 위한 정책(행동 정책, behavior policy)이 다른 학습법

### SARSA

On-policy TD(0) Control Method를 **SARSA**라 부른다. SARSA에서는 상태-가치 함수(state-value function) $v\_\pi$가 아닌, 행동-가치 함수 $q\_\pi$를 사용해 학습을 진행한다. TD(0) Method에서의 행동-가치 함수 추정값 $Q(S\_t,\,A\_t)$의 업데이트 식은 다음과 같다.

$$Q(S_{t},\, A_{t}) \leftarrow Q(S_{t},\, A_{t}) + \alpha [R_{t+1} + \gamma Q(S_{t+1},\, A_{t+1}) - Q(S_{t},\, A_{t})]$$

위 업데이트 식을 보면 $S\_t$, $A\_t$, $R\_{t+1}$, $S\_{t+1}$, $A\_{t+1}$, 이렇게 5개의 값을 사용한다. SARSA의 이름은 이로부터 나왔다. SARSA는 위 업데이트 식을 이용해 현재 정책 $\pi$에 대해 $q\_\pi$를 추정하고, 동시에 정책 $\pi$를 $q\_\pi$에 대해 탐욕적인 정책이 되게 바꾼다. 만약 모든 상태-행동 쌍(state-action pair)이 무한 번 방문되고(visit), 무한한 시간이 지난 후 정책이 탐욕적 정책(greedy policy)로 수렴하면,[^9] SARSA는 100%의 확률로 최적 정책 $\pi\_{*}$와 최적 가치 함수 $q\_{*}$로 수렴한다.

[^9]: ex) $\varepsilon$-greedy policy에서 $\varepsilon = 1/t$인 경우. 초반엔 $\varepsillon$이 크므로 탐색(exploration)이 많이 일어나 모든 상태-행동 쌍이 충분히 방문되고, 시간이 지나며 정책이 탐욕적 정책으로 수렴한다.

{:.pseudo-code-header}
SARSA(On-policy TD(0) Control) ($\varepsilon$-greedy policy 사용)

<div class="pseudo-code" markdown="block">
{:.mb-1}
모든 $s \in \mathcal{S}^{+}$, $a \in \mathcal{A}(s)$에 대해, $Q(s,\,a) \in \mathbb{R}$을 임의의 값으로 초기화. 단, $Q(\text{terminal},\,\cdot) = 0$.

<span class="keyword-highlight">Loop</span> for each episode:

<span class="indent-1"/>$S$를 임의의 상태 $s \in \mathcal{S}^{+}$으로 초기화

<span class="indent-1"/>현재 Q에 대해 $\varepsilon$-greedy policy를 사용하여 $S$에서의 행동 $A$를 선택

<span class="indent-1"/><span class="keyword-highlight">Loop</span> for each step of episode:

<span class="indent-2"/>$A$ 시행하고, 보상 $R$과 다음 상태 $S'$ 관측

<span class="indent-2"/>현재 Q에 대해 $\varepsilon$-greedy policy를 사용하여 $S'$에서의 행동 $A'$를 선택<span class="indent comment-highlight">//Improvement 과정</span>

<span class="indent-2"/>$Q(S,\,A) \leftarrow Q(S,\,A) + \alpha [R + \gamma Q(S',\,A') - Q(S,\,A)]$<span class="indent comment-highlight">//Evaluation 과정</span>

<span class="indent-2"/>$S \leftarrow S'$, $A \leftarrow A'$

<span class="indent-1"/><span class="keyword-highlight">until</span> $S$ is terminal

</div>

위 의사 코드에서, 현재 Q에 대해 $\varepsilon$-greedy policy를 사용하여 $S'$에서의 행동 $A'$를 선택하는 과정이 