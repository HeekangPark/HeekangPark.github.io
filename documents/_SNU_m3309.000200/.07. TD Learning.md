---
title: "L07. TD Learning"
order: 7
date_created: "2021-04-28"
date_modified: "2021-05-26"
---

# TD Method란?

[이전 글](/SNU_m3309.000200/06-monte-carlo-methods)에서 살펴보았던 MC Method를 이용하면 환경에 대한 지식 없이도 경험만으로 최적 가치 함수 및 최적 정책을 찾을 수 있었다. 예를 들어 Nonstationary한 환경에서 every-visit MC Method를 사용하는 상황을 생각해 보자. 시간 $t$에서, 적절한 상수 $0 < \alpha \le 1$에 대해 가치 함수 $V(S\_t)$는 실제로 관측된 Sample Return $G\_t$를 이용해 다음과 같이 업데이트된다.[^1]

$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$$

[^1]: 이를 Constant-$\alpha$ MC Method라 한다.

그런데 MC Method는 한 에피소드가 끝나야 학습을 진행할 수 있다는 단점이 있다. Sample Return $G\_t$를 계산하려면 한 에피소드가 끝나야 하기 때문이다. 이 때문에 에피소드의 길이가 긴 경우 MC Method를 이용하면 학습 시간이 너무 길어진다. 또한 MC Method는 실시간으로 학습할 수 없고, Continuing Task에서는 사용할 수 없다.

그런데 생각해 보면 학습을 위해 꼭 에피소드의 종료까지 기다릴 필요가 없다. 상태 $S\_t$에서 $S\_{t+1}$로 전이하면서 보상 $R\_{t+1}$을 받는 것을 관측했다고 해 보자. 그렇다면, $S\_{t}$에서의 가치 함수 $V(S\_t )$의 추정값은 관측값 $R\_{t+1}$과 또 다른 추정값 $V(S\_{t+1})$을 이용해 다음과 같이 유의미한 업데이트할 수 있다.

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

이런 식으로 **한 추정값($V(S\_t)$)을 관측값($R_{t+1}$)과 다른 추정값($V(S\_{t+1})$)을 이용해 추정하는 방법**을 **TD(Temporal-Difference) Method**라 한다. 관측값을 통해 추정값을 추정한다는 점에서 TD Method는 MC Method와 유사하다. 또한 다른 추정값을 통해 한 추정값을 추정한다는 점에서[^2] TD Method는 DP와 유사하다. 즉 TD Method는 DP와 MC Method를 결합한 방법이라 이해할 수 있다.

[^2]: 이를 bootstrapping이라 한다.

## TD Method의 장점

TD Method는 DP에 비해 다음과 같은 장점이 있다.

{:.no-guide-line}
- TD Method는 보상과 다음 상태의 확률 분포 등을 알려주는 환경에 대한 모델(model)이 없어도 된다 : TD Method는 경험으로부터 학습이 가능하다.

TD Method는 MC Method에 비해 다음과 같은 장점이 있다.

{:.no-guide-line}
- TD Method는 실시간 학습(online learning)이 가능하다 : MC Method에서는 Return을 계산해야 하기 때문에 에피소드가 끝날 때까지 기다려야 하지만, TD Method에서는[^3] 한 스텝(time step)만 기다리면 된다.
- 

[^3]: 정확히는, TD(0) Method에서는

# (Tabular) TD(0) Method

위에서 살펴본 TD Method의 업데이트 식을 조금 더 자세히 알아보자.

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

이 식은 **(Tabular) TD(0) Method** 또는 **(Tabular) one-step TD Method**라 불리는, 가장 간단한 TD Method 업데이트 식이다. 이 식을 일반화하면 TD($\lambda$) Method, $n$-step TD Method라는 TD Method가 된다. 이들에 대해서는 다음 글에서 조금 더 자세히 다루도록 하겠다. 이번 글에서는 TD(0) Method를 이용하여 Prediction 문제와 Control 문제를 풀어보도록 하자.

## TD(0) Prediction

TD(0) Method의 업데이트 식

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})]$$

을 이용하면 특정 정책 $\pi$를 따를 때의 가치 함수 $v\_\pi$를 추정할 수 있다.

{:.pseudo-code-header}
TD(0) Prediction

<div class="pseudo-code" markdown="block">

입력 : 정책 $\pi$

{:.mb-1 .mt-1}
모든 $s \in \mathcal{S}^{+}$에 대해, $V(s) \in \mathbb{R}$을 임의의 값으로 초기화. 단, $V(\text{terminal}) = 0$.

<span class="keyword-highlight">Loop</span> for each episode:

<span class="indent-1"/>$S$를 임의의 상태 $s \in \mathcal{S}^{+}$으로 초기화

<span class="indent-1"/><span class="keyword-highlight">Loop</span>:

<span class="indent-2"/>$A \leftarrow \pi(S)$

<span class="indent-2"/>$A$ 시행하고, 보상 $R$과 다음 상태 $S'$ 관측

<span class="indent-2"/>$V(S) \leftarrow V(S) + \alpha [R + \gamma V(S') - V(S)]$

<span class="indent-2"/>$S \leftarrow S'$

<span class="indent-1"/><span class="keyword-highlight">until</span> $S$ is terminal

</div>

우리는 [이전 글](/SNU_m3309.000200/04-mdp#kramdown_정책policy과-가치-함수value-function)에서 다음을 배웠다.

$$
\begin{align}
v_\pi(s)
&= \mathbb{E}_\pi [G_t \,|\,S_t = s] \tag{1}\\[0.5em]
&= \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} \,|\,S_t = s]\\[0.5em]
&= \mathbb{E}_\pi [R_{t+1} + \gamma v_\pi (S_{t+1}) \,|\,S_t = s] \tag{2}\\[0.5em]
\end{align}
$$

DP에서는 위의 (2)번 식을 이용해 최적 정책을 추정했다. DP가 추정인 이유는 계산 과정에서 현재 단계에서 정확히 알 수 없는 $v\_\pi (S\_{t+1})$ 대신 추정값인 $V(S\_{t+1})$를 쓰기 때문이다. 한편 MC Method에서는 위의 (1)번 식을 이용해 최적 정책을 추정했다. MC Method가 추정값인 이유는 (환경에 대한 모든 정보를 모르므로) $\mathbb{E}[G\_t]$ 대신 샘플링된 값(Sample Return)을 사용하기 때문이다.

DP와 MC Method를 결합한 TD Method의 결과 역시 추정값이다. TD Method는 (2)번 식을 이용해 최적 정책을 추정하는데, 현재 단계에서 정확히 알 수 없는 $v\_\pi (S\_{t+1})$ 대신 추정값인 $V(S\_{t+1})$를 쓰고, 기댓값 대신 샘플링된 값을 사용하기 때문이다.

참고로 DP처럼 가능한 모든 다음 값들을 이용해 현재 값을 업데이트하는 것을 **expected update**라 부른다. 반면 MC Method나 TD Method에서처럼 샘플링된 다음 값 하나만을 이용해 현재 값을 얻베이트하는 것을 **sample update**라 부른다.

### TD error

TD(0) Method의 업데이트 식

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha [\bbox[yellow]{R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})}]$$

에서 대괄호 안 색칠한 부분을 TD error라 부르고, $\delta\_t$라 쓴다. 

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})$$

TD error $\delta\_t$를 계산하려면 보상 $R\_{t+1}$과 상태 $S\_{t+1}$을 사용하기에, $t$번째 TD error $\delta\_t$는 시점 $t+1$에서야 계산 가능해진다.

비슷하게, (Constant-$\alpha$) MC Method의 업데이트 식

$$V(S_t) \leftarrow V(S_t) + \alpha [\bbox[yellow]{G_t - V(S_t)}]$$

에서 대괄호 안 색칠한 부분을 MC error라 부르는데, (MC Method에서 그러는 것처럼) 한 에피소드 동안 $V$가 바뀌지 않는다면 MC error는 TD error의 합 형태로 표현할 수 있다.

$$\begin{align}
G_t - V(S_t)
&= R_{t+1} + \gamma G_{t+1} - V(S_t)\\[0.5em]
&= R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})\\[0.5em]
&= [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] + [\gamma G_{t+1} - \gamma V(S_{t+1})]\\[0.5em]
&= \delta_t + \gamma[G_{t+1} - V(S_{t+1})]\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 [G_{t+2} - V(S_{t+2})]\\[0.5em]
&= \vdots\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} [G_{T} - V(S_{T})]\\[0.5em]
&= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} [0 - 0]\\[0.5em]
&= \sum_{k=t} ^{T-1} \gamma^{k-t} \delta_k
\end{align}
$$

TD(0) Method에서는 그러나 매 시간 간격마다 $V$가 업데이트되므로 위 성질이 완벽히 성립하진 않지만, 만약 $\alpha$의 크기가 충분히 작다면 위 성질은 근사적으로 성립한다.







거칠게 말하자면, MC Method는 
(Constant-$\alpha$) MC Method에서는 $V(S\_t)$가 $G\_t$와 가까워지도록 업데이트가 진행된다. 다시 말해, MC Method의 업데이트의 목표(target)는 $G\_t$이다. 한편, TD(0) Method에서는 $V(S\_t)$가 $R\_{t+1} + \gamma V(S\_{t+1})$와 가까워지도록 업데이트가 진행된다. 다시 말해, TD(0) Method의 업데이트의 목표는 $R\_{t+1} + \gamma V(S\_{t+1})$이다.

## TD(0) Control

TD(0) Method를 이용하여 Control 문제를 푸는 기본적인 아이디어는 [DP]([/SNU_m3309.000200/05-dp), [MC Method](/SNU_m3309.000200/06-monte-carlo-methods)에서처럼 [GPI](/SNU_m3309.000200/05-dp#kramdown_여담--gpi-generalized-policy-iteration)를 이용하는 것이다.