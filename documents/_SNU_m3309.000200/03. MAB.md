---
title: "L03. MAB(Multi-armed Bandits)"
order: 3
date: "2021-03-15"
---

# Evaluative Feedback vs. Instructive Feedback

RL과 지도학습(Supervised Learning)의 큰 차이 중 하나가, RL은 Evaluative Feedback을 이용해 학습되고 지도학습은 Instructive Feedback을 이용해 학습이 진행된다는 것이다. 

Evaluative Feedback은 (에이전트가 취한 행동에 대한 결과값으로서) 취한 행동이 얼마나 좋은지를 알려주는 값이다. 단 Evaluative Feedback은 어떤 행동이 최고의 행동인지 알려주진 않는다. Evaluative Feedback은 에이전트가 이때까지 취한 행동에 대해 종속적인 값이다. RL은 이 값을 이용해 학습을 진행한다.

한편 Instructive Feedback은 어떤 행동이 맞는 행동인지(= 최고의 행동인지)를 알려주는 값이다. 이 값은 에이전트가 이때까지 취한 행동과는 독립적인 값이다.[^1] 지도학습은 이 값을 이용해 학습을 진행한다.

[^1]: 원래 피드백이라는 단어는 어떤 행동을 했을 때 그에 대한 결과값을 뜻하는 용어이다. 그렇기에 Evaluative Feedback에서의 피드백이라는 단어는 적절한 단어다. 말 그대로 에이전트의 행동에 의해 에이전트를 둘러싸고 있는 환경이 그 결과로서 제공하는 값이기 때문이다. 그러나 Instructive Feedback의 경우 행동과 관련없는 독립적인 값으로, 피드백이라는 단어가 사실 적절치 않다. Instructive Feedback이라는 용어는 순전히 Evaluative Feedback에 대한 반댓말로서, Evaluative Feedback과 대구(對句)를 이루고자 Feedback이라는 단어를 쓴 것이다. 

# k-armed Bandit Problem

다음 문제를 생각해 보자.

> 철수는 미국 여행 중 라스베가스 공항을 경유하게 되었다. 다음 비행기는 2시간 후 출발할 예정이라, 철수는 그동안 라스베가스 공항의 명물, 슬롯머신을 당겨 보기로 했다. 다행히 공항에 사람이 별로 없어서 철수는 10대의 슬롯머신을 전세내고 사용할 수 있다. 각 슬롯머신에서 돈을 딸 확률은 기기마다 각자 다른 값으로 고정되어 있고, 한 번에 한 대의 슬롯머신만 당길 수 있다고 할 때, 철수는 어떤 전략을 취해야 가장 많은 돈을 딸 수 있을까?

만약 철수가 전체 슬롯머신의 확률 분포를 정확히 알고 있다면 당연히 그 중 가장 높은 확률을 가진 기기만 2시간 내내 당기는 것이 가장 많은 돈을 따는 방법일 것이다. 하지만 철수는 ~~당연히~~ 슬롯머신들의 확률 분포를 정확히 모르기에 실험을 통해 이를 추정해야 한다. 더 많은 슬롯 머신에 대해 더 많은 실험을 진행할수록 더 정확한 확률 분포를 알 수 있겠지만, 이 과정에 너무 많은 시간을 투자하면 막상 결과적으로 얻는 최종 액수는 적어질 것이다.

이와 같이 k개의 선택 가능한 옵션이 있고, 한 옵션을 선택하면 고정된 확률 분포(stationary probability distribution)에 따라 보상(numerical reward)을 받을 수 있을 때, 특정 시간(횟수) 동안 받을 수 있는 전체 보상을 최대화하는 문제를 k-armed Bandit Problem 또는 Multi-armed Bandit Problem(MAB)이라 한다.[^2]

[^2]: 많은 책이나 블로그에서 K-armed Bandit Problem을 잘못 해석해 "외팔이 강도(One-armed Bandit)가 k개의 슬롯머신을 당길 때 보상을 최대화하는 문제"라 설명을 하는데, 사실 one-armed bandit는 미국에서는 슬롯 머신을 부르는 속어이다(팔 하나(레버)를 달고 당신을 털어가기 때문). k-armed Bandit Problem은 여기에서 이름을 따온 것이다. 즉 k-armed Bandit Problem이라 하면 (정상적인 사람이) 레버가 k개 있는 슬롯 머신에서, k개의 레버 중 하나를 골라 당기는 과정을 여러 번 반복할 때 받을 전체 보상을 최대화하는 문제를 뜻한다.

MAB에서 현재 추정하고 있는 각 행동(action)의 가치(value) 중 가장 큰 값을 가지는 행동을 "탐욕적 행동(greedy action)"이라 한다.[^3] 만약 당신이 탐욕적 행동을 선택한다면 당신은 이전 지식을 활용(exploiting)한 것이다. 하지만 만약 비탐욕적 행동(nongreedy action)을 선택한다면 당신은 탐색(exploring)을 한 것이다. 이를 통해 해당 비탐욕적 행동의 가치를 조금 더 정확히 추정할 수 있게 되기 때문이다. 탐색(Exploration)을 수항하면 단기간 수령하는 보상은 작을 수 있으나, 장기적으로 봤을 때 전체적으로 받을 보상이 커진다. MAB에서는 한 번에 한 가지 선택만 할 수 있으므로[^4] 탐색(Exploration)과 활용(Exploitation)은 서로 상충되는 관계이다. 둘 중 어떤 것을 할지 결정하기 위해서는 추정한 가치의 정확도(precision), 불확실성(uncertainty), 남은 횟수(remaining step) 등 다양한 값들을 복합적으로 고려해야 한다.

[^3]: 당연히 가치는 추정값이기에, 시간이 지나면서 탐욕적 행동 역시 바뀔 것이다.
[^4]: 사실 MAB뿐만 아니라 많은 경우 한 번에 한 가지 선택밖에 할 수 없다.

# Action-value Method

Action-value Method란 각 행동(action)으로 인해 얻을 수 있을 가치(value)를 추정하여 다음 행동을 선택할 때 이를 사용하는 방법을 뜻한다.

## Sample-average Method

각 행동의 가치를 추정하는 가장 간단한 방법은 Sample-average Method이다. 이 방법은 이때까지의 실험 결과로 얻은 보상들을 평균내어 가치로 사용하는 방법이다. 즉 특정 시간 $t$에서 특정 행동 $a$의 가치를 $Q\_t(a)$라 할 때,

$$Q_t(a) = \frac{\text{시간 }t\text{ 이전에 }a\text{가 실행되며 받은 모든 보상의 합}}{\text{시간 }t\text{ 이전에 }a\text{가 실행된 횟수}} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$

이라 추정하는 것이다. 여기서 $\mathbb{1}\_{predicate}$는 $predicate$ 조건이 참일 때 1, 거짓일 때 0이 되는 랜덤 변수(random variable)이다.

만약 위 식에서 분모가 0이면(즉 특정 시간 $t$ 동안 행동 $a$가 한 번도 시행되본 적 없다면) $Q\_t(a)$로 적당한 디폴트 값(ex. 0)을 사용한다. 큰 수의 법칙(the law of large numbers)에 의해, $a$를 여러 번 시행하면 할 수록 $Q\_t(a)$는 실제 가치 $q\_{*}(a)$로 수렴한다.

## ε-Method

추정된 가치를 바탕으로 다음 행동을 선택하는 가장 간단한 방법은 $\varepsilon$-Method이다. 이 방법은 대체로 탐욕적 행동(가장 높은 추정 가치를 가진 행동, 즉 현재 추정 중인 최선 행동(optimal action))을 선택하다가, 간간히 $\varepsilon$의 확률로 가능한 모든 행동들 중 하나를 무작위로 선택하는 방법이다. $\varepsilon$의 확률로 행동을 무작위로 선택할 때는 추정한 행동들의 가치와 독립적으로, 모든 행동들이 동일한 확률을 가지고 있는 상태에서 하나를 뽑는다. 즉 $1-\varepsilon$의 확률로 활용(Exploitation)을 수행하고, $\varepsilon$의 확률로 탐색(Exploration)을 수행하는 것이다.

그렇다면 $\varepsilon$이 얼마일 때가 가장 좋을까? 10-armed Testbed에 대해 실험을 진행하여 이를 확인해보자.

참고로 k-armed Testbed는 k-armed Bandit Problem을 해결하기 위한 Action-value Method의 성능을 비교하기 위해 사용하는 데이터 셋으로, 다음과 같은 방법으로 만든다.

- $k$개의 행동들에 대해, 각 행동 $a\_i$($1 \le i \le k$)의 가치의 참값 $q\_*(a\_i)$는 평균 0, 분산 1인 정규분보에서 무작위로 추출한다.
- 특정 시간 $t$에서 특정 행동 $A\_t \in \\{ a\_i \| 1 \le i \le k \\}$가 선택되었을 때, 이 행동으로 받는 보상 $R\_t$는 평균 $q\_*(A\_t)$, 분산 1인 정규분포에서 무작위로 추출한다.

{% include caption-img.html src="03-10-armed-testbed.png" title="Fig.01 10-armed Testbed" description="출처 : Richard S. Sutton, Andrew G. Barto, <Reinforcement Learning : An Introduction>, 2nd Edition, pg. 28, Figure 2.1" %}

10-armed Testbed를 이용해 $\varepsilon=0.001$, $\varepsilon=0.1$, $\varepsilon=0$(Greedy Method)일 때의 $\varepsilon$-Method의 성능을 비교해 보자. 

<div class="folder">
<p class="folder-btn folder-open-btn">코드 열기</p>
<p class="folder-btn folder-close-btn hidden">코드 닫기</p>

<div class="folder-content hidden" markdown="block">

{% highlight python linenos %}
import numpy as np
from matplotlib import pyplot as plt

class GreedyAgent:
    def __init__(self, k, epsilon):
        self.k = k
        self.count = np.zeros(k, dtype=float)
        self.reward_sum = np.zeros(k, dtype=float)
        self.epsilon = epsilon
    
    def getEstimatedValues(self):
        return np.divide(self.reward_sum, self.count, out=np.zeros_like(self.reward_sum), where=self.count!=0)
    
    def chooseAction(self):
        if np.random.random() < self.epsilon: # exploration
            action = np.random.choice(self.k)
        else: # exploitation
            max_actions = np.argwhere(self.getEstimatedValues() == np.max(self.getEstimatedValues())).flatten()
            action = np.random.choice(max_actions)
            
        return action
    
    def train(self, k, reward):
        self.count[k] += 1
        self.reward_sum[k] += reward
        
    def getTotalReward(self):
        return np.sum(self.reward_sum)
    
    def getAverageReward(self):
        return np.sum(self.reward_sum)/np.sum(self.count)

class Testbed:
    def __init__(self, k):
        self.true_values = np.random.normal(size=k)
        self.k = k
    
    def sample(self, k):
        assert k in range(self.k), "Invalid param:k"
        return np.random.normal(loc=self.true_values[k], scale=1)
    
    def getTrueValues(self):
        return self.true_values

trial = 2000
repeats = 100
epsilons = [0, 0.1, 0.01]
k = 10

average_rewards_sum = np.zeros((len(epsilons), trial), dtype=float)
optimal_action_count = np.zeros((len(epsilons), trial), dtype=int)
for repeat in range(repeats):
    testbed = Testbed(k=k)
    agents = [GreedyAgent(k=k, epsilon=e) for e in epsilons]
    
    optimal_action = np.argmax(testbed.getTrueValues())
    
    for i, agent in enumerate(agents):
        average_rewards = np.zeros(trial, dtype=float)
        
        for t in range(trial):
            action = agent.chooseAction()
            sample = testbed.sample(k=action)
            agent.train(k=action, reward=sample)
            
            average_rewards[t] = agent.getAverageReward()
            if action == optimal_action:
                optimal_action_count[i][t] += 1
            
        average_rewards_sum[i] += average_rewards

average_rewards_sum = average_rewards_sum / repeats
optimal_action_count = optimal_action_count / repeats

for i in range(len(epsilons)):
    line = plt.plot(np.arange(trial) + 1, average_rewards_sum[i], label=f"{epsilons[i]}-greedy")
    
plt.title("Average Reward")
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.legend()
plt.show()

for i in range(len(epsilons)):
    plt.plot(np.arange(trial) + 1, optimal_action_count[i], label=f"{epsilons[i]}-greedy")
    
plt.title("Optimal Action Rate")
plt.xlabel("Steps")
plt.ylabel("Optimal Action Rate")
plt.legend()
plt.show()
{% endhighlight %}

- line 4 ~ 31 : $\varepsilon$-greedy Method를 수행하는 에이전트
  - line 5 : `GreedyAgent.__init__()` 메서드
    - k-armed bandit problem에서의 k(당길 수 있는 팔 수), $\varepsilon$ 값을 인자로 받음
  - line 11 : `GreedyAgent.getEstimatedValues()` 메서드
    - Sample-average Method로 이때까지의 수행 결과로부터 가치(value)를 예상한 값을 반환
  - line 14 : `GreedyAgent.chooseAction()` 메서드
    - 0 이상 1 미만의 랜덤값에 대해, 그 값이 $\varepsilon$보다 작으면(즉 $\varepsilon$의 확률로) 탐색(Exploration)을, $\varepsilon$보다 크면(즉 $1 - \varepsilon$의 확률로) 활용(Exploitation)을 수행
  - line 23 : `GreedyAgent.train()` 메서드
    - 특정 실행에서 `k`번째 팔을 당겼더니 `reward`만큼의 보상을 받았을 때, 이 메서드를 이용해 이 경험을 저장한다.
  - line 27, 30 : `GreedyAgent.getTotalReward()`, `GreedyAgent.getAverageReward()` 메서드
    - 각각 이때까지의 시행 동안 받은 모든 보상의 합, 평균을 반환
- line 33 ~ 43 : k-armed Testbed
  - line 34 : `Testbed.__init__()` 메서드
    - k-armed Testbed에서의 k 값을 인자로 받음
  - line 38 : `Testbed.sample()` 메서드
    - `k`번째 팔을 당겼을 때의 보상을 출력
  - line 42 : `Testbed.getTrueValues()` 메서드
    - k-armed Testbed의 실제 가치(value)를 출력
- line 45 ~ 48 : 각 게임당 팔을 당길 수 있는 기회 횟수 `trial`, 시뮬레이션을 반복할 횟수 `repeats`, 시도해 볼 에이전트의 종류 `epsilons`, 당길 수 있는 팔 수 `k` 초기화
- line 52 ~ 73 : 총 `repeats`번 시뮬레이션 진행
  - 한 시뮬레이션에서는 다음 과정을 진행
    - line 53 : Testbed 생성
    - line 54 : `epsilons`에 있는 값들을 $\varepsilon$으로 가지는 $\varepsilon$-greedy 에이전트 생성
    - line 56 : 생성된 Testbed로부터 최적 행동(= 가치가 가장 큰 행동)을 찾아 저장 (Optimal Action Rate 계산할 때 사용할 예정)
    - line 58 ~ 70 : 각 에이전트마다 총 `trial`번 행동을 수행
- line 75 ~ 82 : Average Reward 그래프 그리기
- line 84 ~ 91 : Optimal Action Rate 그래프 그리기

</div>

<script>
$(".document .document-content .folder .folder-btn.folder-open-btn").click(function() {
    let open_btn = $(this);
    let close_btn = $(this).siblings(".folder-close-btn");
    let content = $(this).siblings(".folder-content");

    open_btn.addClass("hidden");
    close_btn.removeClass("hidden");
    content.removeClass("hidden");
});

$(".document .document-content .folder .folder-btn.folder-close-btn").click(function() {
    let open_btn = $(this).siblings(".folder-open-btn");
    let close_btn = $(this);
    let content = $(this).siblings(".folder-content");

    open_btn.removeClass("hidden");
    close_btn.addClass("hidden");
    content.addClass("hidden");
});
</script>

<style>
.document .document-content .folder .hidden {
    display: none;
}

.document .document-content .folder .folder-btn {
    text-align: center;
    color: #0275d8;
    cursor: pointer;
}

.document .document-content .folder .folder-btn:hover {
    text-decoration: underline;
}
</style>
</div>

{% include caption-img.html src="03-epsilon-method-average-reward.png" title="Fig.02 Average Reward (예시)" description="$\varepsilon=0$, $\varepsilon=0.1$, $\varepsilon=0.01$일 때 $\varepsilon$-greedy Method를 적용 (k=10, trial=2000, repeats=10)" %}

{% include caption-img.html src="03-epsilon-method-optimal-action-rate.png" title="Fig.03 Optimal Action Rate (예시)" description="$\varepsilon=0$, $\varepsilon=0.1$, $\varepsilon=0.01$일 때 $\varepsilon$-greedy Method를 적용 (k=10, trial=2000, repeats=10)" %}

위 그래프에서 볼 수 있듯이 ($\varepsilon$이 얼마든) $\varepsilon$-greedy Method를 적용하면 경험이 쌓일수록 받을 수 있는 평균 보상이 커진다. 특히 Greedy Method($\varepsilon=0$)의 경우 처음에는 다른 방법들보다 조금 더 빨리 좋아지지만, 최적 행동(optimal action)이 아닌 차(次)적 행동(suboptimal action)에 빠지기 쉬워 시간이 지나면 가장 낮은 성능을 보이게 된다. $\varepsilon=0.01$일 때는 $\varepsilon=0.1$일 때보다 최적 행동을 찾는 데 더 오랜 시간이 걸려 비교적 천천히 좋아진다.하지만 충분히 오랜 시간이 지나면 최적 행동을 최대 91%의 확률로만 시행할 수 있는 $\varepsilon=0.1$보다 최대 99.1%의 확률로 시행할 수 있는 $\varepsilon=0.01$가 받는 보상이 더 많아진다. 만약 시간이 지남에 따라 점점 감소하는 $\varepsilon$을 사용한다면 $\varepsilon$이 클 때 빠르게 최적 행동을 탐색하는 장점과 $\varepsilon$이 작을 때 받는 보상을 최대화하는 장점[^5]을 합칠 수 있다.


사실 최적의 $\varepsilon$ 값은 지금 풀고자 하는 문제에 따라 달라진다. 만약 Testbed의 분산이 1이 아닌 10이었다면, $\varepsilon$의 값이 커야 좋을 것이다. 하지만 만약 분산이 0이었다면 Greedy Method($\varepsilon=0$)이 가장 좋은 방법일 것이다. 또한 만약 각 행동들의 실제 가치(true value) $q\_{*}(a)$가 고정되어 있지 않다면(nonstationary) 탐색(exploration)이 매우 중요해진다. 탐색을 통해 현재 추정 중인 최선의 행동보다 더 좋은 행동이 있는지를 찾아야 하기 때문이다.

## Incremental Implementation

Sample-average Method는 각 옵션의 가치를 추정한 값으로 각 옵션들이 이때까지 시행 결과 받은 시행당 평균 보상을 사용한다. 이를 위해서 위 코드에서는 가치 추정값을 사용할 때마다 각 옵션이 선택되어 받은 보상의 합을 저장하는 `reward_sum` 변수를 각 옵션들이 선택된 횟수를 저장하는 `count` 변수로 나눠 매번 평균을 계산했다. 총 $k$개의 옵션이 있다고 할 때, 이 방법은 매 시행마다 k번의 나눗셈 연산이 필요하다. 이를 최적화할 수는 없을까?

만약 특정 옵션이 이때까지 $n - 1$번 뽑혔고, 그때마다 각각 $R\_1$, $R\_2$, ..., $R\_{n - 1}$만큼의 보상을 받았다 해 보자. 그렇다면 해당 옵션의 가치 $Q\_{n}$[^5]은 다음과 같이 계산 가능하다.

[^5]: $Q\_{n - 1}$이 아닌 $Q\_{n}$인 이유는 다음과 같다: k-armed Bandit Problem을 푸는 과정은 이전 결과들로부터 각 행동의 가치를 추정하고, 이를 기반으로 행동을 결정하고, 이를 수행해 보상을 받는 과정을 반복하는 것이다. 즉, 시점 $n - 1$에서 $n - 1$번째 보상을 받았을 때, 이를 이용해 가치를 업데이트하는 것은 시점 $n$일 때이므로 $Q\_{n - 1}$이 아닌 $Q\_n$라 하는 것이다.

$$Q_n = \frac{R_1 + R_2 + \cdots + R\_{n - 1}}{n - 1}$$

그렇다면 $Q\_{n+1}$은 다음과 같이 계산할 수 있다.

$$\begin{align}
Q_{n+1}
&= \frac{R_1 + R_2 + \cdots + R_n}{n}\\[0.5em]
&= \frac{R_1 + R_2 + \cdots + R_{n - 1}}{n - 1} \cdot \frac{n - 1}{n} + \frac{R_n}{n}\\[0.5em]
&= Q_n \cdot (1 - \frac{1}{n}) + \frac{R_n}{n}\\[0.5em]
&= Q_n + \frac{1}{n}(R_n - Q_n)
\end{align}
$$