---
title: "이미지 분류 (Image Classification)"
order: 2
date_created: "2020-12-22"
date_modified: "2020-12-22"
---

[Image Classification note](http://cs231n.github.io/classification)

# Image Classification

## Motivation

이미지 분류(Image Classification) 문제란 주어진 이미지에 대해 사전에 정해진 레이블(label) 집합의 한 원소를 할당하는 문제이다. 이 문제는 컴퓨터 비전 영역의 핵심 문제 중 하나로서, 문제의 단순함에도 불구하고 다양한 응용 분야를 가지고 있다. 또한 객체 탐지(Object Detection), 세그멘테이션(Segmentation)과 같이 이미지 분류 문제와 연관이 적어 보이는 컴퓨터 비전 영역의 다른 과제들도 이미지 분류 문제로 환원시킬 수 있다.

## Example

아래의 예에서 이미지 분류 모델(Image Classification Model)은 하나의 이미지를 입력받아 4개의 레이블(`{고양이(cat), 강아지(dog), 모자(hat), 컵(mug)}`)에 대한 확률을 계산한다. 컴퓨터에게 이미지란 하나의 커다란 3차원 행렬에 불과하다. 아래의 예에서 고양이 이미지는 가로 248 픽셀, 세로 400 픽셀에, RGB 3개의 채널을 가지고 있다. 즉 컴퓨터에게 있어 이 이미지는 248×400×3 = 297,600개의 숫자로 이루어진 행렬이다. 행렬의 각 수들은 최소 0(검정)에서 최대 255(하양)까지의 값을 가질 수 있다. 우리의 과제는 이 수십만 개의 수들을 하나의 레이블("고양이")로 바꾸는 것이다.

{% include caption-img.html src="https://cs231n.github.io/assets/classify.png" outside_img="true" description="이미지 분류 문제란 주어진 이미지의 레이블이 무엇인지, 혹은 위 예제와 같이 각 레이블들에 대한 확신도(confidence)의 분포(distribution)가 어떤지를 예측하는 것이다. 이미지들은 크기 폭(width) × 높이(height) × 3에 원소로 0 ~ 255 사이 정수들을 가지는 3차원 배열이다. 여기서 3은 R, G, B 새개의 색 채널(channel)을 의미한다." %}

## Challenges

이미지의 시각적 개념(ex. "고양이")을 인식하는 것은 사람에게는 쉬운 일이지만, 컴퓨터 비전 알고리즘의 관점에서는 상당히 어려운 과제이다. 시각적 개념을 인식하는 문제가 어려운 이유는 다음과 같다(컴퓨터에게 이미지는 밝기값의 3차원 배열임을 명심하라).

- **시점의 변동(Viewpoint variation)** : 같은 물체도 카메라의 위치에 따라 다르게 보일 수 있다.
- **크기의 변동(Scale variation)** : 같은 물체도 다른 크기로 보일 수 있다.
- **변형(Deformation)** : 우리가 관심있는 물체들은 딱딱한 물체가 아닌 경우가 많다. 이 경우 같은 물체라도 여러 극단적인 모양으로 변형되어 보일 수 있다.
- **가림(Occlusion)** : 우리가 관심있는 물체가 다른 물체에 가려져 물체의 일부분만 보이는 경우가 있다.
- **조명(Illumination conditions)** : 조명은 픽셀 수준에서 큰 차이를 일으킨다.
- **혼잡한 배경(Background clutter)** : 우리가 관심있는 물체가 배경과 섞여 식별이 어려워지는 경우가 있다.
- **넓은 클래스(Intra-class variation)** : 우리가 관심있는 클래스가 너무 광범위한 경우(ex. "의자") 같은 클래스 안에서도 다양한 모양이 있을 수 있다.

좋은 이미지 분류 모델은 적당히 둔감해 위 어려움은 잘 극복하면서 서로 다른 클래스 간의 차이에는 민감해야 한다.

{% include caption-img.html src="https://cs231n.github.io/assets/challenges.jpeg" outside_img="true" %}

## Data-driven approach

어떻게 하면 이미지들을 서로 다른 카테고리로 분류하는 알고리즘을 작성할 수 있을까? 숫자를 정렬하는 알고리즘과 다르게, 이미지로부터 고양이를 분류하는 알고리즘은 명확하게 작성하기 어렵다. 따라서 명시적으로 코드에 각 카테고리에 해당하는 이미지들이 어떻게 생겼는지 작성하는 대신, 컴퓨터에게 각 클래스에 대한 다양한 예제(examples)를 주고 컴퓨터가 이들로부터 각 클래스의 시각적 생김새를 학습하게 하는 학습 알고리즘을 작성한다. 이런 식의 접근 방법은 학습을 위한 레이블링 된 이미지 데이터 셋을 수집하는 과정이 반드시 필요하기 때문에 데이터 기반 접근법(data-driven approach)이라 불린다. 다음은 데이터 셋의 예시이다.

{% include caption-img.html src="https://cs231n.github.io/assets/trainset.jpg" outside_img="true" description="4개의 카테고리에 대한 학습 데이터 셋. 실제로는 수천 개의 클래스와, 각 클래스별로 수십만 개의 이미지를 사용한다." %}

## The image classification pipeline

이미지 분류 과정은 다음과 같이 정리할 수 있다.

- **입력(Input)** : 입력 데이터는 서로 다른 K개의 클래스 중 하나로 레이블링된 N개의 이미지들의 집합이다. 이 데이터를 학습 셋(Training set)이라 한다.
- **학습(Learning)** : 학습 셋을 이용해 각 클래스가 어떻게 생겼는지를 학습시킨다. 이를 "모델을 학습시킨다(learning a model)" 또는 "분류기를 학습시킨다(training a classifier)"고 한다.
- **평가(Evaluation)** : 최종적으로 한 번도 보여진 적 없던 새로운 이미지들의 레이블들을 예측하게 한 후 참값(Ground truth)과 비교해 분류기의 성능을 평가한다. 예측값과 참값이 많이 일치할수록 좋다.

# Nearest Neighbor Classifier

첫 번째 예제로서 우리는 최근접 이웃 분류기(Nearest Neighbor Classifier)라 불리는 것을 만들어 볼 것이다. 이 분류기는 CNN(Convolutional Neural network)과 아무런 상관이 없고 실전에서도 거의 사용되지 않지만, 이미지 분류 문제에 대한 기본적인 접근 아이디어를 익히는데 도움이 될 것이다.

## Example image classification dataset: CIFAR-10

[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)은 대표적인 토이 이미지 분류 데이터 셋 중 하나이다. 이 데이터 셋은 60,000개의 32×32 크기의 이미지로 구성되어 있고, 각 이미지는 10개의 클래스 중 하나로 분류되어 있다. 60,000개의 전체 이미지는 50,000개의 학습 셋(Training set)과 10,000개의 테스트 셋(Test set)으로 분할되어 있다. 아래 이미지는 각 10개의 클래스에 속하는 10개의 이미지 예시를 보여준다.

{% include caption-img.html src="https://cs231n.github.io/assets/nn.jpg" outside_img="true" description="좌측: CIFAR-10 데이터 셋 이미지 예시. 우측: 첫 번째 열은 테스트 이미지들을 보여주고 있다. 나머지 열은 각 테스트 이미지에 대해 학습 셋의 이미지들과 픽셀간 차이를 계산했을 때, 가장 근처에 있는 10개의 이웃(이미지)들을 보여준다." %}

우리에게 50,000개(각 레이블 당 5,000개)의 CIFAR-10 학습 셋이 주어졌고, 나머지 10,000개의 테스트 이미지들을 레이블링해야 한다고 해 보자. 최근접 이웃 분류기는 테스트 이미지를 받아 모든 학습 이미지들과 한장 한장 비교한 후, 가장 가까운 학습 이미지의 레이블을 해당 테스트 이미지의 레이블로 예측한다. 위 이미지의 우측에서 10개의 테스트 이미지에 대해 이 과정을 적용한 예시를 확인할 수 있다. 10개의 테스트 이미지들 중 오직 3개만이 맞는 레이블로 예측되었고, 나머지 7개는 모두 잘못된 것을 볼 수 있다. 예를 들어 8번째 행을 보게 되면, 말 머리 테스트 이미지와 가장 가까운 학습 이미지는 빨간색 자동차 이미지로 계산되었다(아마 강한 검은 배경 때문일 것이다). 그 결과 이 말 이미지는 자동차로 오분류된다.

사실 위 설명에서 각 이미지들을 어떻게 비교했는지 명확히 설명하지 않았다. 이미지들을 비교하는 가장 간단한 방법은 두 개의 이미지에 대해 픽셀별 차(difference)를 모두 합하는 것이다. 즉 두 벡터(이미지) $I\_1$, $I\_2$가 주어졌을 때, 다음과 같이 **L1 Distance**를 계산한다.

$$ d_1 ( I_1 , I_2 ) = \sum_{p} | I^p _1 - I^p _2 | $$

다음은 이를 시각화한 것이다.

{% include caption-img.html src="https://cs231n.github.io/assets/nneg.jpeg" outside_img="true" description="L1 Distance로 픽셀간 차를 계산해 두 이미지를 비교하는 예제. 이 예제에서 이미지에는 하나의 채널만 있다고 가정한다. 두 이미지는 각 원소별 차가 계산된 후 모두 더해져 하나의 숫자가 된다. 만약 두 이미지가 동일하다면 그 결과는 0이 될 것이다. 만약 이미지의 차이가 심하다면 그 결과는 매우 클 것이다." %}

분류기를 코드로 작성하는 방법을 알아보자. 우선 우리는 CIFAR-10 데이터를 학습 데이터/라벨 배열, 테스트 데이터/라벨 배열, 이렇게 4개의 배열 형태로 메모리에 불러들여야 한다. 아래 코드에서 `Xtr`은 학습 셋의 모든 이미지를 담고 있는 크기 50,000 × 32 × 32 × 3짜리 4자원 배열이고, `Ytr`은 `Xtr`의 각 이미지에 상응하는 레이블(0, 1, ..., 9)을 담고 있는 크기 50,000짜리 1차원 벡터이다.

{% highlight python %}
Xtr, Ytr, Xte, Yte = load_CIFAR10("data/cifar10/")  # 데이터를 로딩할 수 있는 magic function(주어짐)

# 모든 이미지를 1차원으로 평탄화(flatten)
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows은 크기 50000 x 3072짜리 배열이 됨
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows은 크기 10000 x 3072짜리 배열이 됨
{% endhighlight %}

모든 이미지들이 하나의 행으로 만들어졌으므로, 이제 분류기를 학습시키고 평가하자.

{% highlight python %}
nn = NearestNeighbor() # 최근접 이웃 분류기 생성
nn.train(Xtr_rows, Ytr) # 학습 이미지와 레이블을 이용해 분류기 학습
Yte_predict = nn.predict(Xte_rows) # 테스트 이미지의 레이블 예측
print(f"accuracy: { np.mean(Yte_predict == Yte) }") # 분류 정확도 계산 및 출력. 분류 정확도는 전체 테스트 이미지 중 맞게 예측된(분류된) 테스트 이미지들의 비율로 계산한다.
{% endhighlight %}

위 코드에서처럼 평가 기준으로는 주로 **정확도(Accuracy)**를 사용한다. 정확도는 맞게 예측된 이미지들의 비율로 정의된다.

우리가 앞으로 만들 모든 분류기는 학습 데이터와 레이블로부터 학습하는 함수 `train(X, y)`가 공통적으로 존재한다. 또한 테스트 데이터를 입력값으로 받아 레이블을 예측하고 출력하는 `predict(X)` 함수도 공통적으로 존재한다.

다음은 위 템플릿을 만족하는, L1 Distance를 사용하는 간단한 최근접 이웃 분류기의 구현이다.

{% highlight python %}
import numpy as np

class NearestNeighbor:
  def __init__(self):
    pass

  def train(self, X, y):
    """
    X : 크기 N x D의 2차원 배열. 각 행은 하나의 example을 의미한다.
    Y : 크기 N의 1차원 벡터.
    """
    # 최근접 이웃 분류기는 학습 데이터를 모두 기억한다.
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """
    X : 크기 N x D의 2차원 배열. 각 행은 (레이블을 예측하기를 바라는) 하나의 example을 의미한다.
    """
    num_test = X.shape[0]
    # 출력 타입이 입력 타입과 같은지 확인해야 한다.
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # 모든 테스트 데이터에 대해서 반복
    for i in range(num_test):
      # i번째 테스트 이미지에 대해, 가장 가까운 학습 이미지를 찾는다.
      # L1 Distance 사용 (차이의 절대값의 합)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
{% endhighlight %}

CIFAR-10에 대해 위 분류기를 돌려보면 정확도가 **38.6%**밖에 나오지 않는다. 찍는 것(10개 클래스에 대해 찍으므로 정확도 10%)보다야 나은 성적이지만 인간의 정확도(약 94%) 또는 CNN SOTA의 정확도(약 95%)에는 턱없이 모자르다. (CIFAR-10에 대한 [Kaggle 리더보드](https://www.kaggle.com/c/cifar-10/leaderboard)를 참조하자.)

## The choice of distance

벡터 간 거리를 재는 방법에는 여러 가지가 있다. 기하학적으로 두 벡터 간의 유클리드 거리(Euclidean distance)를 측정하는 **L2 Distance** 역시 두 벡터 간의 거리를 재기 위해 흔히 사용된다. L2 Distance는 다음과 같이 계산한다.

$$ d_2 ( I_1 , I_2 ) = \sqrt { \sum_{p} ( I^p _1 - I^p _2 )^2 } $$

L1 Distance와 비교하면 각 픽셀별 차이를 계산하는 것 까지는 동일하나 L2 Distance에서는 이들을 제곱하여 모두 더한 후 최종적으로 루트를 씌운다. L2 Distance를 사용하는 최근접 이웃 분류기를 만드려면 위 코드에서 `distance`를 계산하는 한 줄만 다음과 같이 수정하면 된다.

{% highlight python %}
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
{% endhighlight %}

위 코드는 제곱근 함수(`np.sqrt()`)를 사용하지만, 사실 제곱근 함수는 단조 함수(monotonic function)이기에 생략할 수 있다. 제곱근 함수는 거리의 절대적인 크기는 바꾸지만 거리들의 순서는 보존하기에, 제곱근을 사용하든 사용하지 않든 최근접 이웃은 동일하다. L2 Distance를 사용하는 최근접 이웃 분류기를 CIFAR-10 데이터에 대해 돌려 보면 정확도 **35.4%**가 나온다(L1 Distance를 사용했을때의 결과보다 약간 낮다).

## L1 vs. L2

두 측정 지표의 차이점을 아는 것은 중요하다. L2 Distance는 L1 Distance에 비해 두 벡터 간의 차이를 더 부각한다. L1 Distance와 L2 Distance, 다시말해 이미지 쌍의 차이에 대한 L1, L2 norm은 가장 많이 사용되는 p-norm 중 하나이다.

# k-Nearest Neighbor Classifier

아마 몇몇 사람들은 예측 과정에 있어 가장 근접한 이미지의 레이블만 쓴다는 것에 의문을 품었을 것이다. 사실 대부분의 경우에서 **k-최근접 이웃 분류기(kNN, k-Nearest Neighbor Classifier)**를 사용하면 더 높은 성능을 얻을 수 있다. 이 분류기의 아이디어는 매우 간단하다. 학습 데이터 셋에서 테스트 데이터와 가장 가까운 이미지 1장을 찾는 것이 아니라, 가장 가까운 상위 **k개**의 이미지를 찾은 후 이들의 레이블 중 가장 많은 표를 받은 레이블을 테스트 데이터에 대한 예측값으로 사용하는 것이다. k = 1일 때 kNN은 기존 최근접 이웃 분류기와 동일하다. 직관적으로 알 수 있듯이, k값이 커질수록 분류기는 이상치(outlier)들에 대해 더 둔감해진다.

{% include caption-img.html src="https://cs231n.github.io/assets/knn.jpeg" outside_img="true" description="3개의 클래스(빨강, 파랑, 초록)를 가지는 2차원 데이터에 대한 최근접 이웃 분류기(NN Classifier)와 5-최근접 이웃 분류기(5-NN Classifier) 비교 예시. 색칠된 영역은 L2 Distance를 사용하는 각 분류기들의 결정경계(decision boundary)를 나타낸다. 5-최근접 이웃 분류기의 하얀 영역은 투표 결과가 비겨서 모호하게 분류되는 영역을 나타낸다. 위 결과를 잘 보면, 최근접 분류기에서는 이상치들이 작은 섬을 형성해(ex. 파란 영역 안의 초록 점 등) 아마도 잘못된 예측을 할 가능성이 높지만, 5-최근접 분류기에서는 이런 이상치들이 잘 정리되어 테스트 데이터(그림에 표현되어 있진 않다)에 대해 일반화(Generalization)가 더 잘된 모습을 볼 수 있다." %}

실전에서는 거의 대부분 최근접 이웃 분류기보다 kNN을 사용할 것이다. 그런데 k값으로 얼마를 사용하는 것이 적절할까?

# Validation sets for Hyperparameter tuning

kNN을 사용하기 위해서는 k값을 설정해야 한다. 그런데 k값을 얼마로 설정해야 가장 성능이 높을까? 또한 L1 norm, L2 norm 뿐만 아니라 우리가 아직 배우지 않은 다양한 벡터 간 거리 측정 방법(ex. 내적을 이용한 방법 등) 중 어느 방법이 가장 좋을까? 이런 선택값들은 **하이퍼파라미터(Hyperparameter)**라 불리며, 기계학습 알고리즘을 설계할 때 자주 등장한다. 많은 경우 하이퍼파라미터로 어떤 값을 사용해야 하는지 명쾌하게 정하는 방법은 없다.

하이퍼파라미터를 정하는 한 가지 방법은 모든 가능한 값들을 모두 시도해 보고, 가장 잘 작동하는 것이 뭔지 직접 보는 것이다. 사실 이 방법은 꽤 괜찮은 방법이다. 그런데 이 방법을 사용할 때는 많은 주의가 필요하다. **하이퍼파라미터 조정 과정에서 테스트 데이터를 사용하면 안 된다.** 기계학습 알고리즘을 설계할 때, 테스트 셋은 가장 마지막에 딱 한번 사용해야 하는, 아주 귀중한 자원이라고 생각해야 한다. 만약 이렇게 하지 않고 테스트 셋에 잘 작동하는 하이퍼파라미터 값을 찾아 사용할 경우, 모델을 배포할 때 성능이 아주 심각하게 떨어져 있을 것이다. 이 경우 우리는 흔히 테스트 데이터에 모델이 **과적합되었다(overfit)**고 말한다. 다른 말로 설명하면, 만약 테스트 셋을 이용해 하이퍼파라미터 값을 설정할 경우 테스트 셋을 학습 셋으로 사용하는 셈이 된다. 따라서 (테스트 데이터로 평가하는) 분류기의 성능이 지나치게 낙관적으로 평가되게 된다. 그러나 만약 테스트 셋을 마지막에 딱 한번 사용한다면, 테스트 셋은 만들어진 분류기가 얼마나 잘 **일반화(generalization)**되어 있는지를 간접적으로 측정할 수 있는 훌륭한 도구가 된다. 일반화와 관해서는 나중에 조금 더 자세히 얘기하도록 하겠다.

> 테스트 셋을 이용한 평가는 가장 마지막에 딱 한번만 수행해야 한다.

다행히도 테스트 셋에 손을 대지 않으면서 하이퍼파라미터 값을 조정하는 적절한 방법이 있다. 이 아이디어는 학습 셋을 "약간 작아진 학습 셋"과 "**검증 셋(validation set)**", 이렇게 두 개로 나누는 것이다. 예로 들어 CIFAR-10의 경우 전체 50,000개의 학습 셋 중 49,000개는 학습 과정에 사용하고, 나머지 1,000개는 검증을 위해 남겨두는 것이다. 이 검증 셋은 하이퍼파라미터 값을 조정하기 위한 가짜 테스트 셋으로 사용된다.

CIFAR-10을 사용할 경우 다음과 같이 하면 된다.

{% highlight python %}
# 이전 코드에서의 Xtr_rows, Ytr, Xte_rows, Yte가 주어졌다고 가정
# Xtr_rows은 크기 50,000 x 3072의 2차원 배열
Xval_rows = Xtr_rows[:1000, :] # 처음 1,000개를 검증에 사용
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # 뒤 49,000개를 학습에 사용
Ytr = Ytr[1000:]

# 검증 셋에 대해 가장 최고의 성능을 내는 하이퍼파라미터를 탐색
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:
  # 특정 k 값을 사용하고, 검증 셋에 대해 성능 평가
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # k를 입력값으로 받는 약간 수정된 NearestNeighbor 클래스가 있다고 가정
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print(f"accuracy: {acc}")

  # 각 k와, k를 사용했을 때 (검증 셋에 대한) 정확도를 저장
  validation_accuracies.append((k, acc))
{% endhighlight %}

이후 어떤 k 값이 가장 좋은 성능을 보이는지 시각화한 그래프를 그릴 수 있다. 그리고 가장 좋은 성능을 보인 k값을 사용하여 진짜 테스트 셋에 대해 마지막으로 최종 성능을 평가한다.

> 학습 셋을 학습 셋과 검증 셋으로 분할한다. 검증 셋을 이용해 모든 하이퍼파라미터 값을 설정한다. 가장 마지막에 테스트 셋을 한 번 이용해 성능을 평가하고 이를 보고한다.

## Cross validation

만약 학습 데이터가 너무 부족하다면(= 검증 셋의 크기가 너무 작다면) 하이퍼파라미터 값 조정을 위해 교차 검증법(Cross validation)이라는 더 복잡한 방법을 사용할 수 있다. 이전 예제를 이용해 설명하면, 기존 방법은 처음 1,000개의 데이터를 검증 셋으로, 나머지 데이터를 학습 셋으로 사용하는 것이었다. 그러나 교차 검증법에서는 여러 개의 검증 셋들을 만들고, 이들 전체에 대해 반복적으로 성능 검사를 실시해 이들의 평균값을 이용해 k를 결정한다. 이렇게 하면 특정 k값이 얼마만큼의 성능을 내는지 더 정확하게 추정할 수 있다. 예를 들어, 5겹 교차 검증법(5-fold cross validation)에서는 학습 데이터를 5개의 동일한 크기의 묶음(fold)으로 나눈 후 이 중 4개를 학습에 사용하고 나머지 1개를 검증에 사용한다. 이후 각각의 묶음을 한 번씩 검증 묶음(validation fold)로 사용하여 반복적으로 성능을 평가한 후, 5개 성능 평과 결과를 평균내어 최종 성능 평가 지표로 사용한다.

{% include caption-img.html src="https://cs231n.github.io/assets/cvplot.png" outside_img="true" description="하이퍼파라미터 k를 설정하기 위해 5겹 교차 검증법을 수행한 예시. 각각의 k값에 대해 4개의 묶음(fold)을 학습에 사용하고, 나머지 1개를 검증에 사용한다. 그 결과, 각각의 k에 대해 5개의 (서로 다른 검증 묶음에 대한) 정확도를 얻을 수 있다. 그래프에서 y축은 정확도를 나타내고, 각 검증 결과는 점으로 표현되어 있다. 그래프의 추세선(trend line)은 각 k에서의 5개 결과들의 평균값을 이은 것이고, 오차 막대(error bar)는 5개 결과들의 표준편차를 나타낸 것이다. 이 예제에서는 k = 7일 때 가장 좋은 성능이 나온다. 만약 우리가 5개보다 더 많은 묶음을 사용할 경우, 그래프는 더 부드러운 곡선이 될(= 노이즈가 줄어들) 것이다." %}

## In practice

교차 검증법의 계산 비용은 매우 비싸기에, 많은 경우 사람들은 교차 검증법보다는 단일 검증 셋을 사용하는 방법을 더 선호한다. 일반적으로 학습 데이터의 50%-90%를 학습에 사용하고, 나머지는 검증에 사용한다. 그러나 만약 하이퍼파라미터의 수가 많다면 더 큰 검증 셋을 사용해야 한다. 만약 검증 셋의 데이터 수가 수백개 정도로 너무 적다면 교차 검증법을 사용하는 것이 안전하다. 실전에서는 3겹(3-fold), 5겹(5-fold), 10겹(10-fold) 교차 검증법이 많이 사용된다.

{% include caption-img.html src="https://cs231n.github.io/assets/crossval.jpeg" outside_img="true" description="일반적인 데이터 분할법. 학습 셋과 테스트 셋이 주어졌다고 하자. 이때 학습 셋은 여러 개의 묶음으로(이 예제의 경우 5개로) 나눠진다. 묶음 1-4는 학습에 사용된다. 나머지 한 묶음(이 예제에서는 묶음 5(노란색))은 검증 묶음(Validation fold)라 부르고, 하이퍼파라미터 값 조정에 사용된다. 교차 검증법은 여기에서 한 단계 더 나아가 묶음 1-5를 각각 검증 묶음으로 한 번씩 사용한다. 이를 5겹 교차 검증법(5-fold cross validation)이라 한다. 모델이 모두 학습되고 최적의 하이퍼파라미터가 선택된 후, 모델은 가장 마지막으로 빨간색 테스트 셋을 처음이자 마지막으로 이용해 성능 평가를 수행한다." %}

## Pros and Cons of Nearest Neighbor classifier

최근접 이웃 분류기의 장단점을 생각해 보자. 최근접 이웃 분류기의 장점은 이해하고 구현하는 것이 매우 간단하다는 것이다. 추가로 분류기는 그저 학습 데이터를 저장하기만 하면 되기 때문에(아마도 약간의 인덱싱도 수행할 것이다) 학습 시간이 매우 짧다. 그러나 분류기를 사용할 때 입력된 테스트 이미지를 저장된 모든 학습 데이터와 하나씩 비교해야 하기 때문에 계산 비용이 많이 든다. 일반적으로는 학습 시점에서의 효율성보다 사용 시점에서의 효율성이 더 중요하므로 이는 단점이다. 사실 이 수업에서 나중에 다룰 DNN(Deep Neural Network)는 이 trade-off가 반대쪽으로 치우쳐져 있다: DNN은 학습시키는데 많은 비용이 소모되지만, 한번 학습이 완료되면 모델을 사용하는 비용은 매우 저렴하다. 실전에서는 DNN의 방식이 훨씬 더 유리하다.

한편, 최근접 이웃 분류기의 계산복잡도는 여전히 연구되고 있는 분야이다. 몇몇 **ANN (Approximate Nearest Neighbor)** 알고리즘과 라이브러리들은 데이터 집합에서 최근접 이웃를 탐색하는 시간을 줄여준다(ex. [FLANN](https://github.com/mariusmuja/flann)). 이들 알고리즘은 가장 가까운 이웃을 찾는 과정에서 정확도를 약간 희생하고, 일반적으로 kdtree를 만들거나 또는 k-mean 알고리즘 등을 돌리는 전처리/인덱싱 단계를 가진다.

최근접 이웃 분류기는 몇몇 경우 괜찮은 선택이지만(특히 데이터의 차원이 낮을 때 좋은 선택이다), 이미지 분류 문제에서는 별로 좋은 선택이 아니다. 이미지들은 고차원의 객체이고, 고차원 공간에서의 거리는 매우 반직관적일 수 있기 때문이다. 아래 이미지는 우리가 앞에서 학습한 픽셀 기반 L2 Distance이 관념적 유사성(perceptual similarity)와 크게 다를 수 있음을 보여준다.

{% include caption-img.html src="https://cs231n.github.io/assets/samenorm.png" outside_img="true" description="고차원 데이터, 특히 이미지에서의 픽셀 기반 거리는 매우 반직관적이다. 원본 이미지(왼쪽)와 그 옆 나머지 3개의 이미지들은 픽셀 기반 L2 Distance로 계산했을 때 모두 같은 거리만끔 떨어져 있다. 픽셀 기반 거리가 관념적, 의미론적 유사성을 잘 반영하지 못한다는 점은 분명하다." %}

아래는 이미지를 비교하는데 픽셀의 차이를 사용하는 것이 부적절하다는 것을 보여주는 또 다른 시각화 예시이다. t-SNE라 하는 시각화 기술을 사용하면 CIFAR-10 이미지들을 서로간의 (지역적) 거리를 최대한 보존하면서 2차원에 표현할 수 있다. 이 시각화 예제에서 근처에 위치한 이미지들은 픽셀 기반 L2 Distance 관점에서 아주 가까이에 있다는 뜻이다.

{% include caption-img.html src="https://cs231n.github.io/assets/pixels_embed_cifar10.jpg" outside_img="true" description="t-SNE를 이용해 CIFAR-10 이미지를 2차원으로 표현한 결과. 근처에 표현된 이미지들은 픽셀 기반 L2 Distance 관점에서 가까이 있다는 뜻이다. 의미론적 클래스 차이보다 배경에 영향을 더 많이 받은 모습을 확인할 수 있다. <a href=\"https://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg\">이 링크</a>를 클릭하여 더 큰 버전으로 볼 수 있다." %}

위 시각화 예제에서 근처에 있는 이미지들은 이미지의 의미론적 특성에 의해서라기보다는 전반적인 색 분포 혹은 배경의 종류 때문이라는 것을 볼 수 있다. 예를 들어 강아지 사진은 같은 흰색 배경을 사용하는 개구리 사진과 아주 유사하다고 판별된다. 우리는 각 10개의 클래스에 해당하는 사진들이 중요하지 않은 특징(ex. 배경) 또는 약간의 변화(variation)에 영향을 받지 않고 각 클래스들끼리 무리를 형성하기를 바랬다. 그러나 이를 이루려면 단순한 픽셀을 넘어서 더 깊게 들어갸야 한다.

# Summary

- **이미지 분류 문제(Image Classification)**를 배웠다. 하나의 단일 카테고리로 레이블링된 이미지들의 집합이 주어졌을 때, 새로운 테스트 이미지들의 카테고리를 예측하고 그 예측의 정확도를 측정해야 한다.
- 간단한 분류기 중 하나인 **최근접 이웃 분류기(Nearest Neighbor classifier)**를 배웠다. 우리는 k, 이미지 비교를 위한 이미지 간 거리 측정 방법 등 분류기와 관련된 다양한 하이퍼파라미터가 존재하고, 이들을 정하는 명시적인 방법은 없다는 것을 보았다.
- 우리는 하이퍼파라미터 값을 설정하는 올바른 방법은 학습 셋을 학습 셋와 **검증 셋(validation set)**이라 부르는 가짜 테스트 셋, 이렇게 두 개로 분할하는 것임을 배웠다. 다양한 하이퍼파라미터 값들을 시험해, 검증 셋에서 가장 좋은 성능을 보이는 값을 사용하면 된다.
- 만약 부족한 학습 데이터의 양이 고민이라면 **교차 검증법(cross validation)**을 사용할 수 있다는 것을 배웠다. 교차 검증법은 어떤 하이퍼파라미터가 가장 잘 작동하는지 추정할 때의 노이즈를 줄이는데 도움이 된다.
- 최고의 하이퍼파라미터를 찾았으면, 이 값을 고정하고 진짜 테스트 셋에 대해 한 번의 **평가(evaluation)**를 수행한다.
- 최근접 이웃 분류기는 CIFAR-10에 대해 약 40%의 정확도를 보인다는 것을 확인했다. 이 방법은 구현은 간단하지만, 전체 학습 데이터를 저장해야 하고, 테스트 이미지에 대해 검증할 때의 비용이 많이 든다.
- L1 Distance, L2 Distance는 이미지의 의미(semantic content)보다는 배경 색이나 색깔 분포 등에 더 많이 영향을 받기에, 단순 픽셀 값들에 대해 L1 Distance, L2 Distance를 사용하는 것은 별로 좋은 방법이 아니라는 것을 보았다.

다음 강좌에서는 이 문제를 해결하는 방법과 최종적으로 90%의 정확도를 내는, 학습이 완료되면 더 이상 학습 데이터를 다 폐기해도 되고, 테스트 이미지의 레이블을 1밀리세컨드도 안 되는 시간에 추정할 수 있는 해법에 대해 알아볼 것이다.

# Summary : Applying kNN in practice

만약 kNN을 실전에서 사용하고 싶다면(이미지 분류 문제가 아니거나, 베이스라인으로서만 사용하길 바란다.) 다음과 같이 하면 된다.

1. 데이터 전처리 : 데이터 feature를 평균 0, 표준편차 1이 되게 정규화(normalization)시킨다. 정규화에 관해서는 나중에 더 자세히 배울 것이다. 이미지 안 픽셀들은 일반적으로 homogeneous하고, 그렇게 넓은 분포(distribution)를 보이지 않아 데이터 정규화의 필요성이 그렇게 크지 않기 때문에 이 문서에서는 데이터 정규화를 하지 않았다. 
2. 만약 데이터가 너무 고차원이라면 PCA([wiki ref](https://en.wikipedia.org/wiki/Principal_component_analysis), [CS229ref](http://cs229.stanford.edu/notes/cs229-notes10.pdf), [blog ref](https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/)), NCA ([wiki ref](https://en.wikipedia.org/wiki/Neighbourhood_components_analysis), [blog ref](https://kevinzakka.github.io/2020/02/10/nca/)), 혹은 하다못해 [Random Projection](https://scikit-learn.org/stable/modules/random_projection.html) 등의 차원 감소 기법(Dimensionality reduction technique)을 사용하는 것을 고려해야 한다.
3. 학습 데이터를 학습/검증 데이터로 랜덤 분할한다. 일반적으로 학습 데이터의 70-90%를 학습 쪽 분할에 사용한다. 이 설정값은 얼마나 많은 하이퍼파라미터를 사용하는지, 하이퍼파라미터 각각이 얼마나 큰 영향을 끼칠 것으로 예상되는지 등에 따라 달라질 수 있다. 만약 하이퍼파라미터의 수가 많다면, 큰 검증 셋을 사용하여 그들을 효과적으로 추정할 수 있도록 해야 한다. 만약 검증 데이터의 수가 작다면, 학습 데이터를 여러 개의 묶음(fold)으로 나누고 교차 검증법을 시행하는 것이 좋다. 높은 계산 비용을 감당할 수 있다면 교차 검증법을 사용하는 것이 언제나 좋다(묶음 수가 더 많으면 많을수록 더 좋지만 계산 비용이 더 비싸진다).
4. 다양한 k 값과 거리 측정 방법(L1, L2 등)에 대해 kNN 분류기를 학습시키고 검증 데이터에 대해(만약 교차 검증법을 사용한다면, 모든 검증 묶음에 대해) 평가한다.
5. 만약 만들어진 KNN 분류기의 예측에 너무 오랜 시간이 소요된다면, ANN(Approximate Nearest Neighbor) 라이브러리(ex. [FLANN](https://github.com/mariusmuja/flann))를 사용하여 정확도를 약간 희생하는 대신 더 빠르게 예측하도록 하는 방법을 고려해 볼 수 있다.
6. 가장 좋은 성능을 내는 하이퍼파라미터를 탐색한다. 이때 탐색한 최고의 하이퍼파라미터로 설정된 모델을 학습 데이터와 검증 데이터를 합친 전체 학습 데이터로 다시 학습시켜 사용해야 하는지에 대한 의문이 있다(학습 데이터와 검증 데이터를 다시 합칠 경우 데이터의 수가 늘어나므로 최적의 하이퍼파라미터가 바뀔 수 있다). 일반적으로는 검증 데이터는 하이퍼파라미터 추정 과정에서 없어졌다고 보고 최종 분류기에서는 사용하지 않는 게 더 깔끔하다. 최고 성능을 내는 모델의 성능을 테스트 셋에 대해 평가한다. 테스트 셋에서의 정확도를 (주어진 데이터 셋에 대한) kNN 분류기의 성능이라 보고한다.

# Further Reading

- [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf), section 6. 전체 글을 다 읽는 것도 좋다.
- [Recognizing and Learning Object Categories](https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html). ICCV 2005에서의 Object Categorization에 관한 짧은 글