---
title: "지도학습 (Supervised Learning)"
order: 3
date_created: "2020-05-01"
date_modified: "2021-03-08"
---

# 지도학습의 과정

일반적으로 지도학습은 다음과 같이 진행된다.

1. 데이터 수집 및 정제 : 학습을 위해 라벨링된 데이터를 (최대한 많이) 수집한다. 이후 수집한 데이터를 필요에 따라 가공[^1]한다.
2. 모델 선정 : 해결하고자 하는 과제 및 데이터의 종류, 성격 등을 토대로 사용하고자 하는 기계학습 모델을 선정한다. 모델을 입력값을 받아 출력값을 내놓는 함수로 이해할 수도 있는데, 그래서 모델을 가설 함수(Hypothesis Function)이라고도 부른다.[^2] 지도학습 모델에는 로지스틱 회귀(Logistic Regression), SVM(Support Vector Machine), 의사결정나무(Decision Tree), 랜덤 포레스트(Random Forest), NN(Nearest Neighbor), 인공신경망(Neural Network), 다중 레이어 퍼셉트론(Multilayer Perceptron, MLP) 등이 있다.
3. 파라미터 최적화(Parameter Optimization) : 데이터에 맞게 모델의 파라미터를 학습시킨다.

[^1]: 통계적 처리, 노이즈 제거, 특징점(feature) 추출 등
[^2]: 과학적 방법론(데이터 수집 - 가설 수립 - 가설 검증 실험 설계 - 가설 검증 - 가설 폐기 혹은 새로운 검증 실험 설계)에 익숙한 사람이라면 왜 가설 함수라 불리는지 이해할 수 있을 것이다. 

# 파라미터 최적화(Parameter Optimization)

위 과정 중 파라미터 최적화에 대해서 조금 더 알아보자.

지도학습을 위해 데이터를 수집해 입력값 $\boldsymbol{x}$와 정답(label) $y$의 쌍으로 이루어진 데이터 셋을 구성하고, 파라미터 $\boldsymbol{w}$를 가지는 모델 $f$를 선정해 학습시키기로 하였다.

모델이 잘 학습되었다면 $f(x)$와 $y$의 값이 비슷할 것이다. 이 말을 조금 다르게 표현하면, 잘 학습된 모델에서는 $f(\boldsymbol{x})$와 $y$ 간의 오차가 작다. 즉 **모델을 학습시킨다는 것은 $f(\boldsymbol{x})$와 $y$ 간의 오차 $e$를 작게 만드는 과정**이다.

이제 우리는 "모델을 학습시킨다"는 추상적인 목표 대신, "오차를 최소화한다"라는 명확하고 구체적인 목표를 가지게 되었다. 그리고 오차를 최소화하는 문제는 전형적인 최적화 문제(Optimization Problem) 중 하나이다. 이 때문에 모델 학습 과정을 파라미터 최적화라 하는 것이다.

## 오차 함수 (Error Function)

그렇다면 그 오차는 어떻게 구할까? 지도학습에서 오차는 오차 함수(Error Function)라는 함수를 이용하여 구한다. 오차 함수는 비용 함수(Cost Function), 손실 함수(Loss Function), 목표 함수(Objective Function)라고도 불린다. (모두 다 같은 의미이다.) 오차 함수는 $\boldsymbol{w}$에 대한 함수이다.

오차 함수에는 다양한 종류가 있는데, 회귀 문제에서는 [평균제곱오차(MSE, Mean of Squared Error)](/swe3050/04-error-functions#kramdown_평균제곱오차-mse-mean-of-squared-error)가, 분류 문제에서는 [크로스 엔트로피(Cross Entropy)](/swe3050/04-error-functions#kramdown_크로스-엔트로피-cross-entropy)가 많이 사용된다. 해결하고자 하는 과제 및 데이터의 종류, 성격 등을 고려하여 적당한 오차 함수를 선택하면 된다.

각 오차 함수에 대한 자세한 내용은 [해당 문서](/swe3050/04-error-functions)를 참조하기 바란다.

## 최적화 방법

오차 함수를 최소값으로 만드는 $\boldsymbol{w}$를 찾는 문제를 푸는 방법은 크게 두 가지가 있다.

### 해석적 풀이법 (Analytic Solution)

해석적 풀이법은 오차 함수가 언제 최소값을 갖는지 해석적인 방법으로 푸는 것이다.

해석적으로 어떤 함수가 언제 최소값을 가지는지 어떻게 알 수 있을까? 예를 들어, 다음 이차함수를 생각해 보자.



$$f(x) = x^2 + 2x + 3 $$



$f$는 볼록함수(convex function)이므로, 최소값은 이 함수의 도함수(derivative)가 0이 되는 점에서 발생한다. 즉,



$$\frac{df}{dx} = 2x + 2 = 0$$



을 만족시키는 $x = -1$에서 최소값을 가짐을 알 수 있다.

어떤 임의의 볼록함수(convex function)가 주어졌을 때, 이 함수의 최소값(global minimum)은 함수의 [그라디언트(gradient)]({{ site.url }}/{{ site.baseurl }}calculus/02-gradient)의 모든 원소를 0으로 만드는 점이다. 예를 들어, 함수



$$f(x, y) = x^2 +xy - 2x - y^2$$



에 대해, 이 함수의 최솟값은 함수의 그라디언트



$$\nabla f(x, y) = [2x + y - 2, x - 2y] $$



의 각 원소가 0이 되는 점이다. 이로부터 연립방정식을 세우면 다음과 같다.



$$ \begin{cases}
2x + y - 2 = 0\\
x - 2y = 0\\
\end{cases} $$



연립방정식을 풀면



$$ x = \frac {4}{5},\quad y = \frac{2}{5} $$



이므로, 우리는 함수 $f(x, y) = x^2 +xy - 2x - y^2$가 $(\displaystyle\frac{4}{5}, \frac{2}{5})$에서 최소값을 가짐을 알 수 있다.

해석적 풀이법의 해는 닫힌 형식(Closed Form)으로 나오므로, 매우 적은 연산 비용만으로 최적의 파라미터를 구할 수 있다는 장점이 있다. 하지만 다음의 상황에서는 적용하기 어렵다는 단점이 있다.

- 변수의 수가 너무 많거나, 데이터의 수가 너무 많은 경우 : 일반적으로 해석적 풀이법은 행렬 연산을 통해 해를 찾게 되는데, 만약 전체 데이터를 행렬의 형태로 메모리에 올리는 게 불가능하다면 해석적 풀이법을 사용하기 어렵다.
- 수학적으로 최소점이 존재하지 않거나, 계산 불가능한 경우 : 함수가 볼록 함수가 아니거나, 미분 불가능한 등의 이유로 수학적으로 최소점을 계산할수 없는 경우 해석적 풀이법을 사용할 수 없다.


## 수치 계산법 (Numerical Solution)

해석적 풀이법을 적용하기 어려울 때[^4] 수치 계산법은 좋은 대안이 된다. 한 번에 해를 구하는 해석적 풀이법과는 다르게 수치 계산법에서는 여러 번 연산을 반복하면서 점점 더 품질이 좋은 근사해를 구한다.

[^4]: 사실 대부분의 지도학습 문제는 해석적 풀이가 존재하지 않는 문제이다.

수치 계산법에는 다양한 방법이 있다.

- 경사 하강법(Gradient Descent Method)
- 뉴턴법(Newton Method)
- 가우스-뉴턴법(Gauss-Newton Method)
- Levenberg-Marquardt Method
- BFGS
- Conjugate Gradient Method
- 등등...

이 중 경사 하강법은 방법 특성상 항상 최소값에 도달한다고 확신할 수는 없으나, 속도가 빠르고 연산량이 적어 현재 널리 쓰이고 있다.

### 경사 하강법 (Gradient Descent Method)

경사 하강법에서는 그라디언트의 반대 방향으로 가면 함수의 극소점을 찾을 수 있는 그라디언트의 성질을 사용한다.

해석적 풀이법과 동일하게 그라디언트를 사용하기에 헷갈릴 수 있는데, 해석적 풀이법은 그라디언트의 각 원소를 0으로 만드는 값(= 해)을 연립방정식을 풀어 찾는 방법이었다면, 경사 하강법은 그라디언트와 극소점 간의 성질을 이용하여 점진적으로 근사해를 찾아가는 방법이다.

일반적으로 경사 하강법은 다음과 같은 순서로 진행된다.

1. 파라미터 $\boldsymbol{w}$를 무작위 값으로 초기화한다.
2. 다음 식으로 파라미터를 업데이트한다 : $\boldsymbol{w}\_{new} = \boldsymbol{w}\_{old} - \eta \nabla \boldsymbol{w}\_{old}$
3. 2를 필요한 만큼 반복한다.

오차 함수 $\boldsymbol{w}\_{old}$의 값을 최소화할 수 있도록 오차 함수의 그라디언트 $\nabla \boldsymbol{w}\_{old}$의 반대 방향(-)으로 $\boldsymbol{w}$를 점진적으로 움직인다.

이때 $\eta$는 그리스 문자 에타(eta)로서 학습률(learning rate)을 나타낸다. 학습률은 한 번에 파라미터를 얼마나 업데이트할 지를 나타내는 하이퍼파라미터이다.

학습률이 크면...

- 한 번의 업데이트로 파라미터가 크게 바뀐다.
- 최소값으로 다가가는 속도가 빠르다. 최소값에 더 적은 시간으로 도달할 수 있다.
- 최소값이 아닌 극소값(local minimum)으로 빠지더라도 탈출할 가능성이 생긴다.
- 최소값을 지나칠 수도 있다.

학습률이 작으면...
- 한 번의 업데이트로 파라미터가 작게 바뀐다.
- 최소값으로 다가가는 속도가 느리다. 최소값 도달에 더 많은 시간이 걸린다.
- 최소값이 아닌 극소값에 빠질 경우 탈출이 어렵다.