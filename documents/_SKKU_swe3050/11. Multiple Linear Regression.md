---
title: "다중 선형 회귀 (Multiple Linear Regression)"
order: 11
date_created: "2020-05-21"
date_modified: "2021-12-18"
---

다중 선형 회귀는 [단순 선형 회귀](/swe3050/10-simple-linear-regression)와 크게 다르지 않다. 단순 선형 회귀에서의 과정을 잘 이해했다면 다중 선형 회귀로의 확장은 그리 어렵지 않다.

# 문제 상황

다음 데이터는 다음 데이터는 [보스턴 주택 가격 데이터 셋(Boston House-price Dataset)](http://lib.stat.cmu.edu/datasets/boston)으로 불리는 유명한 데이터 셋의 일부이다.

<div class="table-wrapper" markdown="block">

|  CRIM   |   ZN    |  INDUS  |  CHAS   |   NOX   |   RM    |   AGE   |   DIS   |   RAD   |   TAX   | PTRATIO |    B    |  LSTAT  |  MEDV   |
| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |
| 0.00632 |   18    |  2.31   |    0    |  0.538  |  6.575  |  65.2   |  4.09   |    1    |   296   |  15.3   |  396.9  |  4.98   |   24    |
| 0.02731 |    0    |  7.07   |    0    |  0.469  |  6.421  |  78.9   | 4.9671  |    2    |   242   |  17.8   |  396.9  |  9.14   |  21.6   |
| 0.02729 |    0    |  7.07   |    0    |  0.469  |  7.185  |  61.1   | 4.9671  |    2    |   242   |  17.8   | 392.83  |  4.03   |  34.7   |
| 0.03237 |    0    |  2.18   |    0    |  0.458  |  6.998  |  45.8   | 6.0622  |    3    |   222   |  18.7   | 394.63  |  2.94   |  33.4   |
| 0.06905 |    0    |  2.18   |    0    |  0.458  |  7.147  |  54.2   | 6.0622  |    3    |   222   |  18.7   |  396.9  |  5.33   |  36.2   |
| 0.02985 |    0    |  2.18   |    0    |  0.458  |  6.43   |  58.7   | 6.0622  |    3    |   222   |  18.7   | 394.12  |  5.21   |  28.7   |
| 0.08829 |  12.5   |  7.87   |    0    |  0.524  |  6.012  |  66.6   | 5.5605  |    5    |   311   |  15.2   |  395.6  |  12.43  |  22.9   |
| 0.14455 |  12.5   |  7.87   |    0    |  0.524  |  6.172  |  96.1   | 5.9505  |    5    |   311   |  15.2   |  396.9  |  19.15  |  27.1   |
| 0.21124 |  12.5   |  7.87   |    0    |  0.524  |  5.631  |   100   | 6.0821  |    5    |   311   |  15.2   | 386.63  |  29.93  |  16.5   |
| 0.17004 |  12.5   |  7.87   |    0    |  0.524  |  6.004  |  85.9   | 6.5921  |    5    |   311   |  15.2   | 386.71  |  17.1   |  18.9   |
| &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; |

</div>

이 데이터는 1978년 Harrison D.와 Rubinfeld, D.L.이 작성한 논문 \<Hedonic prices and the demand for clean air\>에 등장하는 데이터 셋으로, 1978년 보스턴 시의 506개 마을(town)의 여러 요건(feature)들과 주택 가격 중앙값(MEDV) 정보를 가지고 있다. 다음은 데이터 셋에 대한 자세한 정보이다.

- 레코드 수 : 506개
- 필드 개수 : 14개
  - CRIM : 마을별 1인당 범죄율
  - ZN : 25,000 평방피트를 초과하는 거주지역의 비율
  - INDUS : 비소매상업지역이 점유하고 있는 토지의 비율
  - CHAS : 찰스 강의 경계에 위치한 경우 1, 아니면 0
  - NOX : 10ppm 당 농축 일산화질소
  - RM : 주택 1가구 당 평균 방의 개수
  - AGE : 1940년 이전에 건축된 소유주택의 비율
  - DIS : 5개의 보스턴 직업센터까지의 접근성 지수
  - RAD : 방사형 도로까지의 접근성 지수
  - TAX : $10,000 당 재산세율
  - PTRATIO : 마을별 학생/교사 비율 
  - B : 1000(Bk - 0.63)<sup>2</sup>, Bk는 마을별 흑인 비율
  - LSTAT : 모집단의 하위계층의 비율(%)
  - MEDV : 본인 소유의 주택가격(중앙값) (단위: $1,000)
  - (CAT.MEDV) : MEDV가 $30,000을 넘으면 1, 아니면 0. 데이터 셋에 따라 이 필드가 있는 경우도 있다(원본 데이터에는 없다).

이 데이터 셋은 너무 유명하기에 다양한 기계학습 라이브러리에서 기본적으로 제공하고 있다. 이 글에서는 가장 유명한 파이썬 기계학습 라이브러리 중 하나인 [사이킷런(Scikit-Learn)](https://scikit-learn.org/stable/)에서 제공하는 데이터 셋을 사용하도록 하겠다.

사이킷런은 다음 pip 명령어로 쉽게 설치할 수 있다.

{% highlight python %}
pip install scikit-learn
{% endhighlight %}

보스턴 주택 가격 데이터 셋은 다음 명령어로 받을 수 있다.

{% highlight python %}
from sklearn import datasets

dataset = datasets.load_boston()
{% endhighlight %}

이렇게 받은 보스턴 주택 가격 데이터 셋은 다음과 같은 구조로 되어 있다.

- `dataset` [sklearn.utils.Bunch]
  - `data` [numpy.ndarray] : (506, 13) 506개의 각 행은 한 레코드를 나타냄. 13개의 각 열은 순서대로 각각 CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT를 의미함.
  - `target` [numpy.ndarray] : (506, ) 506개의 각 데이터는 `data`의 각 행에 상응하는 MEDV를 의미함.
  - `feature_names` [numpy.ndarray] : (13, ) 13개의 각 데이터는 `data`의 각 열의 이름을 의미함. 즉, array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']).
  - `filename` [str] : 데이터 셋 파일(.csv)이 저장되어 있는 경로를 나타냄.
  - `DESCR` [str] : 데이터 셋에 대한 설명서

이 데이터를 바탕으로 마을의 요인들과 주택 가격 사이의 관계를 다중 선형 회귀 모델을 이용하여 나타내보자.

# 모델 설계 : 다중 선형 회귀 모델 (Multiple Linear Regression Model)

$d$개의 (독립) 변수[^1]로 이루어진 샘플 $\mathbf{x}[x_1, x_2, \cdots, x_d]$와 레이블 $y$에 대해, 다중 선형 회귀 모델 $f$는 다음과 같이 샘플과 레이블 간 관계를 추정한다.

[^1]: 특징(feature), 속성(attribute), 열(column) 등으로도 쓴다.

$$y = f(\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_d x_d$$

$\mathbf{x}[x_1, x_2, \cdots, x_d]$에 항상 1인 더미 변수(dummy variable) $x_0$를 추가하면($\mathbf{x}[x_0, x_1, x_2, \cdots, x_d]$), 파라미터 $\mathbf{w}[w_0, w_1, w_2, \cdots, w_d]$에 대해 모델 $f$를 다음과 같이 행렬 곱의 형태로 나타낼 수도 있다.

$$y = f(\mathbf{x}) = \mathbf{w}^T \cdot \mathbf{x} = w_0 x_0 + w_1 x_1 + w_2 x_2 + \cdots + w_d x_d$$

# 파라미터 최적화

다중 선형 회귀 모델은 본질적으로 단순 선형 회귀 모델의 열의 개수가 늘어난 것이다. 따라서 다중 선형 회귀 모델의 파라미터 최적화 과정은 단순 선형 회귀 모델의 그것과 같다.[^2]

[^2]: 입력 데이터의 크기가 커진 것(($n$, 2) 행렬 → ($n$, $d$) 행렬) 빼고는 식의 형태가 같다.

## 오차 함수 : 평균 제곱 오차 (MSE, Mean of Squared Error)

단순 선형 회귀 모델에서처럼, 다중 선형 회귀 모델의 파라미터를 최적화하기 위해서는 [평균 제곱 오차(MSE, Mean of Squared Error)](/SKKU_swe3050/04-error-functions#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용한다.

총 $n$쌍의 샘플-레이블 쌍 ($\mathbf{x}_i$, $y_i$) ($i=1, 2, \cdots, n)$가 있는 데이터 셋과 모델 $f$가 주어졌을 때, 모델에 대한 평균 제곱 오차 $J(\mathbf{w})$는 다음과 같이 정의된다.

$$J(\mathbf{w}) = \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(\mathbf{x}_i))^2$$

## 해석적 풀이법 (Analytic Solution)

$J(\mathbf{w})$를 $j$번째 파라미터 $w_j$에 대해 편미분하면 다음과 같다. ($j = 0, 1, \cdots, d$)[^3]

[^3]: 단순 선형 회귀 모델에서는 $j = 0, 1$이었다.

$$
\begin{align}
\frac {\partial}{\partial w_j} J(\mathbf{w}) 
&= \frac {\partial}{\partial w_j} \left( \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(\mathbf{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( \frac {\partial}{\partial w_j} (y_i - f(\mathbf{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\mathbf{x}_i)) \cdot -\frac {\partial}{\partial w_j} f(\mathbf{x}_i) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\mathbf{x}_i)) \cdot -\frac {\partial}{\partial w_j} (w_0x_{i0} + w_1x_{i1} + \cdots + w_d x_{id}) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\mathbf{x}_i)) \cdot -x_{ij} )\\
\end{align}
$$

이제 모든 $j = 0, 1, \cdots, d$에 대해 $\partial J / \partial w_j$를 동시에 0으로 만드는(최소화하는) $\mathbf{w}[w_0, w_1, \cdots, w_d]$를 계산해보자.

$$X = \left[ \begin{array}\\
x_{10} & x_{11} & \cdots & x_{1d}\\
x_{20} & x_{21} & \cdots & x_{2d}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n0} & x_{n1} & \cdots & x_{nd}\\
\end{array} \right],\quad Y = \left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]$$

라 놓고, 단순 선형 회귀 모델에서와 같은 방법으로 식을 변형, 계산하자.

$$\frac {\partial}{\partial w_j} J(\mathbf{w}) = \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\mathbf{x}_i)) \cdot -x_{ij} ) = 0$$

$$\sum _{i=1} ^{n} f(\mathbf{x}_i) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

$$\sum _{i=1} ^{n} (w_0 x_{i0} + w_1 x_{i1} + \cdots + w_d x_{id}) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

$$w_0 \sum _{i=1} ^{n} x_{i0} x_{ij} + w_1 \sum _{i=1} ^{n} x_{i1} x_{ij} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

$$\begin{cases}
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i0} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{i0} = \sum _{i=1} ^{n} y_i \cdot x_{i0}\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i1} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{i1} = \sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad \vdots\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{id} + w_1 \sum _{i=1} ^{n} x_{i1} x_{id} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{id} = \sum _{i=1} ^{n} y_i \cdot x_{id}\\
\end{cases}$$

$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{i0} \\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{i1} \\
\vdots & \vdots & \ddots & \vdots\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{id} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{id} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{id} \\
\end{array} \right]

\cdot

\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]

=

\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\vdots\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{id}\\
\end{array} \right]
$$

$$(X^T \cdot X) \cdot
\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]
=(X^T \cdot Y)$$

$$
∴\,\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]
= (X^T \cdot X)^{-1} \cdot (X^T \cdot Y)
$$

단순 선형 회귀 모델의 결과값과 동일하다.

## 수치 계산법

해석적 풀이법이 그랬듯이, 다중 선형 회귀 모델의 수치 계산법도 단순 선형 회귀 모델의 것과 같다.

오차 함수 $J(\mathbf{w})$의 그라디언트를 계산하면

$$
\begin{align}
\nabla J(\mathbf{w})

&= \left[ \begin{array}\\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\mathbf{x}_i)) \cdot -x_{i0} ) \\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\mathbf{x}_i)) \cdot -x_{i1} ) \\
\vdots\\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\mathbf{x}_i)) \cdot -x_{id} )
\end{array} \right]\\[5pt]

&= -\frac {2}{n}
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1d} & x_{2d} & \cdots & x_{nd} \\
\end{array} \right]

\cdot

\left[ \begin{array}\\
y_1 - f(\mathbf{x}_1)\\
y_2 - f(\mathbf{x}_2)\\
\vdots\\
y_n - f(\mathbf{x}_n)\\
\end{array} \right]\\[5pt]

&= -\frac {2}{n} \, X^T \cdot (Y - f(X))

\end{align}
$$

따라서 다음 업데이트 식을 얻을 수 있다.

$$\mathbf{w}_{new} = \mathbf{w}_{old} - \eta \, \left[ -\frac {2}{n} \, X^T \cdot (Y - f(X)) \right]$$

# 문제 풀이

## 해석적 풀이법

{% highlight python linenos %}

from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

def loss(x, y, w):
    predict_y = x @ w
    return np.mean(np.square((y - predict_y)))

def status(w):
    w_ = np.squeeze(w)
    print("w = [", end="")
    for i in range(14):
        if(i == 13):
            print(w_[i], end="]")
        else:
            print(w_[i], end=", ") 
    print()
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    print("Training Loss = " + str(training_loss))
    print("Validation Loss = " + str(validation_loss))
    
    training_predict_y = training_x @ w
    validation_predict_y = validation_x @ w
    
    print("{:^40s}|{:^40s}".format("training", "validation"))
    print("{:^20s}{:^20s}|{:^20s}{:^20s}".format("predict_y", "true_y", "predict_y", "true_y"))
    for i in range(10):
        print("{:^20f}{:^20f}|{:^20f}{:^20f}".format(float(training_predict_y[i]), float(training_y[i]), float(validation_predict_y[i]), float(validation_y[i])))
    print()
    
np.random.seed(2020) # for consistency
    
# data
dataset = datasets.load_boston()
x = dataset.data
y = dataset.target

# reformat the data
x_ = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1) # x0 = 1인 열 추가
y_ = np.expand_dims(y, axis=1)

# divide data into training set and validation set
training_x = x_[0:406, :]
training_y = y_[0:406, :]

validation_x = x_[406:506, :]
validation_y = y_[406:506, :]

# random w
w = np.random.rand(x_.shape[1], 1) * 100
print("Before Training...")
status(w)

# calculate w
w = np.linalg.inv(training_x.T @ training_x) @ (training_x.T @ training_y)

# validating
print("After Training...")
status(w)

{% endhighlight %}

### 실행 결과

<div class="result-folder" markdown="block">

{: .code-result }
{% highlight text %}
Before Training...
w = [98.62768288615989, 87.33919458206546, 50.97455249715816, 27.183571428207575, 33.691872774596355, 21.695426501692427, 27.647714315379957, 34.33155927794576, 86.2158935459853, 15.669967000948859, 14.088724485241533, 75.70802806406883, 73.63249181536082, 35.56630916357817]
Training Loss = 1524850868.018775
Validation Loss = 1358305164.9512415
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
    38610.133657         24.000000      |    45442.227606         11.900000      
    37956.131113         21.600000      |    41520.261455         27.900000      
    36884.722383         34.700000      |    40287.177078         17.200000      
    36211.775709         33.400000      |    30812.276656         27.500000      
    36759.633153         36.200000      |    20623.400705         15.000000      
    36681.911778         28.700000      |    20203.688657         17.200000      
    39082.735421         22.900000      |    20572.385879         17.900000      
    40473.205793         27.100000      |    34355.978592         16.300000      
    40236.511245         16.500000      |    27405.425977          7.000000      
    39352.695440         18.900000      |    20282.256106          7.200000      

After Training...
w = [30.204298144175937, -0.19127194513780132, 0.04405462728752524, 0.05205068411311231, 1.8916839644777284, -14.940080650014465, 4.757264001200724, 0.002702708744277249, -1.300221193932181, 0.45890271416633155, -0.015584040745901417, -0.8110949054519061, -0.002163551374254169, -0.5323204869379943]
Training Loss = 22.682227893845553
Validation Loss = 33.548282912266075
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     29.142474           24.000000      |      6.787740           11.900000      
     24.846338           21.600000      |     21.367991           27.900000      
     31.161746           34.700000      |     15.422323           17.200000      
     29.332689           33.400000      |     23.760338           27.500000      
     28.780051           36.200000      |     17.130162           15.000000      
     25.458646           28.700000      |     22.371876           17.200000      
     22.516512           22.900000      |      4.788079           17.900000      
     19.259550           27.100000      |     12.578669           16.300000      
     10.796351           16.500000      |     -3.970297            7.000000      
     18.706968           18.900000      |     14.763726            7.200000      

{% endhighlight %}

</div>

## 수치 계산법

[홀드아웃 방법(Hold-out Method)](/SKKU_swe3050/12-model-evaluation#kramdown_홀드아웃-방법-hold-out-method)을 적용하여 주어진 전체 506개 데이터 중 406개 데이터를 학습용 데이터(training data)로 사용하고, 100개 데이터를 검증 데이터(validation data)로 사용하기로 하였다.

또한 직접 추가한 $x_0$(= 1)열을 제외한 나머지 속성(attribute)들에 [z-점수 정규화](/SKKU_swe3050/13-normalization#kramdown_z-점수-정규화-z-score-normalization)을 적용하여 데이터를 전처리한 후 학습시키도록 하겠다.

각 에폭(epoch)당 오차(loss)를 구해서 그래프를 그려보았다.

### SGD

학습률 0.0001로 총 100 에폭(epoch) 학습시켰다.

{% highlight python linenos %}
from sklearn import datasets
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

def loss(x, y, w):
    predict_y = x @ w
    return np.mean(np.square((y - predict_y)))

def status(w):
    w_ = np.squeeze(w)
    print("w = [", end="")
    for i in range(14):
        if(i == 13):
            print(w_[i], end="]")
        else:
            print(w_[i], end=", ") 
    print()
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    print("Training Loss = " + str(training_loss))
    print("Validation Loss = " + str(validation_loss))
    
    training_predict_y = training_x @ w
    validation_predict_y = validation_x @ w
    
    print("{:^40s}|{:^40s}".format("training", "validation"))
    print("{:^20s}{:^20s}|{:^20s}{:^20s}".format("predict_y", "true_y", "predict_y", "true_y"))
    for i in range(10):
        print("{:^20f}{:^20f}|{:^20f}{:^20f}".format(float(training_predict_y[i]), float(training_y[i]), float(validation_predict_y[i]), float(validation_y[i])))
    print()

def plot(title, training_losses, validation_losses):
    plt.title(title)
    plt.plot(range(len(training_losses)), training_losses, label="Train Loss")
    plt.plot(range(len(validation_losses)), validation_losses, label="Valid Loss")
    plt.legend()
    plt.show()
    
np.random.seed(2020) # for consistency

# data
dataset = datasets.load_boston()
x = dataset.data
y = np.expand_dims(dataset.target, axis=1)

# divide data into training set and validation set
training_x = x[0:406, :]
training_y = y[0:406, :]

validation_x = x[406:506, :]
validation_y = y[406:506, :]

# normalization
x_mean = np.expand_dims(np.mean(training_x, axis=0), axis=0)
x_std = np.expand_dims(np.std(training_x, axis=0), axis=0)

training_x = (training_x - x_mean) / x_std
validation_x = (validation_x - x_mean) / x_std

# reformat the data
training_x = np.concatenate((np.ones((training_x.shape[0], 1)), training_x), axis=1) # x0 = 1인 열 추가
validation_x = np.concatenate((np.ones((validation_x.shape[0], 1)), validation_x), axis=1) # x0 = 1인 열 추가

# initialize w
w = np.random.randn(training_x.shape[1], 1)
print("Before Training...")
status(w)

# hyperparameter
epochs = 100
lr = 0.0001

training_losses = []
validation_losses = []
data_num = training_x.shape[0]
for epoch in range(epochs):    
    for i in range(data_num):
        sample = training_x[i:i + 1, :]
        true_y = training_y[i:i + 1, :]
        
        predict_y = sample @ w
        
        # calculate gradient
        gradient = -(2 / sample.shape[0]) * sample.T @ (true_y - predict_y)
        
        # update w
        w = w - lr * gradient
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    training_losses.append(training_loss)
    validation_losses.append(validation_loss)

print("After Training...")
status(w)

plot("SGD", training_losses, validation_losses)
{% endhighlight %}

#### 실행 결과

<div class="result-folder" markdown="block">

{: .code-result }
{% highlight text %}
Before Training...
w = [-1.7688457055759508, 0.07555227120810952, -1.1306297028053423, -0.6514301669047348, -0.8931156263199002, -1.2741009767999079, -0.06115443195334841, 0.06451384411861422, 0.4101129497994469, -0.572882490081088, -0.8013336248612375, 1.3120351922129225, 1.2746988741941558, -1.2143576048675497]
Training Loss = 753.6785628758216
Validation Loss = 684.1089736142626
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     -0.141992           24.000000      |     -7.722083           11.900000      
      2.148229           21.600000      |     -7.135726           27.900000      
      2.819930           34.700000      |     -9.540326           17.200000      
      4.432565           33.400000      |     -12.657010          27.500000      
      4.088140           36.200000      |     -16.055317          15.000000      
      4.091320           28.700000      |     -17.435815          17.200000      
     -1.821722           22.900000      |     -19.733918          17.900000      
     -2.842757           27.100000      |     -11.400887          16.300000      
     -4.993001           16.500000      |     -19.079316           7.000000      
     -2.689193           18.900000      |     -19.859117           7.200000      

After Training...
w = [24.046096955381966, -1.2039408122346007, 0.8188991738299394, -0.05299347631553827, 0.6376611794449474, -1.1896204360445337, 3.5336437015803535, 0.09365915685209668, -2.444607887413131, 1.7687383673634012, -0.946388504153078, -1.6721037403534398, 0.024958459005594746, -3.78919608050688]
Training Loss = 23.063394863489698
Validation Loss = 27.31207360011818
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     30.073015           24.000000      |      5.969322           11.900000      
     24.952358           21.600000      |     20.566706           27.900000      
     31.404645           34.700000      |     13.926197           17.200000      
     29.711463           33.400000      |     22.295255           27.500000      
     29.135643           36.200000      |     16.213041           15.000000      
     25.755190           28.700000      |     20.463822           17.200000      
     22.547353           22.900000      |      2.579790           17.900000      
     19.263107           27.100000      |     11.454125           16.300000      
     10.548520           16.500000      |     -4.921313            7.000000      
     18.819478           18.900000      |     13.198470            7.200000      
{% endhighlight %}

</div>

{% include caption-img.html src="multiple-linear-regression-sgd-loss.png" title="Fig.01 SGD 실행 결과" description="각 에폭(epoch) 당 학습 데이터(training data)와 검증 데이터(validation data)에 대한 손실 함수(Loss Function) 값을 나타낸 그래프" %}

### Mini-Batch GD

학습률 0.001, 배치 크기 10으로 총 100 에폭(epoch) 학습시켰다.

{% highlight python linenos %}
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

def loss(x, y, w):
    predict_y = x @ w
    return np.mean(np.square((y - predict_y)))

def status(w):
    w_ = np.squeeze(w)
    print("w = [", end="")
    for i in range(14):
        if(i == 13):
            print(w_[i], end="]")
        else:
            print(w_[i], end=", ") 
    print()
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    print("Training Loss = " + str(training_loss))
    print("Validation Loss = " + str(validation_loss))
    
    training_predict_y = training_x @ w
    validation_predict_y = validation_x @ w
    
    print("{:^40s}|{:^40s}".format("training", "validation"))
    print("{:^20s}{:^20s}|{:^20s}{:^20s}".format("predict_y", "true_y", "predict_y", "true_y"))
    for i in range(10):
        print("{:^20f}{:^20f}|{:^20f}{:^20f}".format(float(training_predict_y[i]), float(training_y[i]), float(validation_predict_y[i]), float(validation_y[i])))
    print()

def plot(title, training_losses, validation_losses):
    plt.title(title)
    plt.plot(range(len(training_losses)), training_losses, label="Train Loss")
    plt.plot(range(len(validation_losses)), validation_losses, label="Valid Loss")
    plt.legend()
    plt.show()
    
np.random.seed(2020) # for consistency

# data
dataset = datasets.load_boston()
x = dataset.data
y = np.expand_dims(dataset.target, axis=1)

# divide data into training set and validation set
training_x = x[0:406, :]
training_y = y[0:406, :]

validation_x = x[406:506, :]
validation_y = y[406:506, :]

# normalization
x_mean = np.expand_dims(np.mean(training_x, axis=0), axis=0)
x_std = np.expand_dims(np.std(training_x, axis=0), axis=0)

training_x = (training_x - x_mean) / x_std
validation_x = (validation_x - x_mean) / x_std

# reformat the data
training_x = np.concatenate((np.ones((training_x.shape[0], 1)), training_x), axis=1) # x0 = 1인 열 추가
validation_x = np.concatenate((np.ones((validation_x.shape[0], 1)), validation_x), axis=1) # x0 = 1인 열 추가

# initialize w
w = np.random.randn(training_x.shape[1], 1)
print("Before Training...")
status(w)

# hyperparameter
epochs = 100
lr = 0.001
batch_size = 10

training_losses = []
validation_losses = []
data_num = training_x.shape[0]
batch_num = int(np.ceil(data_num / batch_size))
for epoch in range(epochs):    
    for i in range(batch_num):
        sample = training_x[i * batch_size:(i + 1) * batch_size, :]
        true_y = training_y[i * batch_size:(i + 1) * batch_size, :]
        
        predict_y = sample @ w
        
        # calculate gradient
        gradient = -(2 / sample.shape[0]) * sample.T @ (true_y - predict_y)
        
        # update w
        w = w - lr * gradient
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    training_losses.append(training_loss)
    validation_losses.append(validation_loss)

print("After Training...")
status(w)

plot("Mini-Batch GD", training_losses, validation_losses)
{% endhighlight %}

#### 실행 결과

<div class="result-folder" markdown="block">

{: .code-result }
{% highlight text %}
Before Training...
w = [-1.7688457055759508, 0.07555227120810952, -1.1306297028053423, -0.6514301669047348, -0.8931156263199002, -1.2741009767999079, -0.06115443195334841, 0.06451384411861422, 0.4101129497994469, -0.572882490081088, -0.8013336248612375, 1.3120351922129225, 1.2746988741941558, -1.2143576048675497]
Training Loss = 753.6785628758216
Validation Loss = 684.1089736142626
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     -0.141992           24.000000      |     -7.722083           11.900000      
      2.148229           21.600000      |     -7.135726           27.900000      
      2.819930           34.700000      |     -9.540326           17.200000      
      4.432565           33.400000      |     -12.657010          27.500000      
      4.088140           36.200000      |     -16.055317          15.000000      
      4.091320           28.700000      |     -17.435815          17.200000      
     -1.821722           22.900000      |     -19.733918          17.900000      
     -2.842757           27.100000      |     -11.400887          16.300000      
     -4.993001           16.500000      |     -19.079316           7.000000      
     -2.689193           18.900000      |     -19.859117           7.200000      

After Training...
w = [24.003436834807143, -1.2215090991638027, 0.8200889403540019, -0.05295860127998284, 0.6624655049660804, -1.2003254736427142, 3.4960794299898343, 0.08784760009643935, -2.454732102578915, 1.702148946225434, -0.9807414619460263, -1.6826237604074734, 0.0005286244059847394, -3.820119760891585]
Training Loss = 23.102779147478252
Validation Loss = 26.237806389764106
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     30.121356           24.000000      |      5.669553           11.900000      
     24.980634           21.600000      |     20.285571           27.900000      
     31.422691           34.700000      |     13.607298           17.200000      
     29.732706           33.400000      |     22.006951           27.500000      
     29.135327           36.200000      |     16.042926           15.000000      
     25.793132           28.700000      |     20.266870           17.200000      
     22.541829           22.900000      |      2.420059           17.900000      
     19.210466           27.100000      |     11.197669           16.300000      
     10.479796           16.500000      |     -5.197620            7.000000      
     18.789858           18.900000      |     12.963609            7.200000    
{% endhighlight %}

</div>

{% include caption-img.html src="multiple-linear-regression-minibatch-gd-loss.png" title="Fig.02 Mini-Batch GD 실행 결과" description="각 에폭(epoch) 당 학습 데이터(training data)와 검증 데이터(validation data)에 대한 손실 함수(Loss Function) 값을 나타낸 그래프" %}

### BGD

학습률 0.05로 총 100 에폭(epoch) 학습시켰다.

{% highlight python linenos %}
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

def loss(x, y, w):
    predict_y = x @ w
    return np.mean(np.square((y - predict_y)))

def status(w):
    w_ = np.squeeze(w)
    print("w = [", end="")
    for i in range(14):
        if(i == 13):
            print(w_[i], end="]")
        else:
            print(w_[i], end=", ") 
    print()
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    print("Training Loss = " + str(training_loss))
    print("Validation Loss = " + str(validation_loss))
    
    training_predict_y = training_x @ w
    validation_predict_y = validation_x @ w
    
    print("{:^40s}|{:^40s}".format("training", "validation"))
    print("{:^20s}{:^20s}|{:^20s}{:^20s}".format("predict_y", "true_y", "predict_y", "true_y"))
    for i in range(10):
        print("{:^20f}{:^20f}|{:^20f}{:^20f}".format(float(training_predict_y[i]), float(training_y[i]), float(validation_predict_y[i]), float(validation_y[i])))
    print()

def plot(title, training_losses, validation_losses):
    plt.title(title)
    plt.plot(range(len(training_losses)), training_losses, label="Train Loss")
    plt.plot(range(len(validation_losses)), validation_losses, label="Valid Loss")
    plt.legend()
    plt.show()
    
np.random.seed(2020) # for consistency

# data
dataset = datasets.load_boston()
x = dataset.data
y = np.expand_dims(dataset.target, axis=1)

# divide data into training set and validation set
training_x = x[0:406, :]
training_y = y[0:406, :]

validation_x = x[406:506, :]
validation_y = y[406:506, :]

# normalization
x_mean = np.expand_dims(np.mean(training_x, axis=0), axis=0)
x_std = np.expand_dims(np.std(training_x, axis=0), axis=0)
y_mean = np.expand_dims(np.mean(training_y, axis=0), axis=0)
y_std = np.expand_dims(np.std(training_y, axis=0), axis=0)

training_x = (training_x - x_mean) / x_std
training_y = (training_y - y_mean) / y_std

validation_x = (validation_x - x_mean) / x_std
validation_y = (validation_y - y_mean) / y_std

# reformat the data
training_x = np.concatenate((np.ones((training_x.shape[0], 1)), training_x), axis=1) # x0 = 1인 열 추가
validation_x = np.concatenate((np.ones((validation_x.shape[0], 1)), validation_x), axis=1) # x0 = 1인 열 추가

# initialize w
w = np.random.randn(training_x.shape[1], 1)
print("Before Training...")
status(w)

# hyperparameter
epochs = 100
lr = 0.01
batch_size = 10

training_losses = []
validation_losses = []
for epoch in range(epochs):    
    sample = training_x
    true_y = training_y
        
    predict_y = sample @ w
        
    # calculate gradient
    gradient = -(2 / sample.shape[0]) * sample.T @ (true_y - predict_y)

    # update w
    w = w - lr * gradient
    
    training_loss = loss(training_x, training_y, w)
    validation_loss = loss(validation_x, validation_y, w)
    training_losses.append(training_loss)
    validation_losses.append(validation_loss)

print("After Training...")
status(w)

plot("BGD", training_losses, validation_losses)

{% endhighlight %}

#### 실행 결과

<div class="result-folder" markdown="block">

{: .code-result }
{% highlight text %}
Before Training...
w = [-1.7688457055759508, 0.07555227120810952, -1.1306297028053423, -0.6514301669047348, -0.8931156263199002, -1.2741009767999079, -0.06115443195334841, 0.06451384411861422, 0.4101129497994469, -0.572882490081088, -0.8013336248612375, 1.3120351922129225, 1.2746988741941558, -1.2143576048675497]
Training Loss = 753.6785628758216
Validation Loss = 684.1089736142626
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     -0.141992           24.000000      |     -7.722083           11.900000      
      2.148229           21.600000      |     -7.135726           27.900000      
      2.819930           34.700000      |     -9.540326           17.200000      
      4.432565           33.400000      |     -12.657010          27.500000      
      4.088140           36.200000      |     -16.055317          15.000000      
      4.091320           28.700000      |     -17.435815          17.200000      
     -1.821722           22.900000      |     -19.733918          17.900000      
     -2.842757           27.100000      |     -11.400887          16.300000      
     -4.993001           16.500000      |     -19.079316           7.000000      
     -2.689193           18.900000      |     -19.859117           7.200000      

After Training...
w = [24.08921463333388, -1.2342452880789831, 0.9056143830747901, 0.006340630733213238, 0.604844898910663, -1.308049361040244, 3.564610117823015, 0.051459116824070124, -2.576797497662436, 2.008980280037464, -1.1294589522588423, -1.6975980114270075, 0.015616232029127063, -3.696551754224135]
Training Loss = 22.92032732781146
Validation Loss = 28.62756906851129
                training                |               validation               
     predict_y             true_y       |     predict_y             true_y       
     29.897972           24.000000      |      6.213087           11.900000      
     24.888460           21.600000      |     20.760369           27.900000      
     31.331011           34.700000      |     14.392531           17.200000      
     29.591305           33.400000      |     22.722847           27.500000      
     29.041141           36.200000      |     16.354946           15.000000      
     25.622959           28.700000      |     20.933768           17.200000      
     22.530049           22.900000      |      3.121172           17.900000      
     19.276153           27.100000      |     11.739837           16.300000      
     10.672502           16.500000      |     -4.580793            7.000000      
     18.775955           18.900000      |     13.646246            7.200000      
{% endhighlight %}

</div>

{% include caption-img.html src="multiple-linear-regression-bgd-loss.png" title="Fig.03 BGD 실행 결과" description="각 에폭(epoch) 당 학습 데이터(training data)와 검증 데이터(validation data)에 대한 손실 함수(Loss Function) 값을 나타낸 그래프" %}
