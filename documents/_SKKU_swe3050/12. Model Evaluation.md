---
title: "모델 평가 (Model Evaluation)"
order: 12
date_created: "2020-05-26"
date_modified: "2021-12-20"
---

# 모델 평가의 필요성

다음과 같은 상황을 생각해 보자.

1. 어떤 지도학습 문제가 주어져, 적당한 모델을 선택한 후 주어진 데이터 셋을 이용하여 다양한 방법으로 모델을 학습시켰다. 이때, 모델이 얼마나 잘 학습되었는지는 어떻게 알 수 있을까?
2. 어떤 지도학습 문제를 풀 수 있는 다양한 모델들이 주어졌다고 하자. 어떤 모델이 가장 잘 분류하는지 어떻게 알 수 있을까?

위의 두 상황은 모두 **모델의 성능을 평가할 수 있는 방법이 필요**함을 보여주고 있다.

모델을 평가하면 다음과 같은 일을 할 수 있다.

- 모델의 추가적인 학습 방향을 설계할 수 있다 : 하이퍼파라미터 값을 어떻게 설정해야 할 지 등에 대한 정보를 얻을 수 있다.
- 모델이 임의의 상황에 얼마나 잘 대응할 지(일반화되어 있는지) 짐작할 수 있다.
- 어떤 모델이 더 좋은 모델인지 객관적인 비교가 가능해진다.

# 평가 지표

모델 평가를 위해서 고민해야 할 첫 번째 문제는, 무엇을 기준으로 평가해야 할 것인지이다. 즉, 평가 지표가 무엇이냐는 것이다. 이는 문제 유형에 따라 달라지게 된다.

- 분류(Classification) 문제 : 정확도(Accuracy), 에러율(Error Rate), 정밀도(Precision), 재현율(Recall), F1 점수(F1 Score), AUC 등
- 회귀(Regression) 문제 : 평균절대오차(MAE, Mean of Absolute Error), 평균제곱오차(Mean of Squared Error), 평균제곱근오차(Root of Mean of Squared Error) 등

## 분류 문제

### 분류결과표(Confusion Matrix)

분류 문제의 성능 평가를 위해서는 우선 분류결과표를 구해야 한다.

정답[^1]이 `True`, `False`, 이렇게 두 개의 클래스로 구성된 이진 분류 문제(Binary Classification Problem)[^2]가 주어졌다고 해 보자. 이 문제를 풀기 위해 이진 분류기(Binary Classifier)[^3]을 만들었다. 이때 다음 4가지 경우를 생각할 수 있다.

[^1]: 레이블(Label), 참값(Ground Truth) 등으로도 불린다.
[^2]: 주어진 데이터를 두 개의 클래스(카테고리)로 분류하는 문제
[^3]: 분류 문제를 풀기 위한 모델을 분류기(Classifier)라 한다. 이진 분류 문제를 푸기 위한 모델을 이진 분류기(Binary Classifier)라 한다.

- True Positive (TP, 진양성) : 정답이 `True`인 데이터를, `True`로 분류함. 즉, 잘 분류한 경우.
- False Positive (FP, 위양성) : 정답이 `False`인 데이터를, `True`로 분류함. 즉, 잘못 분류한 경우.
- False Negative (FN, 위음성) : 정답이 `True`인 데이터를, `False`로 분류함. 즉, 잘못 분류한 경우.
- True Negative (TN, 진음성) : 정답이 `False`인 데이터를, `False`로 분류함. 즉, 잘 분류한 경우

{% include caption-img.html src="confusion-matrix.png" title="Fig.01 이진 분류 문제 분류 결과" description="열 방향은 실제 정답, 행 방향은 분류기가 분류한 결과를 나타낸다. TP, FP, FN, TN, 이렇게 총 4가지 경우가 나올 수 있다." %}

이름이 조금 헷갈릴 수 있는데, 우선 모델이 `True`로 분류하는 것을 Positive(양성), `False`로 분류하는 것을 Negative(음성)이라고 한다. 그리고 이 분류가 맞으면 앞에 True(진)를 붙이고, 이 분류가 틀리면 False(위)를 붙인다. 예를 들어 False Positive의 경우, "Positive"이므로 모델이 `True`로 분류했지만 "False"이므로 잘못 분류한 것(= 즉 참값은 `False`), 그러니까 "가짜 양성"이라 생각하면 쉽게 외울 수 있다.

모델이 테스트 데이터 셋의 데이터를 분류한 결과에 대해 True Positive의 개수, False Positive의 개수, False Negative의 개수, True Negative의 개수를 세어 표(행렬)로 만들 수 있는데, 이를 **분류결과표(Confusion Matrix)**라 한다.

예를 들어 어떤 모델이 10개의 테스트 데이터를 분류해 다음과 같은 결과를 얻었다고 해 보자.

<div class="table-wrapper" markdown="block">

<p class="title">예시 1</p>

|  idx  | 정답  | 분류값 |   분류 결과    |
| :---: | :---: | :----: | :------------: |
|   1   | True  | False  | False Negative |
|   2   | True  |  True  | True Positive  |
|   3   | True  | False  | False Negative |
|   4   | True  |  True  | True Positive  |
|   5   | False | False  | True Negative  |
|   6   | False |  True  | False Positive |
|   7   | True  |  True  | True Positive  |
|   8   | False | False  | True Negative  |
|   9   | True  | False  | False Negative |
|  10   | True  |  True  | True Positive  |

</div>

위 결과를 분류결과표로 나타내면 다음과 같이 된다.

$$
\left[ \begin{array}\\
TP&FP\\
FN&TN\\
\end{array} \right]
=
\left[ \begin{array}\\
4&1\\
3&2\\
\end{array} \right]
$$
{: #ex1}


만약 이진 분류 문제가 True, False가 아닌 다른 두 개의 클래스로 데이터를 분류해도 분류결과표를 만들 수 있다. 예를 들어 어떤 사진이 고양이(`Cat`)인지 강아지(`Dog`)인지를 분류하는 문제가 있다고 할 때, `Cat` 클래스를 `True` 클래스로, `Dog` 클래스를 `False` 클래스로 놓고 분류결과표를 만들면 된다.[^4] [^5]

[^4]: `Cat` 클래스를 '고양이가 맞는(True) 클래스', `Dog` 클래스를 '고양이가 아닌(False) 클래스'로 바꿨다고 이해할 수 있겠다.
[^5]: 물론 반대로 `Dog` 클래스를 `True`로, `Cat` 클래스를 `False`로 놓고 만들 수도 있다.

### 정확도(Accuracy)

**정확도(Accuracy)**란 모델이 맞게 분류한 데이터의 비율로, 높으면 높을수록 좋다.

$$ \textrm{Accuracy} = \frac {TP + TN}{TP + FP + FN + TN} $$

수식에서 볼 수 있다시피 전체 경우의 수($TP + FP + FN + TN$)에 대해 맞게 분류한 경우의 수($TP + TN$)의 비로 계산할 수 있다. [예시 1](#ex1)에서 정확도는 (4 + 2)/10 = 0.6이다.

정확도는 가장 직관적이고 단순한 평가 지표지만, True Positive와 True Negative 비율의 합으로 계산되기 때문에 `True` 클래스와 `False` 클래스의 빈도수에 차이가 크면[^6] 별로 신뢰할 만한 평가 지표가 될 수 없다. 다음 예시를 살펴보자.

> {:.title}
> 예시 2
> 
> 발병율이 매우 낮지만 한번 발병하면 아주 치명적인 질병 X가 있다고 하자. 회사 A가 질병 X가 발병했는지(`True`) 발병하지 않았는지(`False`) 진단할 수 있는 진단 키트 K를 개발했다고 한다. 근데 사실 K는 가짜 키트로서, 정말 X를 진단하는 것이 아니고 항상 무조건 미발병자라고(`False`)만 진단하는 키트다. 보건 당국은 이 키트를 임의로 선발된 10,000명에 대해서 성능 평가를 실시한다. 10,000명 중 9,990명이 미발병자이고, 오직 10명만이 발병자라 하면, 이 키트는 9,990명의 미발병자에 대해서는 미발병자라 맞게 분류할 테지만(True Negative), 10명의 발병자에 대해서는 미발병자라고 잘못 분류할(False Negative) 것이다.
> 
> $$
> \left[ \begin{array}\\
> TP&FP\\
> FN&TN\\
> \end{array} \right]
> =
> \left[ \begin{array}\\
> 0&0\\
> 10&9990\\
> \end{array} \right]
> $$
{: #ex2 }

이 경우, K의 정확도를 계산해 보면 (0 + 9,990)/10,000 = 99.90%로 나오게 된다. 명백히 잘못된 모델임에도 정확도가 매우 높게 계산된 것을 볼 수 있다.

[^6]: 이를 (정의역(Domain)에서) 데이터가 편향, 편중(bias)되어 있다고 표현한다.

### 에러율(Error Rate)

**에러율(Error Rate)**은 모델이 잘못 분류한 데이터의 비율로, 낮으면 낮을수록 좋다.

$$ \textrm{Error Rate} = 1 - \textrm{Accuracy} = \frac {FP + FN}{TP + FP + FN + TN} $$

수식에서 볼 수 있다시피 전체 경우의 수($TP + FP + FN + TN$)에 대해 잘못 분류한 경우의 수($FP + FN$)의 비로 계산할 수 있다. [예시 1](#ex1)에서 에러율은 (3 + 1)/10 = 0.4이다.

에러율도 정확도와 비슷한 문제가 발생한다. [예시 2](#ex2)에서, 진단 키트 K의 에러율은 고작 (10 + 0)/10,000 = 0.10%밖에 안된다.

### 정밀도(Precision)

**정밀도(Precision)**는 해당 클래스로 분류된 것 중 정답인 것의 비율로, 높으면 높을수록 좋다. 정밀도는 `True` 정밀도(True Precision)[^7]와 `False` 정밀도(False Precision)[^8]으로 구분할 수 있는데, 일반적으로 정밀도라 하면 `True` 정밀도를 의미한다. 각각의 식은 다음과 같다.

[^7]: Positive Predictive Value(PPV)라고도 불린다.
[^8]: Negative Predictive Value(NPV)라고도 불린다.

$$ \textrm{True Precision} = \frac {TP}{TP + FP} $$

$$ \textrm{False Precision} = \frac {TN}{TN + FN} $$

수식에서 볼 수 있다시피 한 클래스로 분류된 경우의 수($TP + FP$, $TN + FN$)에 대해, 맞게 분류한 경우의 수($TP$, $TN$)의 비로 계산할 수 있다. [예시 1](#ex1)에서 `True` 정밀도는 4/(4 + 1) = 0.8, `False` 정밀도는 2/(3 + 2) = 0.4이다.

정밀도를 이용하면 [예시 2](#ex2) 상황이 가지고 있던 문제를 해결할 수 있다. 진단 키트 K의 `False` 정밀도는 0이기 때문이다.

하지만 정밀도에도 한계가 있다. 우리는 정밀도가 높은 모델을 보고 True Positive 빈도수가 클 것이라 기대하는데, False Positive의 빈도를 낮추는 것만으로도 정밀도가 올라갈 수 있다. 다음 예시 상황을 살펴보자.

> {:.title}
> 예시 3
> 
> 회사 B도 X를 진단할 수 있는 진단 키트 L을 개발했다고 한다. L은 정말로 보수적인 진단 키트여서, 증상이 심각한 중증 X 발병자만 발병자로 진단하고, 증상이 약한 경증 X 발병자는 가차 없이 미발병자로 진단하는, 역시 문제가 있는 키트이다. 보건 당국은 아까 그 10,000명(미발병자 9,990명, 발병자 10명)에 대해서 L의 성능 평가를 실시한다. L은 9,990명의 미발병자를 모두 미발병자라 분류하고(True Negative), 10명의 발병자 중 중증 환자 1명만을 발병자로 분류하고(True Positive), 나머지 9명은 미발병자라 분류한다(False Negative).
> 
> $$
> \left[ \begin{array}\\
> TP&FP\\
> FN&TN\\
> \end{array} \right]
> =
> \left[ \begin{array}\\
> 1&0\\
> 9&9990\\
> \end{array} \right]
> $$
{: #ex3}

이 경우 L의 `True` 정밀도를 계산해 보면 1/(1 + 0) = 100%가 나오고, `False` 정밀도를 계산해 보면 9,990/(9,990 + 9) = 99.91%가 나온다. 명백히 잘못된 모델임에도 정밀도가 매우 높게 계산된 것을 볼 수 있다.

### 재현율(Recall)

**재현율(Recall)**은 한 클래스의 원소들 중 그 클래스로 분류된 것의 비율로, 높으면 높을수록 좋다. 정밀도와 마찬가지로 재현율 역시 `True` 재현율(True Recall)[^9]과 `False` 재현율(False Recall)[^10]이 존재한다. 일반적으로 재현율이라 하면 `True` 재현율을 의미한다. 재현율을 수식으로 표현하면 다음과 같다.

[^9]: 민감도(Sensitivity), Hit Rate, True Positive Rate(TPR) 등으로도 불린다.
[^10]: 특이도(Specificity), 선택도(Selectivity), True Negative Rate(TNR) 등으로도 불린다.

$$ \textrm{True Recall} = \frac {TP}{TP + FN} $$

$$ \textrm{False Recall} = \frac {TN}{TN + FP} $$

수식에서 볼 수 있다시피 진짜 `True`인 경우의 수($TP + FN$)에 대해, 맞게 분류한 경우의 수($TP$)의 비, 또는 진짜 `False`인 경우의 수($TN + FP$)에 대해, 맞게 분류한 경우의 수($TN$)의 비로 계산할 수 있다. [예시 1](#ex1)에서 `True` 재현율은 4/(4 + 3) = 0.57, `False` 재현율은 2/(1 + 2) = 0.67이다.

재현율을 이용하면 정밀도에서의 [예시 3](#ex3)이 가지고 있는 문제를 해결할 수 있다. 진단 키트 L의 `True` 재현율은 1/(1 + 9) = 10% 밖에 되지 않기 때문이다.

하지만 (당연하게도) 재현율에도 한계가 있다. 우리는 재현율이 높은 모델을 보고 True Positive 빈도수가 클 것이라 기대하는데, False Negative 빈도를 낮추는 것만으로도 재현율이 올라갈 수 있다. 다음 예시 상황을 살펴보자.

> {:.title}
> 예시 4
> 
> 회사 C도 X를 진단할 수 있는 진단 키트 M를 개발했다고 한다. M은 정말로 진보적인 진단 키트로서, 가벼운 감기 기운만 있어도 X가 발병했다고 진단하는, 역시 문제가 있는 키트이다. ~~제대로 된 회사가 없다.~~ 보건 당국은 다시 10,000명(미발병자 9,990명, 발병자 10명)에 대해서 M의 성능 평가를 실시한다. M은 발병자 10명에 대해서 전원 발병자로 분류하고(True Positive), 미발병자 9,990명 중 감기 기운이 있던 100명도 발병자라 분류하고(False Positive), 나머지 9,890명을 미발병자라 분류한다(True Negative).
>
> $$
> \left[ \begin{array}\\
> TP&FP\\
> FN&TN\\
> \end{array} \right]
> =
> \left[ \begin{array}\\
> 10&100\\
> 0&9890\\
> \end{array} \right]
> $$
{: #ex4}

이 경우 M의 `True` 재현율은 10/(10 + 0) = 100%, `False` 재현율은 9,890/(9,890 + 100) = 99.00%로 계산된다. 잘못된 모델이 높은 재현율을 갖는 상황이 발생한 것을 볼 수 있다.

### F1 점수(F1 Score)

[예시 4](#ex4)에서 M의 `True` 정밀도는 10/(10 + 100) = 9.09%로 매우 낮게 계산된다. 일반적으로 재현율이 높으면 정밀도가 낮고, 정밀도가 높으면 재현율이 낮은 관계가 성립한다. 이 사실을 바탕으로 고안된 평가 지표가 **F1 점수(F1 Score)**[^11]이다. F1 점수는 재현율과 정밀도의 조화 평균으로 정의된다. 수식으로 나타내면 다음과 같다.

[^11]: F-Score라고도 불린다.

$$ \textrm{F1 Score} = \frac {2}{\frac{1}{\textrm{Precision}} + \frac{1}{\textrm{Recall}}} = \frac{2 \cdot \textrm{Precision} \cdot \textrm{Recall}}{\textrm{Precision} + \textrm{Recall}} $$

[예시 1](#ex1)에서 (`True`) 정밀도는 4/(4 + 1) = 0.8, (`True`) 재현율은 4/(4 + 3) = 0.57이므로 F1 점수는 (2 · 0.8 · 0.57)/(0.8 + 0.57) = 0.83이 된다.

F1 점수가 높은 모델은 정밀도와 재현율이 둘 다 높다고 말할 수 있다. 하지만 상술했듯이 정밀도와 재현율은 trade-off 관계이기에, 이 둘을 동시에 올리는 것은 쉽지 않다.

F1 점수는 왜 산술 평균도, 기하 평균도 아닌 조화 평균을 사용할까? 조화 평균으로 계산한 F1 점수는 정밀도와 재현율의 중간에서, 둘 중 작은 값에 조금 더 치우친 값이 된다. 정밀도와 재현율은 극단적인 모델에 취약하다. F1 점수는 이런 극단적인 모델에서도 성능 평가를 잘 하기 위해 고안된 평가 지표이다. 극단적인 모델이어서 정밀도와 재현율 둘 중 어느 하나가 다른 하나에 비해 극단적으로 높아도, 둘 중 더 작은 값에 치우친 F1 점수는 크게 영향을 받지 않게 된다.

F1 점수에서 "1"의 의미는 무엇일까? 사실 다음과 같이 계산되는 F-Measure라는 지표가 있다.

$$ \textrm{F-Measure} = \frac {1}{\alpha \cdot \textrm{Precision}^{-1} + (1-\alpha) \cdot \textrm{Recall}^{-1}} = \frac{(\beta^2 + 1) \cdot \textrm{Precision} \cdot \textrm{Recall}}{\beta^2 \cdot \textrm{Precision} + \textrm{Recall}} \qquad
\textrm{where} \quad \beta^2 = \frac{1 - \alpha}{\alpha}
$$

F1 점수는 F-Measure의 $\beta$가 1인 특수한 경우이다. F1 점수의 1은 여기서 나왔다. 몇몇 특수한 경우 $\beta \neq 1$인 F-Measure를 사용하나, 거의 대부분의 경우 $\beta=1$인 F1 점수를 사용한다.

### AUC(Area Under Curve)

어떤 지도학습 모델을 쓰느냐에 따라 다르겠지만, 많은 경우 분류기의 출력값은 이산적인 값이 아닌 연속적인 실수 값이다. 예를 들어 입력받은 사진이 고양이인지 아닌지를 판단하는 로지스틱 회귀(logistic regression) 모델의 경우, 그 출력값은 (이산적인) 클래스(`Cat`, `Not Cat`)가 아닌 0.6(고양이일 확률 60%, 고양이가 아닐 확률 40%)과 같은 연속적인 값으로 주어진다.

이런 모델을 사용한다면 분류를 위해서는 사람이 직접 임계값(threshold)을 정해줘야 한다. 예를 들어 위 고양이 사진 분류기의 경우, "출력값이 0.7 이상인 경우 `Cat`으로, 그 이하일 경우 `Not Cat`으로 분류한다"와 같은 기준을 직접 정해야 한다.

하지만 이렇게 하면 정확도, 정밀도, 재현율 등의 지표를 쓰는 것이 어려워진다. 임계값을 어떻게 정하느냐에 따라 지표값들이 달라지기 때문이다. 최악의 경우, 제대로 학습되었더라도 임계값이 잘못 설정된 분류기가 제대로 학습되지 않은 모델보다도 성능이 낮게 나올 수도 있게 된다.

어럴 때 쓸 수 있는 지표가 **AUC(Area Under Curve)**이다. AUC는 **ROC(Receiver Operating Characteristic) Curve**라는 그래프의 밑넓이를 의미한다. ROC Curve의 각 축은 다음과 같다.

- x축 : FPR(False Positive Rate) = 1 - TNR(True Negative Rate) = 1 - 특이도(Specificity) = 1 - `False` 재현율(Recall)
- y축 : TPR(True Positive Rate) = 민감도(Sensitivity) = `True` 재현율(Recall)

다양한 임계값들에서의 모델의 `True` 재현율과 `False` 재현율을 구한 후, 이를 그래프 상에 나타내어 선으로 연결한 것이 바로 ROC Curve이다. 이때 `True` 재현율과 `False` 재현율은 trade-off 관계가 있으므로[^12] ROC Curve는 일반적으로 다음과 같이 $y = x$ 위에 있는 호(弧, arc) 모양의 곡선으로 그려진다.[^13]

[^12]: `True` 재현율이 높다는 것은 모델이 `True`라고 더 후하게 분류한다는 것이므로 참값이 `False`인 데이터들이 `True`로 잘못 분류되는 False Positive 경우가 많이져 `False` 재현율이 낮아진다. 역으로, `False` 재현율이 높다는 것은 모델이 `False`라고 더 후하게 분류한다는 것이므로 참값이 `True`인 데이터들이 `False`로 잘못 분류되는 False Negative 경우가 많아져 `True` 재현율이 낮아진다.
[^13]: 사실 `True` 재현율과 `False` 재현율이 trade-off 관계에 있으므로 ROC Curve는 $y=x$ 위에 있는 호, $y=x$ 아래에 있는 호, 이렇게 두 가지 가능성이 있다. 하지만 ROC Curve $y=x$ 밑에 있는 호로 나오는 모델은 FPR, TPR이 50%도 나오지 않는, 찍어서 맞추는 것보다도 못 맞추는 모델이라는 뜻이다. 그런데 이런 경우 단순히 모델의 출력값을 뒤집으면(즉 모델이 `Cat`이라 예측하면 `Not Cat`으로 분류하고, `Not Cat`으로 예측하면 `Cat`으로 분류) 잘 작동하는 모델이 된다! 그리고 이렇게 모델의 출력값을 뒤집으면 ROC Curve도 $y=x$에 대해 뒤집히기 때문에, 정상적인 모델의 ROC Curve는 항상 $y=x$ 위에 있게 된다.

{% include caption-img.html src="roc-curve.png" title="Fig.02 ROC(Receiver Operating Characteristic) Curve" description="ROC Curve는 다양한 임계값들에서의 FPR(False Positive Rate)와 TPR(True Positive Rate)을 그래프 상에 나타낸 뒤, 이를 선으로 연결한 것이다.<br/>왼쪽은 임계값을 10개 사용한 것이고, 오른쪽은 임계값을 100개 사용한 것이다.<br/>점선은 무직위로 찍어서 분류하는 모델의 ROC Curve인 $y=x$ 그래프를 의미한다.<br/>ROC Curve는 일반적으로 이와 같이 $y=x$ 위에 호(弧, arc) 모양의 곡선으로 그려진다." %}

AUC는 0.5 이상 1 이하의 값을 가진다.[^14] AUC 값이 크면 클수록 좋은 모델이다.[^15]

[^14]: 무작위로 찍어서 분류하는 모델의 ROC Curve는 $y=x$이다. $y=x$의 AUC는 0.5이므로, 모든 모델의 AUC는 0.5보다는 크다.
[^15]: ROC Curve가 왼쪽 위로 붙으면 붙을수록(= AUC가 1에 가까워질수록) `True` 재현율과 `False` 재현율이 모두 좋다는 것이므로 좋은 모델이다.

### 다중 분류(Multi-class Classification) 문제로의 확장

이진 분류 문제에서 이진 분류기의 성능을 평가하는 방법을 확장하면, 다중 분류 문제에서 다중 분류기(Multi-class Classifier)의 성능을 평가할 수 있다. 

주어진 과일 사진을 사과, 배, 포도 3개의 클래스로 분류하는 다중 분류기가 있다고 해 보자. 54개의 과일 사진을 이 모델에 넣어 다음과 같은 분류 결과를 얻었다고 해 보자.

{% include caption-img.html src="multi-class-classification-confusion-matrix.png" title="Fig.03 다중 분류 문제에 대한 분류결과표" description="총 54개의 과일 사진을 3개의 클래스(사과, 배, 포도)로 분류하는 다중 분류 문제에 대한 분류결과표" %}

#### 정확도(Accuracy)와 에러율(Error Rate)

정확도는 맞게 분류한 경우(대각선)의 수를 전체 경우의 수로 나눠주면 된다. 

{% include caption-img.html src="multi-class-classification-confusion-matrix-accuracy.png" title="Fig.04 다중 분류 문제에서의 정확도와 에러율" description="정확도는 전체에 대해 맞게 분류한 경우(파란색)의 비율로, 에러율은 1에서 정확도를 빼서 계산할 수 있다." %}

$$\textrm{Accuracy} = \frac{6 + 5 + 10}{54} = 0.39$$

에러율은 1에서 정확도를 빼주면 된다.

$$\textrm{Error Rate} = 1 - \textrm{Accuracy} = 0.61$$

#### 정밀도(Precision)

다중 분류 문제에서 정밀도는 마이크로 정밀도(Micro Precision)와 매크로 정밀도(Macro Precision), 이렇게 두 가지 종류가 있다.

##### 마이크로 정밀도(Micro Precision)

마이크로 정밀도는 클래스 각각에 대한 정밀도이다. 이진 분류 문제에서 사용했던 정밀도를 그대로 확장한 개념이므로 이해가 쉬울 것이다. 모델이 해당 클래스로 분류한 전체 경우의 수에 대해, 정말 해당 클래스인 것들의 경우의 수의 비로 계산할 수 있다.

{% include caption-img.html src="multi-class-classification-confusion-matrix-micro-precision.png" title="Fig.05 '사과' 클래스의 마이크로 정밀도" description="'사과' 클래스의 마이크로 정밀도는 '사과'로 분류된 데이터(초록색) 중 진짜 '사과'인 것(파란색)의 비율로 계산할 수 있다." %}

$$\textrm{Micro Precision}_{ {}_\textrm{Apple}} = \frac{6}{6 + 3 + 9} = 0.33$$

동일한 방식으로, '배' 클래스의 마이크로 정밀도, '포도' 클래스의 마이크로 정밀도을 계산하면 다음과 같이 된다.

$$\textrm{Micro Precision}_{ {}_\textrm{Pear}} = \frac{5}{2 + 5 + 7} = 0.36$$

$$\textrm{Micro Precision}_{ {}_\textrm{Grape}} = \frac{10}{8 + 4 + 10} = 0.45$$

##### 매크로 정밀도(Macro Precision)

매크로 정밀도는 마이크로 정밀도들의 평균이다.

$$\textrm{Macro Precision} = \frac{\textrm{Micro Precision}_{ {}_\textrm{Apple} } + \textrm{Micro Precision}_{ {}_\textrm{Pear} } + \textrm{Micro Precision}_{ {}_\textrm{Grape} }}{3} = \frac{0.33 + 0.36 + 0.45}{3} = 0.38$$

#### 재현율(Recall)

마찬가지로 다중 분류 문제에서는 재현율도 마이크로 재현율(Micro Recall)과 매크로 재현율(Macro Recall), 이렇게 두 가지 종류가 있다.

##### 마이크로 재현율(Micro Recall)

마이크로 재현율은 클래스 각각에 대한 재현율이다. 이진 분류 문제에서 사용했던 재현율을 그대로 확장한 개념이므로 이해가 쉬울 것이다. 정답이 해당 클래스인 경우의 수에 대해, 모델이 해당 클래스로 (맞게) 분류한 경우의 수의 비로 계산할 수 있다.

{% include caption-img.html src="multi-class-classification-confusion-matrix-micro-recall.png" title="Fig.06 '사과' 클래스의 마이크로 재현율" description="'사과' 클래스의 마이크로 재현율은 정답이 '사과'인 데이터(초록색) 중 '사과' 클래스로 분류된 것(파란색)의 비율로 계산할 수 있다." %}

$$\textrm{Micro Recall}_{ {}_\textrm{Apple}} = \frac{6}{6 + 2 + 8} = 0.38$$

동일한 방식으로, '배' 클래스의 마이크로 재현율, '포도' 클래스의 마이크로 재현율을 계산하면 다음과 같이 된다.

$$\textrm{Micro Recall}_{ {}_\textrm{Pear}} = \frac{5}{3 + 5 + 4} = 0.42$$

$$\textrm{Micro Recall}_{ {}_\textrm{Grape}} = \frac{10}{9 + 7 + 10} = 0.38$$

##### 매크로 재현율(Macro Recall)

매크로 재현율은 마이크로 재현율들의 평균이다.

$$\textrm{Macro Recall} = \frac{\textrm{Micro Recall}_{ {}_\textrm{Apple} } + \textrm{Micro Recall}_{ {}_\textrm{Pear} } + \textrm{Micro Recall}_{ {}_\textrm{Grape} }}{3} = \frac{0.38 + 0.42 + 0.38}{3} = 0.39$$

#### F1 점수(F1 Score)

다중 분류 문제에서 F1 점수는 마이크로 F1 점수(Micro F1 Score), 매크로 F1 점수(Macro F1 Score), 마이크로-평균 F1 점수(Micro-average F1 Score), 가중 F1 점수(Weighted F1 Score), 이렇게 4가지가 있다.

##### 마이크로 F1 점수(Micro F1 Score)

마이크로 F1 점수는 각 클래스별 마이크로 정밀도와 마이크로 재현율을 이용해 F1 점수를 계산한 것이다.

$$\textrm{Micro F1 Score} = \frac{2 \cdot \textrm{Micro Precision} \cdot \textrm{Micro Recall}}{\textrm{Micro Precision} + \textrm{Micro Recall}}$$

'사과' 클래스, '배' 클래스, '포도' 클래스에 대해 각각 마이크로 F1 점수를 구해보면 다음과 같이 된다.

$$\textrm{Micro F1 Score}_{ {}_\textrm{Apple}} = \frac{2 \cdot 0.33 \cdot 0.38}{0.33 + 0.38} = 0.35$$

$$\textrm{Micro F1 Score}_{ {}_\textrm{Pear}} = \frac{2 \cdot 0.36 \cdot 0.42}{0.36 + 0.42} = 0.39$$

$$\textrm{Micro F1 Score}_{ {}_\textrm{Grape}} = \frac{2 \cdot 0.45 \cdot 0.38}{0.45 + 0.38} = 0.41$$

##### 매크로 F1 점수(Macro F1 Score)

매크로 F1 점수는 마이크로 F1 점수들의 평균이다.

$$\textrm{Macro F1 Score} = \frac{\textrm{Micro F1 Score}_{ {}_\textrm{Apple}} + \textrm{Micro F1 Score}_{ {}_\textrm{Pear}} + \textrm{Micro F1 Score}_{ {}_\textrm{Grape}}}{3} = \frac{0.35 + 0.39 + 0.41}{3} = 0.38$$

##### 마이크로-평균 F1 점수(Micro-average F1 Score)

마이크로-평균 F1 점수를 계산하려면 우선 주어진 다중 분류결과표를 각 클래스별로, 해당 클래스와 해당 클래스가 아닌 것들, 이렇게 두 가지 클래스를 가진 이진 분류결과표로 바꾼 후, 이들 이진 분류결과표를 산술적으로 더한다.

{% include caption-img.html src="multi-class-classification-confusion-matrix-micro-average-f1-score.png" title="Fig.07 마이크로-평균 F1 점수" description="다중 분류결과표(왼쪽)를 각 클래스별로, 해당 클래스와 해당 클래스가 아닌 것들, 이렇게 두 가지 클래스를 가진 이진 분류결과표로 바꾼 후(가운데), 이들 이진 분류결과표를 산술적으로 더한다(오른쪽)." %}

이렇게 얻은 이진 분류결과표에 대해 정밀도와 재현율을 구한 후, 이로부터 F1 점수를 계산할 수 있다. 이렇게 얻는 F1 점수를 마이크로-평균 F1 점수(Micro-average F1 Score)라고 한다.

$$\textrm{Precision} = \frac{21}{21+33} = 0.39$$

$$\textrm{Recall} = \frac{21}{21+33} = 0.39$$

$$\textrm{Micro-average F1 Score} = \frac{2 \cdot 0.39 \cdot 0.39}{0.39 + 0.39} = 0.39$$

위 결과에서 볼 수 있듯이, 사실 이런 식으로 다중 분류결과표를 이진 분류결과표들로 바꾼 후 이를 모두 더하게 되면 정밀도, 재현율, F1 점수가 모두 같아진다.

##### 가중 F1 점수(Weighted F1 Score)

가중 F1 점수[^16]는 매크로 F1 점수를 구하는 방법과 유사하지만, 이때 단순히 산술 평균을 하는 것이 아니라 각 클래스별 데이터 수로 가중 평균(weighted average)을 구한다.

[^16]: 가중-평균 F1 점수(Weighted-average F1 Score)라 하기도 한다.

'사과' 클래스의 데이터 수는 6 + 2 + 8 = 16개, '배' 클래스의 데이터 수는 3 + 5 + 4 = 12개, '포도' 클래스의 데이터 수는 9 + 7 + 10 = 26개이므로, 가중 F1 점수는 다음과 같이 계산된다.

$$\textrm{Macro F1 Score} = \frac{(16 \cdot 0.35) + (12 \cdot 0.39) + (26 \cdot 0.41)}{16 + 12 + 26} = 0.39$$

## 회귀 문제

데이터 셋 $\\{(\mathbf{x}\_i,\,\mathbf{y}\_i)\,\|\,i = 1,\,2,\,\cdots,\,n\,\\}$이 주어져, 독립변수 $\mathbf{x}$를 이용해 종속변수 $y$를 추정하는 모델 $f$를 만들었다고 해 보자. 이때 다음 기준들을 이용해 모델 $f$의 성능을 측정할 수 있다.

### 평균절대오차(MAE, Mean of Absolute Error), 평균제곱오차(MSE, Mean of Squared Error)

평균절대오차와 평균제곱오차는 [이전 글](/SKKU_swe3050/04-error-functions)에서 자세히 소개한 바 있다. 여기서는 수식만 간단히 서술하겠다.

$$ \textrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - f(\mathbf{x_i})|$$

$$ \textrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\mathbf{x_i}))^2$$

평균절대오차와 평균제곱오차가 작을수록 좋은 모델인 것이다.

파라미터 최적화를 할 때와 마찬가지로 모델을 평가할 때도 평균제곱오차가 평균절대오차보다 더 많이 사용되는데, 그 이유는 오차가 작은 좋은 모델과 오차가 큰 안좋은 모델의 차이를 평균제곱오차가 평균절대오차보다 더 극적으로 보여주기 때문에 그렇다.

### 평균제곱근오차(RMSE, Root of Mean of Squared Error)

상술한 대로 평균제곱오차가 평균절대오차보다 일반적으로 더 좋은 것으로 평가받는데, 평균제곱오차는 (출력) 데이터와 차원이 다르다는 단점이 있다. 이를 보완하기 위해 만들어진 평가 지표가 평균제곱근오차(RMSE, Root of Mean of Squared Error)이다. 평균제곱근오차는 평균제곱오차에 근호를 씌워 단위를 맞춰준 것이다. 수식으로 다음과 같이 나타낼 수 있다.

$$ \textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - f(\mathbf{x_i}))^2}$$

평균제곱근오차는 평균제곱오차의 장점(큰 오차를 극명하게 강조한다)과 평균절대오차의 장점(출력 데이터와 차원이 같다)을 모두 가지고 있다고 평가된다.

### 결정계수(R-squared, $R^2$)

종속변수 $y\_i$들의 평균을 $\bar{y}$라 하자. 이때 종속변수들의 변량을 SST(Sum of Squared Total)라 하고, 다음과 같이 종속변수들의 분산의 합으로 계산할 수 있다.

$$\textrm{SST} = \sum_{i=1}^{n} (y_i - \bar{y})^2$$

모델에 $\mathbf{x}\_i$를 넣었을 때의 출력값을 $\hat{y}\_i = f(\mathbf{x}\_i)$라 하자. $\hat{y}\_i$와 정답 $y\_i$간의 차를 잔차(Residual)라 한다. 그리고 이 잔차들의 제곱의 합을 SSR(Sum of Squared Residual)이라 한다. SSR은 모델이 설명하지 못한 종속변수 $y$의 변량의 크기를 의미한다.

$$\textrm{SSR} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

SST에서 SSR을 뺀 것을 SSE(Sum of Squared Explained)라 하고, 다음과 같이 계산할 수 있다. SSE는 모델이 설명할 수 있는 종속변수 $y$의 변량을 의미한다.

$$\textrm{SSE} = \textrm{SST} - \textrm{SSR} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2$$

{% include caption-img.html src="sst-sse-ssr.png" title="Fig.8 SST, SSE, SSR" description="빨간 실선은 모델 $f$를, 초록 점선은 종속변수들의 평균 $\bar{y}$를 의미한다. 그래프에서 볼 수 있듯이, 종속변수들의 변량 SST은 모델이 설명 가능한 변량 SSE와 모델이 설명하지 못하는 변량 SSR의 합으로 나타내어진다. " %}

이때, 모델의 **결정계수(coefficient of determination)** $R^2$는 다음과 같이 정의된다.[^17]

[^17]: 골때리는게, SSR(Sum of Squared Residual)을 SSE(Sum of Squared Error)로, SSE(Sum of Squared Explained)를 SSR(Sum of Squared (explained by, due to) Regression)이라 부르는 사람도 있다.~~어쩜 이렇게 완벽하게 약어를 반대로 맞췄을까~~ 아래의 식은 Sum of Squared Residual, Sum of Squared Explained를 사용했을 때의 식이다.

$$R^2 = 1 - \frac{\textrm{SSR}}{\textrm{SST}} = \frac{\textrm{SSE}}{\textrm{SST}}$$

이를 말로 풀어 쓰면, 모델의 결정계수는 종속변수 $y$의 변량 중 모델이 설명 가능한 변량의 비율, 즉 모델의 설명력을 의미한다는 것을 알 수 있다. 좋은 모델일수록 (종속변수를 잘 설명할 테니) $R^2$의 크기가 커진다. $R^2$는 0 이상 1 이하의 값을 가질 수 있다.[^18] 

[^18]: 사실 위 식에서 $1 - \frac{\textrm{SSR}}{\textrm{SST}}$ 방식으로 $R^2$를 계산하면 음수가 나올 수도 있다. 그런데 이 말은 만들어진 모델의 설명력이 항상 $y$의 평균으로 찍는 모델보다도 약하다는 뜻이 된다. 정상적으로 웬만큼 동작하는 모델의 $R^2$는 양수가 나올 수밖에 없다.

참고로 선형회귀분석에서는 정답 $y$와 모델의 추정값 $\hat{y}\_i$ 간의 피어슨 상관계수(Pearson Correlation Coefficient, Pearson's R) $R$을 제곱한 값이 결정계수 $R^2$과 같아진다. 그래서 결정계수로 $R^2$이라는 기호를 쓰는 것이다.

### 조정된 결정계수(adjusted R-squared, $R^2\_{adj}$)

결정계수 $R^2$은 모델의 설명력을 훌륭하게 보여주지만, 한 가지 문제가 있다. 결정계수 $R^2$은 독립변수들의 개수가 커지면 커질수록 그 값이 커지는 특성이 있다. 다시말해, 그냥 변수의 개수를 늘리기만 해도 $R^2$이 커져 "좋은 모델"처럼 보이게 된다! 이럴 때 사용할 만한 것이 조정된 결정계수(adjusted coefficient of determination), $R^2\_{adj}$이다. 데이터셋의 크기가 $n$, 독립 변수의 개수가 $d$개일 때, $R^2\_{adj}$은 다음과 같이 계산된다.

$$R^2_{adj} = 1 - \frac{\displaystyle\frac{\textrm{SSR}}{n - d - 1}}{\displaystyle\frac{\textrm{SST}}{n - 1}}$$

$R^2\_{adj}$는 계산 과정에서 자유도(degree of freedom)를 고려하기에 무턱대고 변수의 개수가 늘어나더라도 $R^2$처럼 그 값이 커지지 않는다. 오히려 유의미하지 않은 독립변수를 모델에 추가하면 $R^2\_{adj}$의 값은 줄어든다.

이런 $R^2\_{adj}$의 성질을 이용하면 회귀 모델에서 무의미한 변수를 제거할 수 있다.

> {:.title}
> $R^2\_{adj}$를 이용해 최적의 모델 찾기 (Backward Elimination)
> 
> 1. 사용 가능한 모든 변수를 다 집어넣어 회귀 모델을 만든다. 이 모델을 "원본 모델"이라 하자.
> 2. 원본 모델의 $R^2\_{adj}$를 계산한다.
> 3. 원본 모델에서 변수를 하나씩 제거한 모델을 만들고(ex. 1번 변수가 없는 모델, 2번 변수가 없는 모델, ...), 각 모델들의 $R^2\_{adj}$값을 기록한다.
> 4. 원본 모델의 $R^2\_{adj}$와 비교했을 때 $R^2\_{adj}$가 가장 많이 커진 모델 원본 모델로 놓고, 위 과정을 반복한다.
> 5. 더 이상 원본 모델보다 변수를 제거했을 때 $R^2\_{adj}$가 커지지 않는다면 종료한다.

# 평가 데이터

모델 평가를 위한 평가 지표가 결정되었으면, 이제 두 번째 질문에 대해 고민해야 한다. 그렇다면 평가는 어떠한 데이터를 바탕으로 진행해야 할까?

## 홀드아웃 방법 (Hold-out Method)

홀드아웃 방법은 데이터를 학습 데이터 셋(training dataset)과 테스트 데이터 셋(test dataset)으로 나눈 후, 학습 데이터 셋은 모델을 학습할 때 사용하고, 테스트 데이터 셋은 학습된 모델의 평가를 위해 사용하는 방법이다. 통상적으로 테스트 데이터 셋의 크기는 전체 데이터 셋의 30% ~ 50% 정도가 되게 나눈다.

홀드아웃 방법이 성공적으로 적용되기 위해서는 첫째로, 학습 과정에서는 테스트 데이터를 절대 참조하면 안된다. 테스트 데이터로 모델을 학습시키면 모델이 테스트 데이터에 과적합(overfitting)될 수 있다. 우리가 모델을 평가하는 주된 목적 중 하나가 임의의 경우에도 잘 작동하는지를 확인하고 싶어서인데, 과적합된 데이터를 바탕으로 테스트를 진행하면 모델 평가의 공정성이 훼손된다. 이를 위해 학습 데이터 셋과 테스트 데이터 셋을 나눌 때 겹치는 데이터가 없도록 나눈다.

둘째로, 학습 데이터와 테스트 데이터의 분포(distribution)가 같아야 한다. 즉, 원본 데이터의 분포, 학습 데이터의 분포, 테스트 데이터의 분포가 모두 동일해야 한다.[^20] 50장의 강아지 사진과 50장의 고양이 사진이 주어지고, 이들을 `Dog` 클래스와 `Cat` 클래스로 분류하는 문제를 생각해 보자. 이때 원본 데이터의 분포는 (`Dog`, `Cat`) = 1:1이다. 이때 만약 강아지 사진 10장과 고양이 사진 40장으로 학습 데이터 셋을, 나머지 강아지 사진 40장과 고양이 사진 10장으로 테스트 데이터 셋을 구축한다면, 학습 데이터 셋의 분포는 (`Dog`, `Cat`) = 1:4, 테스트 데이터 셋의 분포는 (`Dog`, `Cat`) = 4:1이 된다. 이렇게 되면 학습 과정에서 과적합이 발생할 가능성이 높아지고, 테스트 데이터 셋의 공정성이 훼손될 가능성이 커진다. 이를 위해 학습 데이터 셋과 테스트 데이터 셋을 나눌 때 랜덤하게 나누는 방법을 주로 사용한다.

[^20]: 그리고 물론, 원본 데이터의 분포는 실제 입력 공간(input space)의 분포와 동일해야 한다. (그렇지 않다면 별로 좋은 데이터가 아닌 것이다.)

홀드아웃 방법은 직관적이고 단순하다는 장점이 있지만, 다음과 같은 단점이 있다.

- 데이터가 부족할 때는 별로 매력적인 방법이 아니다. 학습을 위한 데이터도 부족한데 평가를 위해서 테스트 데이터 셋을 분리해야 하기 때문이다.
- 학습 데이터와 테스트 데이터의 분포를 같게 만드는게 쉬운 일이 아니다. 홀드아웃 방법의 성능은 학습 데이터와 테스트 데이터가 얼마나 잘 분리되었는지에 따라 크게 차이난다.

## k-폴드 교차검증법 (k-fold Cross Validation)

홀드아웃 방법이 가지고 있는 단점을 보완할 수 있는 기술이 k-폴드 교차검증법(k-fold Cross Validation)[^21]이다. k-폴드 교차검증은 다음과 같은 방식으로 작동한다.

[^21]: 그냥 교차검증법(Cross Validation)이라고도 불린다.

1. 데이터를 $k$개의 비슷한 크기의 상호 배제적인(mutually exclusive)[^22] 집합(fold)으로 나눈다: $D_1$, $D_2$, …, $D_k$
2. 다음을 $i = 1, 2, \cdots, k$까지 총 $k$번 반복한다.
   1. $D_i$를 제외한 나머지 집합들로 모델을 학습한다.
   2. $D_i$로 모델을 검증(validate)한다.

[^22]: 겹치는 값 없이, 교집합 없이

k-폴드 교차검증법에서 모든 데이터들은 테스트 데이터 셋에 한 번 포함되고, 학습 데이터 셋에 (k - 1)번 포함된다.

k-폴드 교차검증법은 데이터가 부족할 때도 무리 없이 사용할 수 있다는 장점이 있다. 또한 데이터가 나뉘어지는 방식에 크게 영향을 받지 않는다는 장점도 있다. 하지만 시간이 오래 걸린다는 단점이 있다.

전체 데이터의 개수만큼 집합을 만들어 k-폴드 교차검증법을 실시할 수도 있는데, 이를 특별히 LOOCV(Leave-one-out Cross Validation)라 부른다. LOOCV에서는 1개의 데이터를 제외한 모든 데이터들이 학습에 참가하게 된다. 데이터의 수가 정말 적을 때 고려할 만한 방법이다.

## 결론 : 학습 데이터 셋, 검증 데이터 셋, 테스트 데이터 셋 (Training Dataset, Validation Dataset, Test Dataset)

보통 기계학습에서 모델을 학습시키기 전 가장 먼저 하는 일이 데이터를 분리하는 것이다.[^23] 주어진 전체 데이터 셋을 학습 데이터 셋(Training Dataset), 검증 데이터 셋(Validation Dataset), 테스트 데이터 셋(Test Dataset) 이렇게 세 집합으로 나누게 된다. 학습 데이터 셋은 말 그대로 모델을 학습시키기 위해 사용하는 데이터 셋이다. 검증 데이터 셋과 테스트 데이터 셋은 모델을 평가하기 위해 사용하는 데이터 셋이다. 둘의 차이는 다음과 같다.

[^23]: 혹은 처음부터 분리된 데이터 셋을 가지고 작업하게 된다.

- 검증 데이터 셋은 모의시험용 데이터셋이다. 현재 학습이 올바른 방향으로 되어가고 있는지 검증하는 목적으로 사용된다. 검증 데이터 셋에 대한 결과를 바탕으로 모델의 하이퍼파라미터를 조정한다.
- 테스트 데이터 셋은 실전시험용 데이터셋이다. 테스트 데이터는 모델의 학습이 종료된 이후, 최종적인 모델의 성능을 평가하기 위해 사용한다.

가장 이상적인 경우에는 데이터 수가 많아 전체 데이터 셋을 ~~화끈하게~~ 학습 데이터 셋, 검증 데이터 셋, 테스트 데이터 셋 세 덩이로 나누는 것이다. 하지만 만약 데이터 수가 부족하다면, 실전시험용 테스트 셋을 분리한 후, 나머지 데이터들에 대해 k-폴드 교차검증법 등을 적용하는 방법을 생각해 볼 수 있겠다.

