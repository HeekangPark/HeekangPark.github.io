---
title: "BGD, SGD, Minibatch GD"
order: 5
date_created: "2020-06-13"
date_modified: "2022-01-28"
---

# 배치 경사 하강법 (BGD, Batch Gradient Descent)

일반적으로 경사 하강법은 다음 순서로 진행된다.

1. 파라미터 $\mathbf{w}$를 무작위 값으로 초기화한다.
2. 오차 함수 $\mathbb{E}$에 대해, 다음 식으로 파라미터를 업데이트한다 : $\mathbf{w}\_{new} = \mathbf{w}\_{old} - \eta \nabla \mathbb{E}(\mathbf{w}\_{old})$
3. 2를 필요한 만큼 반복한다.

이때, 전체 데이터에 대한 그라디언트 $\nabla \mathbb{E}(\mathbf{w}\_{old})$를 구해 2번 과정에서 파라미터를 한 번 업데이트하는 방법을 **배치 경사 하강법(BGD, Batch Gradient Descent)**이라 한다.

# 확률적 경사 하강법 (SGD, Stochastic Gradient Descent)

데이터 수가 얼마 되지 않을 때는 배치 경사 하강법이 그렇게 부담스럽지 않다. 하지만 데이터 수가 정말 많다면 어떨까? 그라디언트 $\nabla \mathbb{E}(\mathbf{w})$ 계산 비용이 만만치 않을 것이다. 심지어 메모리에 다 올라가지도 않을 테라바이트 급 데이터를 다루는 경우, 그라디언트 계산을 위해 전체 데이터를 메모리에 올리는 것 자체가 물리적으로 불가능할 수도 있다!

이 문제를 해결하기 위해 **확률적 경사 하강법(SGD, Stochastic Gradient Descent)**이라는 방법이 제시되었다. 일반적으로 오차 함수는 각 데이터에 대한 오차의 합(평균) 형태이므로, 오차 함수의 그라디언트는 보통 데이터 하나에 대한 그라디언트의 합 형태로 표현된다.[^1]

[^1]: 예를 들어 [이 문제](/SKKU_swe3050/07-simple-classification)에서의 그라디언트 $\nabla J (\mathbf{w}) = \sum - y_i \, \mathbf{x}\_{i}$는 $- y_i \, \mathbf{x}_{i}$의 합 형태이다.

확률적 경사 하강법은 이 사실에 착안해, 전체 데이터가 아닌 개별 데이터에 대해 각각 그라디언트를 계산해 파라미터를 여러 번 업데이트한다. 즉, 데이터가 $n$개 있다면, 2번 과정에서 총 $n$번의 업데이트가 일어나는 것이다. 

확률적 경사 하강법으로 데이터 하나하나에 대한 그라디언트를 계산해 파라미터를 업데이트하게 되면 항상 오차 함수가 작아지는 방향으로 움직이는 것을 장담할 수 없다. 하지만 높은 '확률'로 오차 함수가 작아지는 방향으로 움직일 것이다. 이 때문에 '확률적(stochastic)'이라는 단어를 사용한다.[^2]

[^2]: 흔히 확률적 경사 하강법의 '확률적'이라는 말을 데이터 중 일부를 '확률적'으로 선택해서 학습하는 것이라 이해하는 경우가 있는데, 아니다. 확률적 경사 하강법도 모든 데이터를 사용한다.

# 배치 경사 하강법 vs. 확률적 경사 하강법

배치 경사 하강법에서 파라미터들은 아주 안정적으로 최적화된다(오차 함수가 안정적으로 최소화된다). 이 안정성은 단점이 될 수도 있는데, 만약 오차 함수가 볼록 함수(convex function)가 아닐 경우 아주 안정적으로 최소점(global mimimum)이 아닌 극소점(local minimum)으로 수렴해 버릴 수 있다. 또한 앞서 말했던 것처럼 모든 데이터를 다 살펴보는 것이 불가능할 수도 있고, 속도가 많이 느리다.

그에 반해 확률적 경사 하강법은 배치 경사 하강법에 비해 일반적으로 속도가 빠르다. 파라미터들은 훨씬 더 자주 업데이트된다. 하지만 데이터에 노이즈가 많거나 크다면 파라미터가 이상한 값으로 튀어 수렴 속도가 늦어지거나, 심할 경우 수렴이 안될 수도 있다. 하지만 최소점이 아닌 극소점에 빠졌더라도 탈출할 확률이 생긴다.

# 미니배치 경사 하강법(Minibatch GD, Minibatch Gradient Descent)

데이터 전체에 대해 한 번 그라디언트를 계산해 파라미터를 업데이트하는 배치 경사 하강법과, 데이터 각각에 대해 그라디언트를 계산해 파라미터를 여러 번 업데이트하는 확률적 경사 하강법을 절충하여 만들어진 방법이 **미니배치 경사 하강법(Minibatch Gradient Descent)**이다.

미니배치 경사 하강법은 전체 데이터를 몇 개의 뭉치로 나눈 후 그 뭉치에 대해 그라디언트를 계산하고 파라미터 업데이트를 진행한다. 이때 데이터 한 뭉치를 배치(batch)[^3]라 하고, 그 크기(데이터 뭉치 안의 데이터 수)를 배치 크기(batch size)라 한다. 배치 크기가 1이면 미니배치 경사 하강법은 확률적 경사 하강법과 같아지고, 배치 크기가 전체 데이터의 크기이면 미니배치 경사 하강법은 배치 경사 하강법과 같아진다.

참고로 헷갈릴 수 있는 몇 가지 용어를 정리하면 다음과 같다.

- 에폭(에포크, epoch)

  한 에폭을 돌았다는 것은 모델이 데이터 전체를 한 번 훑었다는 것이다. 총 $n$개의 데이터가 있을 때, 한 에폭 동안 확률적 경사 하강법에서는 $n$번, 배치 경사 하강법에서는 1번, 미니배치 경사 하강법에서는 ($n$/배치 크기)번 파라미터 업데이트가 일어난다. 한 에폭에는 한 번 이상 배치가 수행된다.

- 배치(batch)
  
  한 번 그라디언트를 계산하고 파라미터를 업데이트하는 작업, 또는 그 작업을 위해 사용하는 데이터 뭉치를 의미한다. 데이터 뭉치의 크기(데이터 수)를 배치 크기(batch size)라 한다.

- 미니배치(mini-batch)
  
  *미니배치 경사 하강법에서* 한 번 그라디언트를 계산하고 파라미터를 업데이트하는 작업, 또는 그 작업을 위해 사용하는 데이터를 의미한다. 즉, 미니배치 경사 하강법에서의 배치이다. 때때로 배치는 전체 데이터라는 의미로 쓰이는데, 이 경우 그 중 일부의 데이터 뭉치를 표현할 때 미니배치라는 표현을 쓴다.


