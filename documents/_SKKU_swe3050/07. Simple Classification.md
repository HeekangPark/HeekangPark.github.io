---
title: "단순분류(Simple Classification)"
order: 7
date_created: "2020-05-08"
date_modified: "2021-11-04"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순분류(Simple Classification) 문제를 풀어보자.

# 문제 상황

다음과 같이 입력 $\boldsymbol{x}(x_1, x_2)$에 대한 출력(레이블) $y$의 데이터가 주어졌다고 해 보자.

<div class="table-wrapper" markdown="block">

| $x_1$ | $x_2$ |  $y$  |
| :---: | :---: | :---: |
| -2.58 | 2.93  |   1   |
| 1.31  | -5.90 |  -1   |
| -2.87 | 8.99  |   1   |
| 4.72  | -9.44 |  -1   |
| -8.65 | 8.96  |   1   |
| -1.63 | -9.10 |  -1   |
| -7.35 | -3.05 |   1   |
| 9.06  | -6.63 |  -1   |
| -2.88 | 7.55  |   1   |
| 1.16  | -7.63 |  -1   |
| -9.49 | 2.22  |   1   |
| 7.62  | -7.60 |  -1   |
| -7.49 | 7.62  |   1   |
| 4.90  | 8.06  |  -1   |
| -2.88 | -3.11 |   1   |
| 2.76  | 0.79  |  -1   |
| -7.65 | 9.42  |   1   |
| 9.37  | 0.39  |  -1   |
| -0.59 | 7.96  |   1   |
| 9.02  | -0.76 |  -1   |
| -4.54 | -6.58 |   1   |
| 7.46  | 6.30  |  -1   |
| -7.22 | -1.24 |   1   |
| 5.39  | 7.48  |  -1   |
| 0.41  | 6.26  |   1   |
| -0.40 | -9.40 |  -1   |
| -4.50 | -5.37 |   1   |
| 5.73  | -1.66 |  -1   |
| -8.93 | -0.80 |   1   |
| 7.09  | 1.56  |  -1   |
| -3.33 | 9.85  |   1   |
| 6.76  | 7.00  |  -1   |
| -4.79 | 9.76  |   1   |
| 5.89  | 3.02  |  -1   |
| -0.88 | 6.14  |   1   |
| 8.40  | 5.63  |  -1   |
| -7.59 | 9.02  |   1   |
| 0.58  | -0.11 |  -1   |
| -8.49 | -9.60 |   1   |
| 7.69  | -6.41 |  -1   |

</div>

{% include caption-img.html src="simple-classification-data.png" title="Fig.01 데이터" description="빨간 점은 $y=1$, 파란 점은 $y=-1$인 데이터이다." %}

이 데이터를 바탕으로, 새로운 입력 $\boldsymbol{x}_{new}$가 주어졌을 때, 출력 $y$값이 -1일지(파란 점) 1일지(빨간 점) 추정할 수 있는 모델을 설계해보자.

# 모델 설계 : 퍼셉트론 (Perceptron)

산점도를 보게 되면 왼쪽 위가 빨간 점($y=1$)이고, 오른쪽 아래가 파란 점($y=-1$)이다. 이에 다음과 같은 모델을 생각할 수 있다.



$$f(\boldsymbol{x}) = \mathrm{sign}(\boldsymbol{w}^T \boldsymbol{x})$$



모델의 파라미터 $\boldsymbol{w} = [w_1, w_2]$는 2×1 벡터이다. $\mathrm{sign}$함수는 입력값의 부호를 반환하는 함수이다.[^1]

[^1]: 입력값이 양수면 +1을, 음수면 -1을 반환한다.

즉 이 모델은 $w_1 x_1 + w_2 x_2$의 값이 양수이면 빨간 점, 음수이면 파란 점이라 분류하는 모델인 것이다. 기하적으로는 직선 $w_1 x_1 + w_2 x_2 = 0$보다 위에 있으면 빨간 점, 밑에 있으면 파란 점이라 분류하는 모델이라 이해할 수 있겠다.

이 모델을 [퍼셉트론(Perceptron)](/swe3050/08-perceptron)이라 한다.

# 파라미터 최적화

## 오차 함수의 정의 : 0-1 손실 함수 (0-1 Loss Function)

경사 하강법을 사용하기 위해서는 우선 오차 함수를 정의해야 한다. 이 문제에서 사용할 오차 함수는 "0-1 손실 함수(0-1 Loss Function)"이라 불리는 함수이다. 주어진 데이터 셋 $D$에 대해 0-1 손실 함수 $J(\boldsymbol{w})$는 다음과 같이 정의된다.



$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, D } -y_i \cdot (\boldsymbol{w}^T \boldsymbol{x}_i) \cdot M(\boldsymbol{w}, \boldsymbol{x}_i, y_i)$$



이때 $M(\boldsymbol{w}, \boldsymbol{x}, y)$는 다음과 같이 정의되는 함수이다.



$$M(\boldsymbol{w}, \boldsymbol{x}, y) = \begin{cases} 1 \qquad \text{if } y \neq f(\boldsymbol{x}) \\ 0 \qquad \text{if } y = f(\boldsymbol{x})\end{cases}$$



$M(\boldsymbol{w}, \boldsymbol{x}, y)$은 제대로 분류되지 않은 데이터($y \neq f(\boldsymbol{x})$)만 골라주는 함수이다. 이를 사용하여 $J(\boldsymbol{w})$에서는 제대로 분류되지 않은 데이터에 대해, $-y_i \cdot (\boldsymbol{w}^T \boldsymbol{x}_i)$들의 합을 구하게 된다.

$J(\boldsymbol{w})$에는 다음과 같은 특징이 있다.

- $J(\boldsymbol{w}) \ge 0$ : $y_i$와 $\boldsymbol{w}^T \boldsymbol{x}_i$의 부호가 다를 때만 $M(\boldsymbol{w}, \boldsymbol{x}, y)$이 활성화(1)된다.
- 최적화되면, 즉 모든 데이터가 제대로 분류되었으면 $J(\boldsymbol{w}) = 0$
- 잘못 분류된 데이터가 많을수록 $J(\boldsymbol{w})$의 값이 커진다.

안타깝게도 0-1 손실 함수는 해석적 풀이법이 존재하지 않는다. 이 문제는 경사 하강법으로 풀어야 한다.

## 오차 함수의 그라디언트 구하기

오차함수의 그라디언트를 구해 보자. 그런데 $M(\boldsymbol{w}, \boldsymbol{x}, y)$은 미분 불가능한 함수이기에, 다음과 같이 $J(\boldsymbol{w})$의 형태를 변형한다.



$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } -y_i \cdot (\boldsymbol{w}^T \boldsymbol{x}_i)$$



$(\boldsymbol{x}_i, y_i)$들의 집합을 "전체 데이터의 집합" $D$에서 "제대로 분류되지 않은 데이터들의 집합" $S$로 바꾸면서 미분 불가능한 $M(\boldsymbol{w}, \boldsymbol{x}, y)$을 제거했다. 이제는 미분이 가능하다.

$\boldsymbol{w}$의 $j$번째 원소 $w_j$에 대해,

$$\frac{\partial }{\partial w_j} J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } \frac{\partial}{\partial w_j} ( -y_i (w_0 x_{i0} + w_1 x_{i1} + \cdots + w_nx_{in}) ) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i x_{ij}$$

가 된다.

따라서 $J(\boldsymbol{w})$의 그라디언트는 다음과 같이 계산된다.



$$\nabla J(\boldsymbol{w}) = [\frac{\partial }{\partial w_0} J(\boldsymbol{w}), \frac{\partial }{\partial w_1} J(\boldsymbol{w}), \cdots, \frac{\partial }{\partial w_n} J(\boldsymbol{w})] = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i \boldsymbol{x}_{i}$$



## 경사 하강법 적용하기

이제 경사 하강법을 적용해 보자.

우선 파라미터를 랜덤한 값으로 초기화한다. 이후 업데이트를 진행한다. 경사 하강법에서 파라미터 업데이트는 $\boldsymbol{w}\_{new} = \boldsymbol{w}\_{old} - \eta \nabla J(\boldsymbol{w}_{old})$을 이용하여 진행된다. 이 식에 계산한 그라디언트를 넣고 학습률 $\eta = 1$로 설정하면 다음 업데이트 식을 얻을 수 있다.



$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} + \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } y_i \boldsymbol{x}_{i}$$



이제 이 식을 가지고 더이상 잘못 분류된 데이터가 없을 때까지 파라미터를 업데이트 하면 된다.

# 문제 풀이

이제 위의 내용을 바탕으로 실제 동작하는 코드를 작성해 보자.

참고로 아래 코드들의 실행 결과는 seed가 2020일 때 계산된 결과이다. `np.random.seed(2020)`라 두면 같은 실행 결과를 얻을 수 있을 것이다.

## BGD

{% highlight python linenos %}
import numpy as np
import matplotlib.pyplot as plt

def draw(title):
    print(title)
    
    pos_x = X[Y == 1]
    neg_x = X[Y == -1]

    t = -(w[0] / w[1])
    tan_x = np.array(range(-15, 15))
    tan_y = t * tan_x

    plt.scatter(pos_x[:, 0], pos_x[:, 1], c="r")
    plt.scatter(neg_x[:, 0], neg_x[:, 1], c="b")
    plt.plot(tan_x, tan_y, c='k')
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.show()
    
    print(w)
    print()

np.random.seed(2020) # for consistency

# Data
x1 = [
    -2.58, -2.87, -8.65, -7.35, -2.88, -9.49, -7.49, -2.88, -7.65, -0.59,
    -4.54, -7.22,  0.41, -4.50, -8.93, -3.33, -4.79, -0.88, -7.59, -8.49,
     1.31,  4.72, -1.63,  9.06,  1.16,  7.62,  4.90,  2.76,  9.37,  9.02,
     7.46,  5.39, -0.40,  5.73,  7.09,  6.76,  5.89,  8.40,  0.58,  7.69
] 

x2 = [
     2.93,  8.99,  8.96, -3.05,  7.55,  2.22,  7.62, -3.11,  9.42,  7.96,
    -6.58, -1.24,  6.26, -5.37, -0.80,  9.85,  9.76,  6.14,  9.02, -9.60,
    -5.90, -9.44, -9.10, -6.63, -7.63, -7.60,  8.06,  0.79,  0.39, -0.76,
     6.30,  7.48, -9.40, -1.66,  1.56,  7.00,  3.02,  5.63, -0.11, -6.41
]

X = np.empty([len(x1), 2], dtype=np.float32)
X[:, 0], X[:, 1] = x1, x2

Y = [
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
]

Y = np.array(Y, dtype=np.int32)

eta = 1 # Hyperparameter : Learning Rate

# initialize w
w = np.random.rand(X.shape[1])

draw("Before")

while True:
    S = []
    
    for i in range(len(X)):
        x, y = X[i], Y[i]
        predict_y = (1 if w.dot(x) > 0 else -1)
        
        if predict_y != y: # if misclassified
            S.append([x, y])
    
    if len(S) == 0:
        break
    
    gradient = np.zeros(w.shape[0])
    for wrong_data in S:
        x, y = wrong_data
        gradient += -y * x # calc gradient
    
    w = w - eta * gradient # update w

draw("After")
{% endhighlight %}

### 실행 결과

{% include caption-img.html class="no-max-height" src="simple-classification-bgd-result.png" title="Fig.02 BGD" description="위 그래프는 최적화 전, 아래 그래프는 최적화 후. 빨간 점은 $y=1$, 파란 점은 $y=-1$, 검은 선은 파라미터를 나타냄. 그래프 아래 숫자는 w를 나타냄." %}

## SGD

{% highlight python linenos %}
import numpy as np
import matplotlib.pyplot as plt

def draw(title):
    print(title)
    
    pos_x = X[Y == 1]
    neg_x = X[Y == -1]

    t = -(w[0] / w[1])
    tan_x = np.array(range(-15, 15))
    tan_y = t * tan_x

    plt.scatter(pos_x[:, 0], pos_x[:, 1], c="r")
    plt.scatter(neg_x[:, 0], neg_x[:, 1], c="b")
    plt.plot(tan_x, tan_y, c='k')
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.show()
    
    print(w)
    print()

np.random.seed(2020) # for consistency

# Data
x1 = [
    -2.58, -2.87, -8.65, -7.35, -2.88, -9.49, -7.49, -2.88, -7.65, -0.59,
    -4.54, -7.22,  0.41, -4.50, -8.93, -3.33, -4.79, -0.88, -7.59, -8.49,
     1.31,  4.72, -1.63,  9.06,  1.16,  7.62,  4.90,  2.76,  9.37,  9.02,
     7.46,  5.39, -0.40,  5.73,  7.09,  6.76,  5.89,  8.40,  0.58,  7.69
] 

x2 = [
     2.93,  8.99,  8.96, -3.05,  7.55,  2.22,  7.62, -3.11,  9.42,  7.96,
    -6.58, -1.24,  6.26, -5.37, -0.80,  9.85,  9.76,  6.14,  9.02, -9.60,
    -5.90, -9.44, -9.10, -6.63, -7.63, -7.60,  8.06,  0.79,  0.39, -0.76,
     6.30,  7.48, -9.40, -1.66,  1.56,  7.00,  3.02,  5.63, -0.11, -6.41
]

X = np.empty([len(x1), 2], dtype=np.float32)
X[:, 0], X[:, 1] = x1, x2

Y = [
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
]

Y = np.array(Y, dtype=np.int32)

eta = 1 # Hyperparameter : Learning Rate

# initialize w
w = np.random.rand(X.shape[1])

draw("Before")

while True:
    quit = True
    
    for i in range(len(X)):
        x, y = X[i], Y[i]
        predict_y = (1 if w.dot(x) > 0 else -1)
        
        if predict_y != y: # if misclassified
            gradient = -y * x # calc gradient
            w = w - eta * gradient # update w
            quit = False
    
    if quit == True:
        break

draw("After")
{% endhighlight %}

### 실행 결과

{% include caption-img.html class="no-max-height" src="simple-classification-sgd-result.png" title="Fig.03 SGD" description="위 그래프는 최적화 전, 아래 그래프는 최적화 후. 빨간 점은 $y=1$, 파란 점은 $y=-1$, 검은 선은 파라미터를 나타냄. 그래프 아래 숫자는 w를 나타냄." %}

<style>
.table-wrapper {
    overflow-x: auto;
    overflow-y: auto;
    max-height: 30em;
}

.table-wrapper.no-max-height {
    max-height: none;
}

.table-wrapper table {
    border-collapse: separate;
    border-spacing: 0;
    border-top: 1px solid #888888;
    overflow-x: auto;
    margin-left: auto;
    margin-right: auto;
}

.table-wrapper table th {
    background-color: #dde3ee;
}

.table-wrapper table th,
.table-wrapper table td {
    padding-top: 0.5em;
    padding-bottom: 0.5em;
    padding-left: 1em;
    padding-right: 1em;

    border-bottom: 1px solid #888888;
}

.table-wrapper table > tbody tr:nth-child(even) td {
    background-color: #eef1f7;
}

.table-wrapper table > tbody tr:nth-child(odd) td {
    background-color: #ffffff;
}
</style>