---
title: "퍼셉트론(Perceptron)"
order: 7
date_created: "2020-05-11"
date_modified: "2022-01-28"
---

[단순분류 문서](/SKKU_swe3050/07-simple-classification)에서 사용한 모델

$$f(\boldsymbol{x}) = \mathrm{sign}(\mathbf{w}^T \mathbf{x})$$

를 퍼셉트론(Perceptron)이라 한다. 이번 글에서는 퍼셉트론에 대해 알아보자.

# 퍼셉트론 (Perceptron)

퍼셉트론은 1957년 프랑크 로젠블라트(Frank Rosenblatt)라는 사람에 의해 고안된, 신경 세포(뉴런, neuron)의 동작을 모방하여 만들어진 분류기(classifier)이다.

신경 세포는 가지 돌기(dendrite)로부터 신호를 받아들여, 그 총합이 특정 값(역치, threshold) 이상일 경우 활성화(activate)되어 축삭 돌기 말단(axon terminal)으로 신호를 전달한다. 이와 유사하게, 퍼셉트론은 두 가지 요소로 구성된다.

- 선형 결합기(linear combiner) : 선형 연산(입력의 가중합(weighted sum)[^1])을 수행한다. 신경 세포의 가지 돌기(dendrite)를 모사한 부분이다.
- 하드 리미터(hard limiter) : 선형 결합기로부터의 출력값이 임계값(threshold)를 넘으면 +1, 넘지 못하면 -1을 출력한다. 신경 세포의 축삭 돌기 말단(axon terminal)을 모사한 부분이다.

[^1]: 가중합이란 입력($x$)들을 더할 때 각각에 가중치($w$)를 곱하여 더한다는 뜻이다.

{% include caption-img.html src="perceptron-neuron.png" title="Fig.01 신경 세포와 퍼셉트론" description="왼쪽은 신경 세포를, 오른쪽은 퍼셉트론을 도식화한 것이다. 신경 세포에서 머리 부분(왼쪽)의 가지처럼 뻗어 나온 것이 가지 돌기(dendrite)이고, 꼬리 부분(오른쪽)이 축삭 돌기 말단(axon terminal)이다. 퍼셉트론에서 왼쪽 반원이 선형 결합기(linear combiner)이고, 오른쪽 반원이 하드 리미터(hard limiter)이다." %}

퍼셉트론은 다음과 같이 식으로 나타낼 수도 있다.

$$f(\mathbf{x}) = \mathrm{sign}(\mathbf{w}^T \mathbf{x})$$

여기서 $\mathbf{w}^T \mathbf{x}$ 부분이 선형 결합기이고, $\mathrm{sign}$ 부분이 하드 리미터이다. $\mathrm{sign}$ 하드 리미터의 임계값은 0이다.

퍼셉트론을 이용하면 샘플을 "출력 결과로 (+1)이 나오는 집합"와 "(-1)이 나오는 집합", 이렇게 두 개의 집합(class)으로 분류할 수 있다. 이를 기하학적으로 생각하면, 퍼셉트론은 $n$차원의 입력 공간(input space)을 두 개의 영역(클래스)으로 나누는 초평면(hyperplane)을 나타낸다고 할 수 있다. 그리고 퍼셉트론을 학습시킨다는 것은 바로 그 초평면을 찾는 과정이라 이해할 수 있다.

# 퍼셉트론 학습 알고리즘 (PLA, Perceptron Learning Algorithm)

퍼셉트론은 퍼셉트론 학습 알고리즘(PLA, Perceptron Learning Algorithm)을 통해 학습시킬 수 있다. 퍼셉트론 학습 알고리즘은 경사 하강법을 이용하여 퍼셉트론 모델의 파라미터인 가중치($\mathbf{w}$)의 최적화를 수행한다.

샘플 $\mathbf{x}$와 레이블 $y$가 주어졌고, 가중치 $\mathbf{w}$를 파라미터로 갖는 퍼셉트론 모델 $f(\mathbf{x}) = \mathrm{sign}(\mathbf{w}^T \mathbf{x})$을 해당 데이터에 대해 학습시키려 하는 상황을 생각해 보자. 모델이 데이터를 얼마나 잘못 분류하는지를 나타내는 오차 함수 $J(\mathbf{w})$가 주어졌을 때, 퍼셉트론 학습 알고리즘은 다음과 같은 순서로 작동한다.

1. $\mathbf{w}$를 무작위 값으로 초기화한다.
2. 현재의 $\mathbf{w}$에 대해 $J(\mathbf{w})$를 계산한다. 즉, 현재 모델이 데이터를 얼마나 잘못 분류하는지를 오차를 계산한다.
3. 경사 하강법을 이용하여 파라미터를 업데이트한다 : $\mathbf{w}\_{new} = \mathbf{w}\_{old} - \eta \nabla J(\mathbf{w}_{old})$
4. 2~3 과정을 잘못 분류된 데이터가 없어질 때까지(= $J(\mathbf{w})$가 0이 될 때까지 = 최적화 될 때까지) 반복한다.

# XOR 문제

퍼셉트론은 선형적 연산을 통해 데이터를 분류한다. 즉 퍼셉트론은 선형적으로 분리 가능한(linearly separable) 데이터는 놀라울 정도로 잘 분류할 수 있다.

예를 들어, 다음 데이터를 봐 보자.

<div class="table-wrapper" markdown="block">

| $x_1$ | $x_2$ | OR($x_1$, $x_2$) |
| :---: | :---: | :--------------: |
|   0   |   0   |        -1        |
|   0   |   1   |        1         |
|   1   |   0   |        1         |
|   1   |   1   |        1         |

</div>

<div class="table-wrapper" markdown="block">

| $x_1$ | $x_2$ | AND($x_1$, $x_2$) |
| :---: | :---: | :---------------: |
|   0   |   0   |        -1         |
|   0   |   1   |        -1         |
|   1   |   0   |        -1         |
|   1   |   1   |         1         |

</div>

{% include caption-img.html src="perceptron-or-and.png" title="Fig.02 OR GATE, AND GATE" description="왼쪽 그래프는 OR GATE을, 오른쪽 그래프는 AND GATE을 나타낸다. 빨간 선은 이들 데이터를 분류하는 퍼셉트론을 나타낸다." %}

위 표는 논리회로에서 많이 사용되는 OR GATE와 AND GATE의 진리표(truth table)를 나타낸 것이다. 그래프에서 확인할 수 있듯이 이들 두 GATE는 선형 분리가능(linearly separable)하다. 이 말은, 퍼셉트론을 학습시키면 AND GATE와 OR GATE 역할을 하게 할 수 있다는 것이다. 또 [이전 글](/SKKU_swe3050/07-simple-classification)에서도 퍼셉트론이 정말 잘 작동하지 않았던가. 이런 성공적인 결과를 보고 사람들은 퍼셉트론에 많은 기대를 가졌다.

하지만 다음 데이터를 보자.

<div class="table-wrapper" markdown="block">

| $x_1$ | $x_2$ | XOR($x_1$, $x_2$) |
| :---: | :---: | :---------------: |
|   0   |   0   |        -1         |
|   0   |   1   |         1         |
|   1   |   0   |         1         |
|   1   |   1   |        -1         |

</div>

{% include caption-img.html src="perceptron-xor.png" title="Fig.03 XOR GATE" description="XOR 연산 데이터는 선형적으로 분류가 불가능하다." %}

XOR GATE는 선형 분리가능하지 않다. 이 말은, 퍼셉트론은 이 간단한 XOR GATE마저도 학습할 수 없다는 뜻이다.[^2] 사실 우리가 풀고 싶은 대부분의 문제들은 선형 분리가능하지 않은 데이터를 사용하기에, 이 실패는 많은 사람들을 실망시켰다. 그 결과 전문가 시스템(Expert System)이 등장하기 전까지 한동안 인공지능 분야에 지원이 뚝 끊기게 되었다. 이 시기를 인공지능의 첫 번째 겨울이라 한다.

[^2]: 사실 단층 퍼셉트론(single-layer perceptron)이 아닌 다층 퍼셉트론(multi-layer perceptron)을 사용하면 선형 분리가능하지 않는 데이터들도 분류할 수 있다. 하지만 당시엔 이 사실이 알려지지 않았었다. 

# 퍼셉트론과 인공 신경(Artificial Neuron)과의 차이

인공 신경망(ANN, Artificial Neural Network)에서 사용되는 인공 신경과 퍼셉트론은 거의 유사하다. 일반적으로 퍼셉트론을 인공 신경의 전신이라고 본다.

굳이 한 가지 차이점을 꼽자면, 퍼셉트론은 -1 또는 1로 양자화된 출력만 내놓는 하드 리미터를 사용할 때, 인공 신경은 0에서 1 사이의 실수를 출력으로 내놓는 소프트 리미터[^3]를 주로 사용한다는 차이가 있다.

[^3]: ex. 시그모이드(sigmoid) 함수