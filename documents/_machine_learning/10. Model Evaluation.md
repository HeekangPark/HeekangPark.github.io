---
title: "모델 평가 (Model Evaluation)"
order: 10
date: "2020-05-26"
isWriting: true
output: false
---

다음과 같은 상황을 생각해 보자.

1. 어떤 지도학습 문제가 주어져, 적당한 모델을 선택한 후 주어진 데이터 셋을 이용하여 다양한 방법으로 모델을 학습시켰다. 이때, 모델이 얼마나 잘 학습되었는지는 어떻게 알 수 있을까?
2. 어떤 지도학습 문제를 풀 수 있는 다양한 모델들이 주어졌다고 하자. 어떤 모델이 가장 잘 분류하는지 어떻게 알 수 있을까?

위의 두 상황은 모두 **모델의 성능을 평가할 수 있는 방법이 필요**함을 보여주고 있다. 이번 글에서는 모델을 평가하는 방법에 대해 서술하겠다.

# 평가 지표

모델 평가를 위해서 고민해야 할 첫 번째 문제는, 무엇을 기준으로 평가해야 할 것인지이다. 즉, 평가의 기준이 무엇이냐는 것이다. 이는 문제 유형에 따라 달라지게 된다.

1. 분류(Classification) 문제 : 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1 점수(F1 Score)
2. 회귀(Regression) 문제 : 평균절대오차(MAE, Mean of Absolute Error), 평균제곱오차(Mean of Squared Error), 평균제곱근오차(Root of Mean of Squared Error)

## 분류 문제

### 분류결과표 (Confusion Matrix)

분류 문제의 성능 평가를 위해서는 우선 분류결과표를 구해야 한다.

정답[^1]이 `True`, `False`, 이렇게 두 개의 클래스로 구성된 이진 분류 문제(Binary Classification Problem)[^2]가 주어졌다고 해 보자. 이 문제를 풀기 위해 이진 분류기(Binary Classifier)[^3]을 만들었다. 이때 다음 4가지 경우를 생각할 수 있다.

[^1]: 레이블(Label), 참값(Ground Truth) 등으로도 불린다.
[^2]: 주어진 데이터를 바탕으로 두 개의 클래스(카테고리)로 분류하는 문제
[^3]: 분류 문제를 풀기 위한 모델을 분류기(Classifier)라 한다. 이진 분류 문제를 푸기 위한 모델을 이진 분류기(Binary Classifier)라 한다.

- True Positive (TP) : 정답이 `True`인 데이터를, `True`로 분류함. 즉, 잘 분류한 경우.
- False Positive (FP) : 정답이 `False`인 데이터를, `True`로 분류함. 즉, 잘못 분류한 경우.
- False Negative (FN) : 정답이 `True`인 데이터를, `False`로 분류함. 즉, 잘못 분류한 경우.
- True Negative (TN) : 정답이 `False`인 데이터를, `False`로 분류함. 즉, 잘 분류한 경우

(이름이 조금 헷갈릴 수 있는데, 잘 분류한 경우 "True", 잘못 분류한 경우 "False"가 붙고, 분류기가 `True`로 분류한 경우 "Positive", 분류기가 `False`로 분류한 경우 "Negative"가 붙는다.)

{% include caption-img.html src="confusion-matrix.png" title="Fig.01 이진 분류 문제 분류 결과" %}

모델이 테스트 데이터 셋의 데이터를 분류한 결과에 대해 True Positive의 개수, False Positive의 개수, False Negative의 개수, True Negative의 개수를 세어 표(행렬)로 만들 수 있는데, 이를 분류결과표(Confusion Matrix)라 한다.

예를 들어 10개의 테스트 데이터를 모델이 분류한 다음 결과로부터

<div class="table-wrapper" markdown="block">

|  idx  | 정답  | 분류값 |   분류 결과    |
| :---: | :---: | :----: | :------------: |
|   1   | True  | False  | False Negative |
|   2   | True  |  True  | True Positive  |
|   3   | True  | False  | False Negative |
|   4   | True  |  True  | True Positive  |
|   5   | False | False  | True Negative  |
|   6   | False |  True  | False Positive |
|   7   | True  |  True  | True Positive  |
|   8   | False | False  | True Negative  |
|   9   | True  | False  | False Negative |
|  10   | True  |  True  | True Positive  |

</div>

다음 2×2 분류결과표를 얻을 수 있다.

<div class="mathjax-wrapper" id="ex" markdown="block">

$$
\left[ \begin{array}\\
TP&FP\\
FN&TN\\
\end{array} \right]
=
\left[ \begin{array}\\
4&1\\
3&2\\
\end{array} \right]
 $$

</div>

만약 이진 분류 문제가 True, False가 아닌 다른 두 개의 클래스로 데이터를 분류해도 분류결과표를 만들 수 있다. 예를 들어 어떤 사진이 고양이(`Cat`)인지 강아지(`Dog`)인지를 분류하는 문제가 있다고 할 때, `Cat` 클래스를 `True` 클래스로, `Dog` 클래스를 `False` 클래스로 놓고 분류결과표를 만들면 된다.[^4] [^5]

[^4]: `Cat` 클래스를 '고양이가 맞는(True) 클래스', `Dog` 클래스를 '고양이가 아닌(False) 클래스'로 바꿨다고 이해할 수 있겠다.
[^5]: 물론 반대로 `Dog` 클래스를 `True`로, `Cat` 클래스를 `False`로 놓고 만들 수도 있다.

분류 문제의 평가 지표인 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1 점수(F1 Score)는 모두 분류결과표에서 시작한다.

### 정확도 (Accuracy)

정확도(Accuracy)란 모델이 맞게 분류한 데이터의 비율을 뜻한다.

<div class="mathjax-wrapper" markdown="block">

$$ Accuracy = \frac {TP + TN}{TP + FP + FN + TN} $$

</div>

수식에서 볼 수 있다시피 전체 경우의 수($TP + FP + FN + TN$)에 대해 맞게 분류한 경우의 수($TP + TN$)의 비로 계산할 수 있다. 위의 [예제](#ex)에서 정확도는 6/10 = 0.6이라 할 수 있다.

정확도는 가장 직관적이고 단순한 평가 지표지만, 정확도는 True Positive와 True Negative의 합으로 계산되기 때문에 `True` 클래스와 `False` 클래스의 빈도수에 차이가 크면 별로 신뢰할 만한 평가 지표가 될 수 없다. 예를 들어, 발병율이 매우 낮지만 한번 발병하면 아주 치명적인 질병 X가 있다고 하자. 한 회사가 질병 X가 발병했는지(`True`) 발병하지 않았는지(`False`) 진단할 수 있는 진단 키트 K를 개발했다고 한다. 근데 사실 K는 가짜 키트로서, 항상 미발병자라고(`False`) 진단한다. 회사는 이 키트를 임의로 선발된 10,000명에 대해서 성능 평가를 실시한다. 10,000명 중 9,999명이 미발병자이고, 오직 1명만이 발병자라 하면, 이 키트는 9,999명의 미발병자에 대해서는 미발병자라 맞게 분류할 테지만(True Negative), 1명의 발병자에 대해서는 미발병자라고 잘못 분류할(False Negative) 것이다. 이 경우, K의 정확도를 계산해 보면 9,999/10,000 = 99.99%로 나오게 된다!

