---
title: "모델 평가 (Model Evaluation)"
order: 12
date: "2020-05-26"
---

# 모델 평가의 필요성

다음과 같은 상황을 생각해 보자.

1. 어떤 지도학습 문제가 주어져, 적당한 모델을 선택한 후 주어진 데이터 셋을 이용하여 다양한 방법으로 모델을 학습시켰다. 이때, 모델이 얼마나 잘 학습되었는지는 어떻게 알 수 있을까?
2. 어떤 지도학습 문제를 풀 수 있는 다양한 모델들이 주어졌다고 하자. 어떤 모델이 가장 잘 분류하는지 어떻게 알 수 있을까?

위의 두 상황은 모두 **모델의 성능을 평가할 수 있는 방법이 필요**함을 보여주고 있다.

모델을 평가하면 다음과 같은 일을 할 수 있다.

- 모델의 추가적인 학습 방향을 설계할 수 있다 : 하이퍼파라미터 값을 어떻게 설정해야 할 지 등에 대한 정보를 얻을 수 있다.
- 모델이 임의의 상황에 얼마나 잘 대응할 지(일반화되어 있는지) 짐작할 수 있다.
- 어떤 모델이 더 좋은 모델인지 객관적인 비교가 가능해진다.

# 평가 지표

모델 평가를 위해서 고민해야 할 첫 번째 문제는, 무엇을 기준으로 평가해야 할 것인지이다. 즉, 평가 지표가 무엇이냐는 것이다. 이는 문제 유형에 따라 달라지게 된다.

- 분류(Classification) 문제 : 정확도(Accuracy), 에러율(Error Rate), 정밀도(Precision), 재현율(Recall), F1 점수(F1 Score) 등
- 회귀(Regression) 문제 : 평균절대오차(MAE, Mean of Absolute Error), 평균제곱오차(Mean of Squared Error), 평균제곱근오차(Root of Mean of Squared Error) 등

## 분류 문제

### 분류결과표 (Confusion Matrix)

분류 문제의 성능 평가를 위해서는 우선 분류결과표를 구해야 한다.

정답[^1]이 `True`, `False`, 이렇게 두 개의 클래스로 구성된 이진 분류 문제(Binary Classification Problem)[^2]가 주어졌다고 해 보자. 이 문제를 풀기 위해 이진 분류기(Binary Classifier)[^3]을 만들었다. 이때 다음 4가지 경우를 생각할 수 있다.

[^1]: 레이블(Label), 참값(Ground Truth) 등으로도 불린다.
[^2]: 주어진 데이터를 바탕으로 두 개의 클래스(카테고리)로 분류하는 문제
[^3]: 분류 문제를 풀기 위한 모델을 분류기(Classifier)라 한다. 이진 분류 문제를 푸기 위한 모델을 이진 분류기(Binary Classifier)라 한다.

- True Positive (TP) : 정답이 `True`인 데이터를, `True`로 분류함. 즉, 잘 분류한 경우.
- False Positive (FP) : 정답이 `False`인 데이터를, `True`로 분류함. 즉, 잘못 분류한 경우.
- False Negative (FN) : 정답이 `True`인 데이터를, `False`로 분류함. 즉, 잘못 분류한 경우.
- True Negative (TN) : 정답이 `False`인 데이터를, `False`로 분류함. 즉, 잘 분류한 경우

(이름이 조금 헷갈릴 수 있는데, 잘 분류한 경우 "True", 잘못 분류한 경우 "False"가 붙고, 분류기가 `True`로 분류한 경우 "Positive", 분류기가 `False`로 분류한 경우 "Negative"가 붙는다.)

{% include caption-img.html src="confusion-matrix.png" title="Fig.01 이진 분류 문제 분류 결과" %}

모델이 테스트 데이터 셋의 데이터를 분류한 결과에 대해 True Positive의 개수, False Positive의 개수, False Negative의 개수, True Negative의 개수를 세어 표(행렬)로 만들 수 있는데, 이를 분류결과표(Confusion Matrix)라 한다.

예를 들어 10개의 테스트 데이터를 모델이 분류한 다음 결과로부터

<div class="table-wrapper" markdown="block">

|  idx  | 정답  | 분류값 |   분류 결과    |
| :---: | :---: | :----: | :------------: |
|   1   | True  | False  | False Negative |
|   2   | True  |  True  | True Positive  |
|   3   | True  | False  | False Negative |
|   4   | True  |  True  | True Positive  |
|   5   | False | False  | True Negative  |
|   6   | False |  True  | False Positive |
|   7   | True  |  True  | True Positive  |
|   8   | False | False  | True Negative  |
|   9   | True  | False  | False Negative |
|  10   | True  |  True  | True Positive  |

</div>

다음 2×2 분류결과표를 얻을 수 있다.

<div class="mathjax-wrapper" id="ex1" markdown="block">

$$
\left[ \begin{array}\\
TP&FP\\
FN&TN\\
\end{array} \right]
=
\left[ \begin{array}\\
4&1\\
3&2\\
\end{array} \right]
 $$

</div>

만약 이진 분류 문제가 True, False가 아닌 다른 두 개의 클래스로 데이터를 분류해도 분류결과표를 만들 수 있다. 예를 들어 어떤 사진이 고양이(`Cat`)인지 강아지(`Dog`)인지를 분류하는 문제가 있다고 할 때, `Cat` 클래스를 `True` 클래스로, `Dog` 클래스를 `False` 클래스로 놓고 분류결과표를 만들면 된다.[^4] [^5]

[^4]: `Cat` 클래스를 '고양이가 맞는(True) 클래스', `Dog` 클래스를 '고양이가 아닌(False) 클래스'로 바꿨다고 이해할 수 있겠다.
[^5]: 물론 반대로 `Dog` 클래스를 `True`로, `Cat` 클래스를 `False`로 놓고 만들 수도 있다.

### 정확도 (Accuracy)

정확도(Accuracy)란 모델이 맞게 분류한 데이터의 비율로, 높으면 높을수록 좋다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{Accuracy} = \frac {TP + TN}{TP + FP + FN + TN} $$

</div>

수식에서 볼 수 있다시피 전체 경우의 수($TP + FP + FN + TN$)에 대해 맞게 분류한 경우의 수($TP + TN$)의 비로 계산할 수 있다. 위의 [예제](#ex1)에서 정확도는 6/10 = 0.6이다.

정확도는 가장 직관적이고 단순한 평가 지표지만, True Positive와 True Negative 비율의 합으로 계산되기 때문에 `True` 클래스와 `False` 클래스의 빈도수에 차이가 크면[^6] 별로 신뢰할 만한 평가 지표가 될 수 없다. 다음 예시를 살펴보자.

> 발병율이 매우 낮지만 한번 발병하면 아주 치명적인 질병 X가 있다고 하자. 회사 A가 질병 X가 발병했는지(`True`) 발병하지 않았는지(`False`) 진단할 수 있는 진단 키트 K를 개발했다고 한다. 근데 사실 K는 가짜 키트로서, 정말 X를 진단하는 것이 아니고 항상 무조건 미발병자라고(`False`)만 진단하는 키트다. 보건 당국은 이 키트를 임의로 선발된 10,000명에 대해서 성능 평가를 실시한다. 10,000명 중 9,990명이 미발병자이고, 오직 10명만이 발병자라 하면, 이 키트는 9,990명의 미발병자에 대해서는 미발병자라 맞게 분류할 테지만(True Negative), 10명의 발병자에 대해서는 미발병자라고 잘못 분류할(False Negative) 것이다.
{: #ex2}

이 경우, K의 정확도를 계산해 보면 9,990/10,000 = 99.90%로 나오게 된다. 명백히 잘못된 모델임에도 정확도가 매우 높게 계산된 것을 볼 수 있다.


[^6]: 이를 (정의역(Domain)에서) 데이터가 편중(bias)되어 있다고 표현한다.

### 에러율 (Error Rate)

에러율(Error Rate)은 모델이 잘못 분류한 데이터의 비율로, 낮으면 낮을수록 좋다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{Error Rate} = 1 - \textrm{Accuracy} = \frac {FP + FN}{TP + FP + FN + TN} $$

</div>

수식에서 볼 수 있다시피 전체 경우의 수($TP + FP + FN + TN$)에 대해 잘못 분류한 경우의 수($FP + FN$)의 비로 계산할 수 있다. 위의 [예제](#ex1)에서 에러율은 4/10 = 0.4이다.

에러율도 정확도와 비슷한 문제가 발생한다. 정확도의 [예시](#ex2)에서, 진단 키트 K의 에러율은 고작 0.10%밖에 안된다.

### 정밀도 (Precision)

정밀도는 해당 클래스로 분류된 것 중 정답인 것의 비율로, 높으면 높을수록 좋다. 정밀도는 `True` 정밀도(True Precision)[^7]와 `False` 정밀도(False Precision)[^8]으로 구분할 수 있는데, 일반적으로 정밀도라 하면 `True` 정밀도를 의미한다. 각각의 식은 다음과 같다.

[^7]: Positive Predictive Value(PPV)라고도 불린다.
[^8]: Negative Predictive Value(NPV)라고도 불린다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{True Precision} = \frac {TP}{TP + FP} $$

</div>

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{False Precision} = \frac {TN}{TN + FN} $$

</div>

수식에서 볼 수 있다시피 한 클래스로 분류된 경우의 수($TP + FP$, $TN + FN$)에 대해, 맞게 분류한 경우의 수($TP$, $TN$)의 비로 계산할 수 있다. 위의 [예제](#ex1)에서 `True` 정밀도는 4/5 = 0.8, `False` 정밀도는 2/5 = 0.4이다.

정밀도를 이용하면 정확도에서의 [예시](#ex2) 상황이 가지고 있던 문제를 해결할 수 있다. 진단 키트 K의 `False` 정밀도는 0이기 때문이다.

하지만 정밀도에도 한계가 있다. 우리는 정밀도가 높은 모델을 보고 True Positive 빈도수가 클 것이라 기대하는데, False Positive의 빈도를 낮추는 것만으로도 정밀도가 올라갈 수 있다. 진단 키트 [예시](#ex2) 상황을 조금 더 살펴보자.

> 회사 B도 X를 진단할 수 있는 진단 키트 L을 개발했다고 한다. L은 정말로 보수적인 진단 키트여서, 증상이 심각한 중증 X 발병자만 발병자로 진단하고, 증상이 약한 경증 X 발병자는 가차 없이 미발병자로 진단하는, 역시 문제가 있는 키트이다. 보건 당국은 아까 그 10,000명(미발병자 9,990명, 발병자 10명)에 대해서 L의 성능 평가를 실시한다. L은 9,990명의 미발병자를 모두 미발병자라 분류하고(True Negative), 10명의 발병자 중 중증 환자 1명만을 발병자로 분류하고(True Positive), 나머지 9명은 미발병자라 분류한다(False Negative).
{: #ex3}

이 경우 L의 `True` 정밀도를 계산해 보면 1/(1 + 0) = 100%가 나오고, `False` 정밀도를 계산해 보면 9,990/(9,990 + 9) = 99.91%가 나온다. 명백히 잘못된 모델임에도 정밀도가 매우 높게 계산된 것을 볼 수 있다.

### 재현율 (Recall)

재현율은 한 클래스의 원소들 중 그 클래스로 분류된 것의 비율로, 높으면 높을수록 좋다. 정밀도와 마찬가지로 재현율 역시 `True` 재현율(True Recall)[^9]과 `False` 재현율(False Recall)[^10]이 존재한다. 일반적으로 재현율이라 하면 `True` 재현율을 의미한다. 재현율을 수식으로 표현하면 다음과 같다.

[^9]: 민감도(Sensitivity), Hit Rate, True Positive Rate(TPR) 등으로도 불린다.
[^10]: 특이도(Specificity), 선택도(Selectivity), True Negative Rate(TNR) 등으로도 불린다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{True Recall} = \frac {TP}{TP + FN} $$

</div>

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{False Recall} = \frac {TN}{TN + FP} $$

</div>

수식에서 볼 수 있다시피 진짜 `True`인 경우의 수($TP + FN$)에 대해, 맞게 분류한 경우의 수($TP$)의 비, 또는 진짜 `False`인 경우의 수($TN + FP$)에 대해, 맞게 분류한 경우의 수($TN$)의 비로 계산할 수 있다. 위의 [예제](#ex1)에서 `True` 재현율은 4/7 = 0.57, `False` 재현율은 2/3 = 0.67이다.

재현율을 이용하면 정밀도에서의 [예제](#ex3) 상황이 가지고 있는 문제를 해결할 수 있다. 진단 키트 L의 `True` 재현율은 1/(1 + 9) = 10% 밖에 되지 않기 때문이다.

하지만 (당연하게도) 재현율에도 한계가 있다. 우리는 재현율이 높은 모델을 보고 True Positive 빈도수가 클 것이라 기대하는데, False Negative 빈도를 낮추는 것만으로도 재현율이 올라갈 수 있다. 진단 키트 [예시](#ex3) 상황을 좀 더 살펴보자.

> 회사 C도 X를 진단할 수 있는 진단 키트 M를 개발했다고 한다. M은 정말로 진보적인 진단 키트로서, 가벼운 감기 기운만 있어도 X가 발병했다고 진단하는, 역시 문제가 있는 키트이다. ~~제대로 된 회사가 없다.~~ 보건 당국은 다시 10,000명(미발병자 9,990명, 발병자 10명)에 대해서 M의 성능 평가를 실시한다. M은 발병자 10명에 대해서 전원 발병자로 분류하고(True Positive), 미발병자 9,990명 중 감기 기운이 있던 100명도 발병자라 분류하고(False Positive), 나머지 9,890명을 미발병자라 분류한다(True Negative).
{: #ex4}

이 경우 M의 `True` 재현율은 10/(10 + 0) = 100%, `False` 재현율은 9,890/(9,890 + 100) = 99.00%로 계산된다. 잘못된 모델이 높은 재현율을 갖는 상황이 발생한 것을 볼 수 있다.

### F1 점수 (F1 Score)

재현율에서의 [예제](#ex4)에서 M의 `True` 정밀도는 10/(10 + 100) = 9.09%로 매우 낮게 계산된다. 일반적으로 재현율이 높으면 정밀도가 낮고, 정밀도가 높으면 재현율이 낮은 관계가 성립한다. 이 사실을 바탕으로 고안된 평가 지표가 F1 점수(F1 Score)[^11]이다. F1 점수는 재현율과 정밀도의 조화 평균으로 정의된다. 수식으로 나타내면 다음과 같다.

[^11]: F-Score라고도 불린다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{F1 Score} = \frac {2}{\frac{1}{\textrm{Precision}} + \frac{1}{\textrm{Recall}}} = \frac{2 \cdot \textrm{Precision} \cdot \textrm{Recall}}{\textrm{Precision} + \textrm{Recall}} $$

</div>

위의 [예제](#ex1)에서 정밀도는 4/5 = 0.8, 재현율은 4/7 = 0.57이므로 F1 점수는 40/48 = 0.83이 된다.

F1 점수에 대해 몇 가지 사족을 달아 보면...

- F1 점수가 높은 모델은 정밀도와 재현율이 둘 다 높다고 말할 수 있다. 하지만 상술했듯이 정밀도와 재현율은 trade-off 관계이기에, 이 둘을 동시에 올리는 것은 쉽지 않다.
- F1 점수는 왜 산술 평균도, 기하 평균도 아닌 조화 평균을 사용할까? 조화 평균으로 계산한 F1 점수는 정밀도와 재현율의 중간에서, 둘 중 작은 값에 조금 더 치우친 값이 된다.  정밀도와 재현율은 극단적인 모델에 취약하다. F1 점수는 이런 극단적인 모델에서도 성능 평가를 잘 하기 위해 고안된 평가 지표이다. 극단적인 모델이어서 정밀도와 재현율 둘 중 어느 하나가 다른 하나에 비해 극단적으로 높아도, 둘 중 더 작은 값에 치우친 F1 점수는 크게 영향을 받지 않게 된다.
- F1 점수는 다음과 같이 계산되는 F-Measure이라고 하는 지표의 특수한 형태($\beta=1$)이다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{F-Measure} = \frac {1}{\alpha \cdot \textrm{Precision}^{-1} + (1-\alpha) \cdot \textrm{Recall}^{-1}} = \frac{(\beta^2 + 1) \cdot \textrm{Precision} \cdot \textrm{Recall}}{\beta^2 \cdot \textrm{Precision} + \textrm{Recall}} \qquad
\textrm{where} \quad \beta^2 = \frac{1 - \alpha}{\alpha}
$$

</div>

### 다중 분류(Multi-class Classification) 문제로의 확장

이진 분류 문제에서 이진 분류기의 성능을 평가하는 방법을 확장하면, 다중 분류 문제에서 다중 분류기(Multi-class Classifier)의 성능을 평가할 수 있다.

주어진 입력 데이터를 사과, 배, 포도 클래스로 분류하는 다중 분류 문제를 생각하자. 이 문제를 해결하기 위해 만들어진 다중 분류기의 분류결과표가 다음과 같다고 해 보자.

{% include caption-img.html src="multi-class-classification-confusion-matrix.png" title="Fig.02 다중 분류 분류결과표" %}

이때 정확도는 다음과 같이 맞게 분류한 경우(대각선)의 수를 전체 경우의 수로 나눠주면 된다. 

- 정확도 = (6 + 5 + 10) / 54 = 0.39

에러율은 1에서 정확도를 빼주면 된다.

- 에러율 = 1 - 정확도 = 0.61

정밀도는 마이크로 정밀도(Micro Precision)와 매크로 정밀도(Macro Precision)으로 나누어 생각할 수 있다. 마이크로 정밀도는 클래스 각각에 대한 정밀도로, 이진 분류 문제에서 사용했던 정밀도를 그대로 확장한 개념이다. 모델이 해당 클래스로 분류한 전체 경우의 수에 대해, 정말 해당 클래스인 것들의 경우의 수의 비로 계산할 수 있다.

{% include caption-img.html src="multi-class-classification-confusion-matrix-micro-precision.png" title="Fig.03 '사과' 마이크로 정밀도" %}

- "사과" 마이크로 정밀도 = 6 / (6 + 3 + 9) = 0.33
- "배" 마이크로 정밀도 = 5 / (2 + 5 + 7) = 0.36
- "포도" 마이크로 정밀도 = 10 / (8 + 4 + 10) = 0.45

매크로 정밀도는 마이크로 정밀도들의 평균이다.

- 매크로 정밀도 = (0.33 + 0.36 + 0.45) / 3 = 0.38

마찬가지로 재현율도 마이크로 재현율(Micro Recall)과 매크로 재현율(Macro Recall)로 나누어 생각할 수 있다. 마이크로 재현율은 클래스 각각에 대한 재현율로, 이진 분류 문제에서 사용했던 재현율을 그대로 확장한 개념이다. 해당 클래스 전체 경우의 수에 대해, 모델이 해당 클래스로 맞게 분류한 경우의 수의 비로 계산할 수 있다.

{% include caption-img.html src="multi-class-classification-confusion-matrix-micro-recall.png" title="Fig.03 '사과' 마이크로 재현율" %}

- "사과" 마이크로 재현율 = 6 / (6 + 2 + 8) = 0.38
- "배" 마이크로 재현율 = 5 / (3 + 5 + 4) = 0.42
- "포도" 마이크로 재현율 = 10 / (9 + 7 + 10) = 0.38

매크로 재현율은 마이크로 재현율들의 평균이다.

- 매크로 재현율 = (0.38 + 0.42 + 0.38) / 3 = 0.39

## 회귀 문제

### 평균절대오차 (MAE, Mean of Absolute Error), 평균제곱오차 (MSE, Mean of Squared Error)

평균절대오차와 평균제곱오차는 [이전 글](/machine_learning/04-error-functions)에서 소개한 바 있다. 여기서는 간단히 수식만 소개하겠다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - f(\boldsymbol{x_i})|$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\boldsymbol{x_i}))^2$$

</div>

파라미터 최적화를 할 때와 마찬가지로 모델을 평가할 때도 평균제곱오차가 평균절대오차보다 더 많이 사용되는데, 그 이유는 오차가 작은 좋은 모델과 오차가 큰 안좋은 모델의 차이를 평균제곱오차가 평균절대오차보다 더 극적으로 보여주기 때문에 그렇다.

### 평균제곱근오차 (RMSE, Root of Mean of Squared Error)

상술한 대로 평균제곱오차가 평균절대오차보다 일반적으로 더 좋은 것으로 평가받는데, 평균제곱오차는 (출력) 데이터와 차원이 다르다는 결정적인 단점이 있다. 이를 보완하기 위해 만들어진 평가 지표가 평균제곱근오차(RMSE, Root of Mean of Squared Error)이다. 평균제곱근오차는 평균제곱오차에 근호를 씌워 단위를 맞춰준 것이다. 수식으로 다음과 같이 나타낼 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - f(\boldsymbol{x_i}))^2}$$

</div>

평균제곱근오차는 평균제곱오차의 장점(큰 오차를 극명하게 강조한다)과 평균절대오차의 장점(출력 데이터와 차원이 같다)을 모두 가지고 있다고 평가되며, 셋 중 가장 많이 쓰이는 평가 지표이다.

# 평가 데이터

모델 평가를 위한 평가 지표가 결정되었으면, 이제 두 번째 질문에 대해 고민해야 한다. 그렇다면 평가는 어떠한 데이터를 바탕으로 진행해야 할까?

## 홀드아웃 방법 (Hold-out Method)

홀드아웃 방법은 데이터를 학습 데이터 셋(training dataset)과 테스트 데이터 셋(test dataset)으로 나눈 후, 학습 데이터 셋은 모델을 학습할 때 사용하고, 테스트 데이터 셋은 학습된 모델의 평가를 위해 사용하는 방법이다. 통상적으로 테스트 데이터 셋의 크기는 전체 데이터 셋의 30% ~ 50% 정도가 되게 나눈다.

홀드아웃 방법이 성공적으로 적용되기 위해서는 첫째로, 학습 과정에서는 테스트 데이터를 절대 참조하면 안된다. 테스트 데이터로 모델을 학습시키면 모델이 테스트 데이터에 과적합(overfitting)될 수 있다. 우리가 모델을 평가하는 주된 목적 중 하나가 임의의 경우에도 잘 작동하는지를 확인하고 싶어서인데, 과적합된 데이터를 바탕으로 테스트를 진행하면 모델 평가의 공정성이 훼손된다. 이를 위해 학습 데이터 셋과 테스트 데이터 셋을 나눌 때 겹치는 데이터가 없도록 나눈다.

둘째로, 학습 데이터와 테스트 데이터의 분포(distribution)가 같아야 한다. 즉, 원본 데이터의 분포, 학습 데이터의 분포, 테스트 데이터의 분포가 모두 동일해야 한다.[^12] 50장의 강아지 사진과 50장의 고양이 사진이 주어지고, 이들을 `Dog` 클래스와 `Cat` 클래스로 분류하는 문제를 생각해 보자. 이때 원본 데이터의 분포는 (`Dog`, `Cat`) = 1:1이다. 이때 만약 강아지 사진 10장과 고양이 사진 40장으로 학습 데이터 셋을, 나머지 강아지 사진 40장과 고양이 사진 10장으로 테스트 데이터 셋을 구축한다면, 학습 데이터 셋의 분포는 (`Dog`, `Cat`) = 1:4, 테스트 데이터 셋의 분포는 (`Dog`, `Cat`) = 4:1이 된다. 이렇게 되면 학습 과정에서 과적합이 발생할 가능성이 높아지고, 테스트 데이터 셋의 공정성이 훼손될 가능성이 커진다. 이를 위해 학습 데이터 셋과 테스트 데이터 셋을 나눌 때 랜덤하게 나누는 방법을 주로 사용한다.

[^12]: 그리고 물론, 원본 데이터의 분포는 실제 입력 공간(input space)의 분포와 동일해야 한다. (그렇지 않다면 별로 좋은 데이터가 아닌 것이다.)

홀드아웃 방법은 직관적이고 단순하다는 장점이 있지만, 다음과 같은 단점이 있다.

- 데이터가 부족할 때는 별로 매력적인 방법이 아니다. 학습을 위한 데이터도 부족한데 평가를 위해서 테스트 데이터 셋을 분리해야 하기 때문이다.
- 학습 데이터와 테스트 데이터의 분포를 같게 만드는게 쉬운 일이 아니다. 홀드아웃 방법의 성능은 학습 데이터와 테스트 데이터가 얼마나 잘 분리되었는지에 따라 크게 차이난다.

## k-폴드 교차검증법 (k-fold Cross Validation)

홀드아웃 방법이 가지고 있는 단점을 보완할 수 있는 기술이 k-폴드 교차검증법(k-fold Cross Validation)[^13]이다. k-폴드 교차검증은 다음과 같은 방식으로 작동한다.

[^13]: 그냥 교차검증법(Cross Validation)이라고도 불린다.

1. 데이터를 $k$개의 비슷한 크기의 상호 배제적인(mutually exclusive)[^14] 집합(fold)으로 나눈다: $D_1$, $D_2$, …, $D_k$
2. 다음을 $i = 1, 2, \cdots, k$까지 총 $k$번 반복한다.
   1. $D_i$를 제외한 나머지 집합들로 모델을 학습한다.
   2. $D_i$로 모델을 검증(validate)한다.

[^14]: 겹치는 값 없이, 교집합 없이

k-폴드 교차검증법에서 모든 데이터들은 테스트 데이터 셋에 한 번 포함되고, 학습 데이터 셋에 (k - 1)번 포함된다.

k-폴드 교차검증법은 데이터가 부족할 때도 무리 없이 사용할 수 있다는 장점이 있다. 또한 데이터가 나뉘어지는 방식에 크게 영향을 받지 않는다는 장점도 있다. 하지만 시간이 오래 걸린다는 단점이 있다.

전체 데이터의 개수만큼 집합을 만들어 k-폴드 교차검증법을 실시할 수도 있는데, 이를 특별히 LOOCV(Leave-one-out Cross Validation)라 부른다. LOOCV에서는 1개의 데이터를 제외한 모든 데이터들이 학습에 참가하게 된다. 데이터의 수가 정말 적을 때 고려할 만한 방법이다.

## 결론 : 학습 데이터 셋, 검증 데이터 셋, 테스트 데이터 셋 (Training Dataset, Validation Dataset, Test Dataset)

보통 기계학습에서 모델을 학습시키기 전 가장 먼저 하는 일이 데이터를 분리하는 것이다.[^15] 주어진 전체 데이터 셋을 학습 데이터 셋(Training Dataset), 검증 데이터 셋(Validation Dataset), 테스트 데이터 셋(Test Dataset) 이렇게 세 집합으로 나누게 된다. 학습 데이터 셋은 말 그대로 모델을 학습시키기 위해 사용하는 데이터 셋이다. 검증 데이터 셋과 테스트 데이터 셋은 모델을 평가하기 위해 사용하는 데이터 셋이다. 둘의 차이는 다음과 같다.

[^15]: 혹은 처음부터 분리된 데이터 셋을 가지고 작업하게 된다.

- 검증 데이터 셋은 모의시험용 데이터셋이다. 현재 학습이 올바른 방향으로 되어가고 있는지 검증하는 목적으로 사용된다. 검증 데이터 셋에 대한 결과를 바탕으로 모델의 하이퍼파라미터를 조정한다.
- 테스트 데이터 셋은 실전시험용 데이터셋이다. 테스트 데이터는 모델의 학습이 종료된 이후, 최종적인 모델의 성능을 평가하기 위해 사용한다.

가장 이상적인 경우에는 데이터 수가 많아 전체 데이터 셋을 ~~화끈하게~~ 학습 데이터 셋, 검증 데이터 셋, 테스트 데이터 셋 세 덩이로 나누는 것이다. 하지만 만약 데이터 수가 부족하다면, 실전시험용 테스트 셋을 분리한 후, 나머지 데이터들에 대해 k-폴드 교차검증법 등을 적용하는 방법을 생각해 볼 수 있겠다.



