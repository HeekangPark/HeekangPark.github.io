---
title: "지도학습 예제 : 단순분류(Simple Classification)"
order: 5
date: "2020-05-08"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순분류(Simple Classification) 문제를 풀어보자.

# 문제 상황

다음과 같이 입력 $\boldsymbol{x}(x_1, x_2)$에 대한 출력(레이블) $y$의 데이터가 주어졌다고 해 보자.

| $x_1$ | $x_2$ |  $y$  |       | $x_1$ | $x_2$ |  $y$  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| -2.58 | 2.93  |   1   |       | 1.31  | -5.90 |  -1   |
| -2.87 | 8.99  |   1   |       | 4.72  | -9.44 |  -1   |
| -8.65 | 8.96  |   1   |       | -1.63 | -9.10 |  -1   |
| -7.35 | -3.05 |   1   |       | 9.06  | -6.63 |  -1   |
| -2.88 | 7.55  |   1   |       | 1.16  | -7.63 |  -1   |
| -9.49 | 2.22  |   1   |       | 7.62  | -7.60 |  -1   |
| -7.49 | 7.62  |   1   |       | 4.90  | 8.06  |  -1   |
| -2.88 | -3.11 |   1   |       | 2.76  | 0.79  |  -1   |
| -7.65 | 9.42  |   1   |       | 9.37  | 0.39  |  -1   |
| -0.59 | 7.96  |   1   |       | 9.02  | -0.76 |  -1   |
| -4.54 | -6.58 |   1   |       | 7.46  | 6.30  |  -1   |
| -7.22 | -1.24 |   1   |       | 5.39  | 7.48  |  -1   |
| 0.41  | 6.26  |   1   |       | -0.40 | -9.40 |  -1   |
| -4.50 | -5.37 |   1   |       | 5.73  | -1.66 |  -1   |
| -8.93 | -0.80 |   1   |       | 7.09  | 1.56  |  -1   |
| -3.33 | 9.85  |   1   |       | 6.76  | 7.00  |  -1   |
| -4.79 | 9.76  |   1   |       | 5.89  | 3.02  |  -1   |
| -0.88 | 6.14  |   1   |       | 8.40  | 5.63  |  -1   |
| -7.59 | 9.02  |   1   |       | 0.58  | -0.11 |  -1   |
| -8.49 | -9.60 |   1   |       | 7.69  | -6.41 |  -1   |

{% include caption-img.html src="simple-classification-data.png" title="Fig.01 데이터" description="빨간 점은 $y=1$, 파란 점은 $y=-1$인 데이터이다." %}

# 데이터 분석

이 데이터를 바탕으로, 새로운 입력 $\boldsymbol{x}_{new}$가 주어졌을 때, 출력 $y$값이 -1일지(파란 점) 1일지(빨간 점) 추정할 수 있는 모델을 설계해보자.

산점도를 보게 되면 왼쪽 위가 빨간 점($y=1$)이고, 오른쪽 아래가 파란 점($y=-1$)이다. 개략적으로 적당한 선형함수가 있어, ($x_1$, $x_2$)가 선형함수의 그래프보다 위에 있으면($x_2 > kx_1$이면) $y=1$, ($x_1$, $x_2$)가 선형함수의 그래프보다 아레에 있으면($x_2 < kx_1$이면) $y=-1$인 형태임을 확인할 수 있다.

# 모델 설계

이에 다음과 같은 모델을 생각할 수 있다.

$$f(\boldsymbol{x}) = \mathrm{sign}(\boldsymbol{w}^\intercal \boldsymbol{x})$$

($\mathrm{sign}$함수는 입력값의 부호를 반환하는 함수이다. 즉 입력값이 양수면 +1을, 음수면 -1을 출력한다.)

참고로 이 모델은 [퍼셉트론(Perceptron)]({{ site.url}}{{ site.baseurl }}/machine_learning/06-perceptron/)의 일종이다.

# 파라미터 최적화

이제 이렇게 만든 모델을 경사 하강법을 이용해 학습시켜 보자. 

## 오차 함수의 정의 : 0-1 손실 함수 (0-1 Loss Function)

경사 하강법을 사용하기 위해서는 우선 오차 함수를 정의해야 한다. 이 문제에서 사용할 오차 함수는 "0-1 손실 함수(0-1 Loss Function)"이라 불리는 함수이다. 주어진 데이터 셋 $D$에 대해 0-1 손실 함수 $J(\boldsymbol{w})$는 다음과 같이 정의된다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, D } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i) \cdot M(\boldsymbol{w}, \boldsymbol{x}_i, y_i)$$

이때 $M(\boldsymbol{w}, \boldsymbol{x}, y)$는 다음과 같이 정의되는 함수이다.

$$M(\boldsymbol{w}, \boldsymbol{x}, y) = \begin{cases} 1 \qquad \text{if } y \neq f(\boldsymbol{x}) \\ 0 \qquad \text{if } y = f(\boldsymbol{x})\end{cases}$$

$M(\boldsymbol{w}, \boldsymbol{x}, y)$은 제대로 분류되지 않은 데이터($y \neq f(\boldsymbol{x})$)만 골라주는 함수이다. 이를 사용하여 $J(\boldsymbol{w})$에서는 제대로 분류되지 않은 데이터에 대해, $-y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$들의 합을 구하게 된다.

$J(\boldsymbol{w})$를 다음과 같이 바꿀 수도 있다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$$

$M(\boldsymbol{w}, \boldsymbol{x}, y)$을 사용하지 않은 대신, $(\boldsymbol{x}_i, y_i)$들의 집합을 "전체 데이터의 집합" $D$에서 "제대로 분류되지 않은 데이터들의 집합" $S$로 바꾸었다. 실제 계산할 때는 이 형태를 사용하여 계산한다.

$J(\boldsymbol{w})$에는 다음과 같은 특징이 있다.

- $J(\boldsymbol{w}) \ge 0$ : $y_i$와 $\boldsymbol{w}^\intercal \boldsymbol{x}_i$의 부호가 다를 때만 $M(\boldsymbol{w}, \boldsymbol{x}, y)$이 활성화(1)된다.
- 최적화되면, 즉 모든 데이터가 제대로 분류되었으면 $J(\boldsymbol{w}) = 0$
- 잘못 분류된 데이터가 많을수록 $J(\boldsymbol{w})$의 값이 커진다.

## 오차 함수의 그라디언트 구하기

이제 오차함수의 그라디언트를 구해 보자. $\boldsymbol{w}$의 $j$번째 원소 $w_j$에 대해,

$$\frac{\partial }{\partial w_j} J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } \frac{\partial}{\partial w_j} ( -y_i (w_0 x_{i0} + w_1 x_{i1}) ) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i w_{ij}$$

가 된다.

## 경사 하강법 적용하기

이제 경사 하강법을 적용해 보자.

우선 파라미터를 랜덤한 값으로 초기화한다. 이후 업데이트를 진행한다. 경사하강법에서 파라미터 업데이트는 $\boldsymbol{w}\_{new} = \boldsymbol{w}\_{old} - \eta \nabla J(\boldsymbol{w}_{old})$을 이용하여 진행된다. 이 식에 계산한 그라디언트를 넣고 학습률 $\eta = 1$로 설정하면 다음 업데이트 식을 얻을 수 있다.

$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} + \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } y_i w_{old \, ij}$$

이제 이 식을 가지고 더이상 잘못 분류된 데이터가 없을 때까지 파라미터를 업데이트 하면 된다.

근데 한 가지 더 짚어야 할 것이 있다. 이번 문제같이 데이터 수가 얼마 되지 않을 때는 모든 데이터를 다 살펴본 후, 제대로 분류되지 않은 데이터들의 집합 $S$를 구하는 것이 별로 어렵지 않다. 하지만 데이터 수가 너무 많다면 어떨까? $S$를 구하고, 이를 이용해 그라디언트를 계산하는 비용이 만만치 않을 것이다. 심지어 메모리에 다 올라가지도 않을 양의 데이터를 다루는 경우(기계학습에서는 테라바이트 급 데이터를 다룰 때도 있다), $S$를 메모리에 올리는 것 자체가 물리적으로 불가능할 수도 있다! 이 점을 고려하면, 우리는 다음 두 가지 모델을 생각할 수 있다.

1. 데이터를 살펴보면서 제대로 분류되지 않은 데이터들의 집합 $S$를 구한 후, 파라미터를 한 번 업데이트하기
2. 데이터를 살펴보면서 제대로 분류되지 않은 데이터가 나올 때마다 파라미터를 한 번 업데이트하기

첫 번째 방법을 배치 경사 하강법(BGD, Batch Gradient Descent Method)이라 하고, 두 번째 방법을 확률적 경사 하강법(SGD, Stochastic Gradient Descent Method)이라 한다.

### 배치 경사 하강법 (BGD, Batch Gradient Descent Method)

배치 경사 하강법에서 파라미터들은 아주 안정적으로 최적화된다(오차 함수가 안정적으로 최소화된다). 대신 앞서 말했던 것처럼 모든 데이터를 다 살펴보는 것이 불가능할 수도 있고, 무엇보다 속도가 많이 느리다.

배치 경사 하강법을 의사 코드(Pseudo Code)로 나타내면 다음과 같다.

{% highlight python linenos %}

initialize(w) # w 랜덤 값으로 초기화

do
S = []

# S 구하기
for data in dataset:
    input, label = data
    predict = model(input)

    if predict != label:
        S.append(data)

# 파리미터 업데이트하기
if S not empty:
    for wrong_data in S:
        x, y = wrong_data
        w = w + $\sum_{(x)}$
        $$\boldsymbol{w} = \boldsymbol{w}_{old} + \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } y_i w_{old \, ij}$$

{% endhighlight %}