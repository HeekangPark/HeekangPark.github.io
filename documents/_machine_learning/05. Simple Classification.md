---
title: "지도학습 예제 : 단순분류(Simple Classification)"
order: 5
date: "2020-05-08"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순분류(Simple Classification) 문제를 풀어보자.

# 문제 상황

다음과 같이 입력 $\boldsymbol{x}(x_1, x_2)$에 대한 출력(레이블) $y$의 데이터가 주어졌다고 해 보자.

| $x_1$ | $x_2$ |  $y$  |       | $x_1$ | $x_2$ |  $y$  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| -2.58 | 2.93  |   1   |       | 1.31  | -5.90 |  -1   |
| -2.87 | 8.99  |   1   |       | 4.72  | -9.44 |  -1   |
| -8.65 | 8.96  |   1   |       | -1.63 | -9.10 |  -1   |
| -7.35 | -3.05 |   1   |       | 9.06  | -6.63 |  -1   |
| -2.88 | 7.55  |   1   |       | 1.16  | -7.63 |  -1   |
| -9.49 | 2.22  |   1   |       | 7.62  | -7.60 |  -1   |
| -7.49 | 7.62  |   1   |       | 4.90  | 8.06  |  -1   |
| -2.88 | -3.11 |   1   |       | 2.76  | 0.79  |  -1   |
| -7.65 | 9.42  |   1   |       | 9.37  | 0.39  |  -1   |
| -0.59 | 7.96  |   1   |       | 9.02  | -0.76 |  -1   |
| -4.54 | -6.58 |   1   |       | 7.46  | 6.30  |  -1   |
| -7.22 | -1.24 |   1   |       | 5.39  | 7.48  |  -1   |
| 0.41  | 6.26  |   1   |       | -0.40 | -9.40 |  -1   |
| -4.50 | -5.37 |   1   |       | 5.73  | -1.66 |  -1   |
| -8.93 | -0.80 |   1   |       | 7.09  | 1.56  |  -1   |
| -3.33 | 9.85  |   1   |       | 6.76  | 7.00  |  -1   |
| -4.79 | 9.76  |   1   |       | 5.89  | 3.02  |  -1   |
| -0.88 | 6.14  |   1   |       | 8.40  | 5.63  |  -1   |
| -7.59 | 9.02  |   1   |       | 0.58  | -0.11 |  -1   |
| -8.49 | -9.60 |   1   |       | 7.69  | -6.41 |  -1   |

{% include caption-img.html src="simple-classification-data.png" title="Fig.01 데이터" description="빨간 점은 $y=1$, 파란 점은 $y=-1$인 데이터이다." %}

# 데이터 분석

이 데이터를 바탕으로, 새로운 입력 $\boldsymbol{x}_{new}$가 주어졌을 때, 출력 $y$값이 -1일지(파란 점) 1일지(빨간 점) 추정할 수 있는 모델을 설계해보자.

산점도를 보게 되면 왼쪽 위가 빨간 점($y=1$)이고, 오른쪽 아래가 파란 점($y=-1$)이다. 개략적으로 적당한 선형함수가 있어, ($x_1$, $x_2$)가 선형함수의 그래프보다 위에 있으면($x_2 > kx_1$이면) $y=1$, ($x_1$, $x_2$)가 선형함수의 그래프보다 아레에 있으면($x_2 < kx_1$이면) $y=-1$인 형태임을 확인할 수 있다.

# 모델 설계 : 퍼셉트론 (Perceptron)

이에 다음과 같은 모델을 생각하자.

$$f(\boldsymbol{x}) = \mathrm{sign}(\boldsymbol{w}^\intercal \boldsymbol{x})$$

($\mathrm{sign}$함수는 입력값의 부호를 반환하는 함수이다. 즉 입력값이 양수면 +1을, 음수면 -1을 출력한다.)

이와 같은 모델을 퍼셉트론(Perceptron)이라 한다.

## 퍼셉트론

잠시 퍼셉트론에 대해 조금 알아보고 가자. 퍼셉트론은 1957년 프랑크 로젠블라트(Frank Rosenblatt)라는 사람에 의해 최초로 고안되었다.

퍼셉트론은 신경 세포(뉴런, neuron)의 동작을 모방하여 만들어졌다. 신경 세포는 가지 돌기로부터 신호를 받아들여, 그 총합이 특정 값(역치, threshold) 이상일 경우 축삭 돌기 말단으로 신호를 전달한다. 퍼셉트론도 마찬가지로 외부에서 신호를 받아들인 후, 그 총합이 활성화 함수(activation function)을 활성화시키면 신호가 전송(1)되고, 활성화시키지 못하면 신호가 전송되지 않는(-1 또는 0) 식으로 작동한다.

우리가 방금 만든 모델로 설명하면, $\mathrm{sign}$ 함수가 활성화 함수이다. $\boldsymbol{w}^\intercal \boldsymbol{x}$의 값이 양수이면 $\mathrm{sign}$ 함수가 활성화(1)되고, 그 값이 음수이면 $\mathrm{sign}$ 함수는 비활성화(-1)된다.

퍼셉트론은 놀랄 만큼 잘 작동했다. 그 결과 사람들은 곧 있으면 사람처럼 행동할 수 있는 컴퓨터가 곧 개발될 것이라는 낙관적인 생각을 가졌고, 기계학습, 인공지능 분야에 많은 지원이 들어갔다. 이렇게 기계학습 분야에 봄이 찾아오나 했는데...

## XOR 문제

퍼셉트론은 선형적인 기준으로 분류 가능한 데이터에 대해서는 잘 작동하지만, 그렇지 않은 데이터에 대해서는 작동하지 않는다.

예를 들어, $x_1$, $x_2$ 값이 0(False), 1(True)일 때의 AND와 OR 연산을 살펴보자.

| $x_1$ | $x_2$ | OR($x_1$, $x_2$) |       | $x_1$ | $x_2$ | AND($x_1$, $x_2$) |
| :---: | :---: | :--------------: | :---: | :---: | :---: | :---------------: |
|   0   |   0   |        -1        |       |   0   |   0   |        -1         |
|   0   |   1   |        1         |       |   0   |   1   |        -1         |
|   1   |   0   |        1         |       |   1   |   0   |        -1         |
|   1   |   1   |        1         |       |   1   |   1   |         1         |


{% include caption-img.html src="perceptron-or-and.png" title="Fig.02 OR, AND 연산과 퍼셉트론" description="왼쪽 그래프는 OR 연산을, 오른쪽 그래프는 AND 연산을 나타낸다. 빨간 선은 이들 데이터를 분류하는 퍼셉트론을 나타낸다." %}

이렇게 선형적으로 분류가 가능한 데이터에 대해서는 (아래 후술할) 퍼셉트론 학습 알고리즘(PLA, Perceptron Learning Algorithm)을 사용하면 퍼셉트론 학습이 가능했다. 즉 AND와 OR을 컴퓨터에게 "가르칠" 수 있었던 것이다.

하지만 선형적인 기준으로 분류 불가능한 데이터들도 있다. 대표적으로 XOR 연산이 있다.

| $x_1$ | $x_2$ | XOR($x_1$, $x_2$) |
| :---: | :---: | :---------------: |
|   0   |   0   |        -1         |
|   0   |   1   |         1         |
|   1   |   0   |         1         |
|   1   |   1   |        -1         |

{% include caption-img.html src="perceptron-xor.png" title="Fig.03 XOR 연산" description="XOR 연산 데이터는 선형적으로 분류가 불가능하다." %}

XOR 연산은 선형적으로 분류가 불가능하다. XOR은 비교적 간단한 문제인데, 이것을 컴퓨터에게 가르칠 수 없다는 사실은 많은 사람들을 실망하게 했다. 그 결과 기계학습과 인공지능 분야에 대한 지원이 뚝 끊겼다. 이렇게 인공지능의 첫 번째 겨울이 오게 된다.


# 파라미터 최적화 : 퍼셉트론 학습 알고리즘 (PLA, Perceptron Learning Algorithm)

이제 이렇게 만든 퍼셉트론 모델을 학습시켜 보자. 퍼셉트론은 퍼셉트론 학습 알고리즘(PLA, Perceptron Learning Algorithm)을 통해 학습시킬 수 있다. 퍼셉트론 학습 알고리즘이란 경사 하강법을 이용하여 퍼셉트론 모델의 파라미터 최적화를 수행하는 알고리즘이다.

## 오차 함수의 정의 : 0-1 손실 함수 (0-1 Loss Function)

경사 하강법을 사용하기 위해서는 우선 오차 함수를 정의해야 한다. 이 문제에서 사용할 오차 함수는 "0-1 손실 함수(0-1 Loss Function)"이라 불리는 함수이다. 주어진 데이터 셋 $D$에 대해 0-1 손실 함수 $J(\boldsymbol{w})$는 다음과 같이 정의된다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, D } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i) \cdot M(\boldsymbol{w}, \boldsymbol{x}_i, y_i)$$

이때 $M(\boldsymbol{w}, \boldsymbol{x}, y)$는 다음과 같이 정의되는 함수이다.

$$M(\boldsymbol{w}, \boldsymbol{x}, y) = \begin{cases} 1 \qquad \text{if } y \neq f(\boldsymbol{x}) \\ 0 \qquad \text{if } y = f(\boldsymbol{x})\end{cases}$$

$M(\boldsymbol{w}, \boldsymbol{x}, y)$은 제대로 분류되지 않은 데이터($y \neq f(\boldsymbol{x})$)만 골라주는 함수이다. 이를 사용하여 $J(\boldsymbol{w})$에서는 제대로 분류되지 않은 데이터에 대해, $-y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$들의 합을 구하게 된다.

$J(\boldsymbol{w})$에는 다음과 같은 특징이 있다.

- $J(\boldsymbol{w}) \ge 0$ : $y_i$와 $\boldsymbol{w}^\intercal \boldsymbol{x}_i$의 부호가 다를 때만 $M(\boldsymbol{w}, \boldsymbol{x}, y)$이 활성화(1)된다.
- 최적화되면, 즉 모든 데이터가 제대로 분류되었으면 $J(\boldsymbol{w}) = 0$
- 잘못 분류된 데이터가 많을수록 $J(\boldsymbol{w})$의 값이 커진다.

## 오차 함수의 그라디언트 구하기

그라디언트를 구하기 위해서는 $J(\boldsymbol{w})$의 형태를 바꿔 줄 필요가 있다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$$

보이다시피 $M(\boldsymbol{w}, \boldsymbol{x}, y)$을 사용하지 않은 대신, $(\boldsymbol{x}_i, y_i)$들의 집합을 전체 데이터의 집합 $D$에서 제대로 분류되지 않은 데이터들의 집합 $S$로 바꾸었다.

이제 이 함수에 대해서 그라디언트를 구해 보자. $\boldsymbol{w}$의 $j$번째 원소 $w_j$에 대해,

$$\frac{\partial }{\partial w_j} J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } \frac{\partial}{\partial w_j} ( -y_i (w_0 x_{i0} + w_1 x_{i1}) ) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i w_{ij}$$

가 된다.

## 퍼셉트론 학습 알고리즘 (PLA, Perceptron Learning Algorithm)

이제 퍼셉트론 학습 알고리즘을 적용시키자. 퍼셉트론 학습 알고리즘은 다음과 같은 순서로 작동한다.

1. 모델의 파라미터 $\boldsymbol{w}$를 무작위 값으로 초기화한다.
2. 현재의 $\boldsymbol{w}$에 대해 $J(\boldsymbol{w})$를 계산한다. 즉, 현재 모델이 데이터를 얼마나 잘못 분류하는지를 오차를 계산한다.
3. 경사 하강법을 이용하여 파라미터를 업데이트한다.
4. 2~3 과정을 잘못 분류된 데이터가 없어질 때까지(= $J(\boldsymbol{w})$가 0이 될 때까지 = 최적화 될 때까지) 수행한다.



