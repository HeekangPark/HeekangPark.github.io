---
title: "지도학습 예제 : 단순분류(Simple Classification)"
order: 5
date: "2020-05-08"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순분류(Simple Classification) 문제를 풀어보자.

# 문제 상황

다음과 같이 입력 $\boldsymbol{x}(x_1, x_2)$에 대한 출력(레이블) $y$의 데이터가 주어졌다고 해 보자.

| $x_1$ | $x_2$ |  $y$  |       | $x_1$ | $x_2$ |  $y$  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| -2.58 | 2.93  |   1   |       | 1.31  | -5.90 |  -1   |
| -2.87 | 8.99  |   1   |       | 4.72  | -9.44 |  -1   |
| -8.65 | 8.96  |   1   |       | -1.63 | -9.10 |  -1   |
| -7.35 | -3.05 |   1   |       | 9.06  | -6.63 |  -1   |
| -2.88 | 7.55  |   1   |       | 1.16  | -7.63 |  -1   |
| -9.49 | 2.22  |   1   |       | 7.62  | -7.60 |  -1   |
| -7.49 | 7.62  |   1   |       | 4.90  | 8.06  |  -1   |
| -2.88 | -3.11 |   1   |       | 2.76  | 0.79  |  -1   |
| -7.65 | 9.42  |   1   |       | 9.37  | 0.39  |  -1   |
| -0.59 | 7.96  |   1   |       | 9.02  | -0.76 |  -1   |
| -4.54 | -6.58 |   1   |       | 7.46  | 6.30  |  -1   |
| -7.22 | -1.24 |   1   |       | 5.39  | 7.48  |  -1   |
| 0.41  | 6.26  |   1   |       | -0.40 | -9.40 |  -1   |
| -4.50 | -5.37 |   1   |       | 5.73  | -1.66 |  -1   |
| -8.93 | -0.80 |   1   |       | 7.09  | 1.56  |  -1   |
| -3.33 | 9.85  |   1   |       | 6.76  | 7.00  |  -1   |
| -4.79 | 9.76  |   1   |       | 5.89  | 3.02  |  -1   |
| -0.88 | 6.14  |   1   |       | 8.40  | 5.63  |  -1   |
| -7.59 | 9.02  |   1   |       | 0.58  | -0.11 |  -1   |
| -8.49 | -9.60 |   1   |       | 7.69  | -6.41 |  -1   |

{% include caption-img.html src="simple-classification-data.png" title="Fig.01 데이터" description="빨간 점은 $y=1$, 파란 점은 $y=-1$인 데이터이다." %}

# 데이터 분석

이 데이터를 바탕으로, 새로운 입력 $\boldsymbol{x}_{new}$가 주어졌을 때, 출력 $y$값이 -1일지(파란 점) 1일지(빨간 점) 추정할 수 있는 모델을 설계해보자.

산점도를 보게 되면 왼쪽 위가 빨간 점($y=1$)이고, 오른쪽 아래가 파란 점($y=-1$)이다. 개략적으로 적당한 선형함수가 있어, ($x_1$, $x_2$)가 선형함수의 그래프보다 위에 있으면($x_2 > kx_1$이면) $y=1$, ($x_1$, $x_2$)가 선형함수의 그래프보다 아레에 있으면($x_2 < kx_1$이면) $y=-1$인 형태임을 확인할 수 있다.

# 모델 설계

이에 다음과 같은 모델을 생각할 수 있다.

$$f(\boldsymbol{x}) = \mathrm{sign}(\boldsymbol{w}^\intercal \boldsymbol{x})$$

($\mathrm{sign}$함수는 입력값의 부호를 반환하는 함수이다. 즉 입력값이 양수면 +1을, 음수면 -1을 출력한다.)

참고로 이 모델은 [퍼셉트론(Perceptron)]({{ site.url}}{{ site.baseurl }}/machine_learning/06-perceptron/)의 일종이다.

# 파라미터 최적화

이제 이렇게 만든 모델을 경사 하강법을 이용해 학습시켜 보자. 

## 오차 함수의 정의 : 0-1 손실 함수 (0-1 Loss Function)

경사 하강법을 사용하기 위해서는 우선 오차 함수를 정의해야 한다. 이 문제에서 사용할 오차 함수는 "0-1 손실 함수(0-1 Loss Function)"이라 불리는 함수이다. 주어진 데이터 셋 $D$에 대해 0-1 손실 함수 $J(\boldsymbol{w})$는 다음과 같이 정의된다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, D } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i) \cdot M(\boldsymbol{w}, \boldsymbol{x}_i, y_i)$$

이때 $M(\boldsymbol{w}, \boldsymbol{x}, y)$는 다음과 같이 정의되는 함수이다.

$$M(\boldsymbol{w}, \boldsymbol{x}, y) = \begin{cases} 1 \qquad \text{if } y \neq f(\boldsymbol{x}) \\ 0 \qquad \text{if } y = f(\boldsymbol{x})\end{cases}$$

$M(\boldsymbol{w}, \boldsymbol{x}, y)$은 제대로 분류되지 않은 데이터($y \neq f(\boldsymbol{x})$)만 골라주는 함수이다. 이를 사용하여 $J(\boldsymbol{w})$에서는 제대로 분류되지 않은 데이터에 대해, $-y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$들의 합을 구하게 된다.

$J(\boldsymbol{w})$를 다음과 같이 바꿀 수도 있다.

$$J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } -y_i \cdot (\boldsymbol{w}^\intercal \boldsymbol{x}_i)$$

$M(\boldsymbol{w}, \boldsymbol{x}, y)$을 사용하지 않은 대신, $(\boldsymbol{x}_i, y_i)$들의 집합을 "전체 데이터의 집합" $D$에서 "제대로 분류되지 않은 데이터들의 집합" $S$로 바꾸었다. 실제 계산할 때는 이 형태를 사용하여 계산한다.

$J(\boldsymbol{w})$에는 다음과 같은 특징이 있다.

- $J(\boldsymbol{w}) \ge 0$ : $y_i$와 $\boldsymbol{w}^\intercal \boldsymbol{x}_i$의 부호가 다를 때만 $M(\boldsymbol{w}, \boldsymbol{x}, y)$이 활성화(1)된다.
- 최적화되면, 즉 모든 데이터가 제대로 분류되었으면 $J(\boldsymbol{w}) = 0$
- 잘못 분류된 데이터가 많을수록 $J(\boldsymbol{w})$의 값이 커진다.

## 오차 함수의 그라디언트 구하기

이제 오차함수의 그라디언트를 구해 보자. $\boldsymbol{w}$의 $j$번째 원소 $w_j$에 대해,

$$\frac{\partial }{\partial w_j} J(\boldsymbol{w}) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } \frac{\partial}{\partial w_j} ( -y_i (w_0 x_{i0} + w_1 x_{i1} + \cdots + w_nx_{in}) ) = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i x_{ij}$$

가 된다.

따라서 그라디언트는 다음과 같이 계산된다.

$$\nabla J(\boldsymbol{w}) = [\frac{\partial }{\partial w_0} J(\boldsymbol{w}), \frac{\partial }{\partial w_1} J(\boldsymbol{w}), \cdots, \frac{\partial }{\partial w_n} J(\boldsymbol{w})] = \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } - y_i \boldsymbol{x}_{i}$$

## 경사 하강법 적용하기

이제 경사 하강법을 적용해 보자.

우선 파라미터를 랜덤한 값으로 초기화한다. 이후 업데이트를 진행한다. 경사하강법에서 파라미터 업데이트는 $\boldsymbol{w}\_{new} = \boldsymbol{w}\_{old} - \eta \nabla J(\boldsymbol{w}_{old})$을 이용하여 진행된다. 이 식에 계산한 그라디언트를 넣고 학습률 $\eta = 1$로 설정하면 다음 업데이트 식을 얻을 수 있다.

$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} + \sum_{(\boldsymbol{x}_i, y_i) \, \in \, S } y_i \boldsymbol{x}_{i}$$

이제 이 식을 가지고 더이상 잘못 분류된 데이터가 없을 때까지 파라미터를 업데이트 하면 된다.

## 배치 경사 하강법 (BGD, Batch Gradient Descent Method), 확률적 경사 하강법 (SGD, Stochastic Gradient Descent Method)

근데 한 가지 생각해 봐야 할 것이 있다. 이번 문제같이 데이터 수가 얼마 되지 않을 때는 모든 데이터를 다 살펴본 후, 제대로 분류되지 않은 데이터들의 집합 $S$를 구하는 것이 별로 어렵지 않다. 하지만 데이터 수가 너무 많다면 어떨까? $S$를 구하고, 이를 이용해 그라디언트를 계산하는 비용이 만만치 않을 것이다. 심지어 메모리에 다 올라가지도 않을 테라바이트 급 데이터를 다루는 경우, $S$를 메모리에 올리는 것 자체가 물리적으로 불가능할 수도 있다!

이 문제를 해결하기 위해 확률적 경사 하강법 (SGD, Stochastic Gradient Descent Method)이라는 방법이 제시되었다. 경사 하강법은 오차 함수의 그라디언트 반대 방향으로 움직여 오차를 최소화하는 방향으로 움직이는 것이다. 이때 (전체 데이터 셋에 대한) 그라디언트는 보통 데이터 하나에 대한 그라디언트의 합 형태로 표현된다. 예를 들어 우리 문제의 경우, 그라디언트 $\nabla J(\boldsymbol{w}) = \sum - y_i \boldsymbol{x}\_{i}$는 $- y_i \boldsymbol{x}_{i}$의 합 형태이다.

확률적 경사 하강법은 이 사실에 착안한다. 확률적 경사 하강법은 **전체 데이터를 살펴보면서, 제대로 분류되지 않은 데이터를 찾을 때마다 그 데이터 하나에 대한 그라디언트를 계산해 파라미터를 업데이트하는 방법**이다. 이에 대비에 기존 방법, 즉 **전체 데이터를 살펴보면서 제대로 분류되지 않은 데이터들의 집합 $S$를 구한 후, $S$에 대한 그라디언트를 구해 파라미터를 한 번 업데이트하는 방법**을 배치 경사 하강법(BGD, Batch Gradient Descent Method)이라 한다.

확률적 경사 하강법으로 데이터 하나하나에 대한 그라디언트를 계산해 파라미터를 업데이트하게 되면 사실 항상 오차 함수가 작아지는 방향으로 움직이는 것을 장담할 수 없다. 하지만 높은 '확률'로 오차 함수가 작아지는 방향으로 움직일 것이다. 이 때문에 '확률적(stochastic)'이라는 단어로 이 방법을 나타내는 것이다.

두 방법의 장단점을 알아보자. 배치 경사 하강법에서 파라미터들은 아주 안정적으로 최적화된다(오차 함수가 안정적으로 최소화된다). 이 안정성은 단점이 될 수도 있는데, 만약 오차 함수가 볼록 함수(convex function)가 아닐 경우 아주 안정적으로 최소점(global mimimum)이 아닌 극소점(local minimum)으로 수렴해 버릴 수 있다. 또한 앞서 말했던 것처럼 모든 데이터를 다 살펴보는 것이 불가능할 수도 있고, 속도가 많이 느리다.

그에 반해 확률적 경사 하강법은 배치 경사 하강법에 비해 일반적으로 속도가 빠르다. 파라미터들은 훨씬 더 자주 업데이트된다. 하지만 데이터에 노이즈가 많거나 크다면 파라미터가 이상한 값으로 튀어 수렴 속도가 늦어지거나, 심할 경우 수렴이 안될 수도 있다. 대신 최소점이 아닌 극소점에 빠졌더라도 탈출할 확률이 생긴다.

배치 경사 하강법을 의사 코드(Pseudo Code)로 나타내면 다음과 같다.

{% highlight python linenos %}
eta = 1 # 하이퍼파라미터 : 학습률 (learning rate)

initialize(w) # w 랜덤 값으로 초기화

while True:
    S = []

    # 제대로 분류되지 않은 데이터 찾기
    for data in dataset:
        x, y = data
        predict_y = model(x)

        if predict_y != y: # 제대로 분류되지 않은 데이터라면
            S.append(data) # S에 저장

    if S is empty: # S가 공집합이면(모든 데이터가 제대로 분류되었다면)
        break # 종료

    # 파리미터 업데이트하기
    gradient = 0
    for wrong_data in S:
        x, y = wrong_data
        gradient += -y * x # 그라디언트 계산식

    w = w - eta * gradient # 파라미터 업데이트

{% endhighlight %}

확률적 경사 하강법을 의사 코드로 나타내면 다음과 같다.

{% highlight python linenos %}
eta = 1 # 하이퍼파라미터 : 학습률 (learning rate)

initialize(w) # w 랜덤 값으로 초기화

while True:
    quit = True

    for data in dataset:
        x, y = data
        predict_y = model(x)

        if predict_y != y: # 제대로 분류되지 않은 데이터라면
            gradient = -y * x # 그라디언트 계산식
            w = w - eta * gradient # 파라미터 업데이트
            quit = False

    if quit is True: # 모든 데이터가 제대로 분류되었다면
        break # 종료

{% endhighlight %}

# 문제 풀이

이제 위의 내용을 바탕으로 실제 동작하는 코드를 작성해 보자.

참고로 아래 코드들의 실행 결과는 seed가 2020일 때 계산된 결과이다. `np.random.seed(2020)`라 두면 같은 실행 결과를 얻을 수 있을 것이다.

## BGD

{% highlight python linenos %}
import numpy as np

def draw(title):
    print(title)
    
    pos_x = X[Y == 1]
    neg_x = X[Y == -1]

    t = -(w[0] / w[1])
    tan_x = np.array(range(-15, 15))
    tan_y = t * tan_x

    plt.scatter(pos_x[:, 0], pos_x[:, 1], c="r")
    plt.scatter(neg_x[:, 0], neg_x[:, 1], c="b")
    plt.plot(tan_x, tan_y, c='k')
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.show()
    
    print(w)
    print()

np.random.seed(2020) # for consistency

# Data
x1 = [
    -2.58, -2.87, -8.65, -7.35, -2.88, -9.49, -7.49, -2.88, -7.65, -0.59,
    -4.54, -7.22,  0.41, -4.50, -8.93, -3.33, -4.79, -0.88, -7.59, -8.49,
     1.31,  4.72, -1.63,  9.06,  1.16,  7.62,  4.90,  2.76,  9.37,  9.02,
     7.46,  5.39, -0.40,  5.73,  7.09,  6.76,  5.89,  8.40,  0.58,  7.69
] 

x2 = [
     2.93,  8.99,  8.96, -3.05,  7.55,  2.22,  7.62, -3.11,  9.42,  7.96,
    -6.58, -1.24,  6.26, -5.37, -0.80,  9.85,  9.76,  6.14,  9.02, -9.60,
    -5.90, -9.44, -9.10, -6.63, -7.63, -7.60,  8.06,  0.79,  0.39, -0.76,
     6.30,  7.48, -9.40, -1.66,  1.56,  7.00,  3.02,  5.63, -0.11, -6.41
]

X = np.empty([len(x1), 2], dtype=np.float32)
X[:, 0], X[:, 1] = x1, x2

Y = [
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
]

Y = np.array(Y, dtype=np.int32)

eta = 1 # Hyperparameter : Learning Rate

# initialize w
w = np.random.rand(X.shape[1])

draw("Before")

while True:
    S = []
    
    for i in range(len(X)):
        x, y = X[i], Y[i]
        predict_y = (1 if w.dot(x) > 0 else -1)
        
        if predict_y != y: # if misclassified
            S.append([x, y])
    
    if len(S) == 0:
        break
    
    gradient = np.zeros(w.shape[0])
    for wrong_data in S:
        x, y = wrong_data
        gradient += -y * x # calc gradient
    
    w = w - eta * gradient # update w

draw("After")
{% endhighlight %}

### 실행 결과

{% include caption-img.html src="simple-classification-bgd-result.png" title="Fig.02 BGD" description="위 그래프는 최적화 전, 아래 그래프는 최적화 후. 빨간 점은 $y=1$, 파란 점은 $y=-1$, 검은 선은 파라미터를 나타냄. 그래프 아래 수는 w를 나타냄." %}

## SGD

{% highlight python linenos %}
import numpy as np

def draw(title):
    print(title)
    
    pos_x = X[Y == 1]
    neg_x = X[Y == -1]

    t = -(w[0] / w[1])
    tan_x = np.array(range(-15, 15))
    tan_y = t * tan_x

    plt.scatter(pos_x[:, 0], pos_x[:, 1], c="r")
    plt.scatter(neg_x[:, 0], neg_x[:, 1], c="b")
    plt.plot(tan_x, tan_y, c='k')
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.show()
    
    print(w)
    print()

np.random.seed(2020) # for consistency

# Data
x1 = [
    -2.58, -2.87, -8.65, -7.35, -2.88, -9.49, -7.49, -2.88, -7.65, -0.59,
    -4.54, -7.22,  0.41, -4.50, -8.93, -3.33, -4.79, -0.88, -7.59, -8.49,
     1.31,  4.72, -1.63,  9.06,  1.16,  7.62,  4.90,  2.76,  9.37,  9.02,
     7.46,  5.39, -0.40,  5.73,  7.09,  6.76,  5.89,  8.40,  0.58,  7.69
] 

x2 = [
     2.93,  8.99,  8.96, -3.05,  7.55,  2.22,  7.62, -3.11,  9.42,  7.96,
    -6.58, -1.24,  6.26, -5.37, -0.80,  9.85,  9.76,  6.14,  9.02, -9.60,
    -5.90, -9.44, -9.10, -6.63, -7.63, -7.60,  8.06,  0.79,  0.39, -0.76,
     6.30,  7.48, -9.40, -1.66,  1.56,  7.00,  3.02,  5.63, -0.11, -6.41
]

X = np.empty([len(x1), 2], dtype=np.float32)
X[:, 0], X[:, 1] = x1, x2

Y = [
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
]

Y = np.array(Y, dtype=np.int32)

eta = 1 # Hyperparameter : Learning Rate

# initialize w
w = np.random.rand(X.shape[1])

draw("Before")

while True:
    quit = True
    
    for i in range(len(X)):
        x, y = X[i], Y[i]
        predict_y = (1 if w.dot(x) > 0 else -1)
        
        if predict_y != y: # if misclassified
            gradient = -y * x # calc gradient
            w = w - eta * gradient # update w
            quit = False
    
    if quit == True:
        break

draw("After")
{% endhighlight %}

### 실행 결과

{% include caption-img.html src="simple-classification-sgd-result.png" title="Fig.03 SGD" description="위 그래프는 최적화 전, 아래 그래프는 최적화 후. 빨간 점은 $y=1$, 파란 점은 $y=-1$, 검은 선은 파라미터를 나타냄. 그래프 아래 수는 w를 나타냄." %}
