---
title: "단순 선형 회귀 (Simple Linear Regression)"
order: 10
date: "2020-05-14"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순 선형 회귀(Simple Linear Regression Model) 문제를 풀어보자.

# 문제 상황

독립변수 $x$와 종속변수 $y$에 대해 다음과 같이 데이터가 주어졌다고 하자.

<div class="table-wrapper" markdown="block">

|  $x$  |  $y$  |
| :---: | :---: |
| 2.96  | 4.12  |
| 8.00  | 6.28  |
| 1.59  | 3.05  |
| 4.22  | 5.02  |
| 5.18  | 5.01  |
| 5.57  | 5.03  |
| 1.18  | 3.19  |
| 4.75  | 4.75  |
| 1.69  | 3.40  |
| 7.96  | 6.43  |
| 1.80  | 3.72  |
| 4.47  | 4.24  |
| 3.04  | 4.21  |
| 2.67  | 3.75  |
| 3.11  | 3.76  |
| 5.48  | 5.55  |
| 0.80  | 2.58  |
| 8.77  | 6.95  |
| 4.77  | 4.46  |
| 2.72  | 4.35  |
| 2.81  | 3.77  |
| 2.97  | 3.99  |
| 9.74  | 7.58  |
| 7.77  | 6.60  |
| 4.10  | 4.48  |
| 5.85  | 5.70  |
| 2.50  | 3.67  |
| 7.01  | 6.07  |
| 6.36  | 5.69  |
| 2.05  | 3.47  |
| 0.02  | 2.57  |
| 8.19  | 7.00  |
| 0.08  | 2.27  |
| 8.80  | 7.22  |
| 9.76  | 7.40  |
| 9.47  | 7.09  |
| 1.55  | 3.10  |
| 3.73  | 4.84  |
| 3.60  | 4.18  |
| 2.50  | 3.63  |

</div>

{% include caption-img.html src="simple-linear-regression-data.png" title="Fig.01 산점도" description="독립변수 $x$와 종속변수 $y$에 대한 산점도" %}

# 모델 설계 : 단순 선형 회귀 모델 (Simple Linear Regression Model)

산점도에서 볼 수 있듯이 $x$, $y$ 간에는 아주 강한 선형 상관관계가 있다. 이를 바탕으로 다음과 같이 모델을 세울 수 있다.



$$y = f(x) = w_1 x + w_0$$



이 모델을 단순 선형 회귀 모델(Simple Linear Regression Model)이라 한다. 단순 선형 회귀 모델은 독립 변수와 종속 변수가 1개씩 있고, 모델의 차수가 일차식인 모델이다.

위 모델을 다음과 같이 쓸 수도 있다.



$$y = f(x) = w_1 x_1 + w_0 x_0$$



위와 같이 $x$를 $x_1$으로 (이름을) 바꾸고, 항상 1인 변수 $x_0$를 추가한다. 이렇게 하면 이 모델을 다음과 같이 이해할 수 있다.



$$y = f(x) = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = \begin{bmatrix} w_0\\w_1\end{bmatrix} ^\intercal  \cdot \begin{bmatrix} x_0\\x_1\end{bmatrix}$$



# 파리미터 최적화

## 오차 함수 : 평균 제곱 오차 (MSE, Mean of Squared Error)

단순 선형 회귀 모델의 파라미터를 최적화하기 위해서는 [평균 제곱 오차(MSE, Mean of Squared Error)](/machine_learning/04-error-functions#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용한다.

총 $n$쌍의 입력 데이터-출력 데이터(레이블) 쌍 ($\boldsymbol{x}_i$, $y_i$) ($i=1, 2, \cdots, n)$가 있는 데이터 셋과 모델 $f$가 주어졌을 때, 모델에 대한 평균 제곱 오차 $J(\boldsymbol{w})$는 다음과 같이 정의된다.



$$J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(x_i))^2$$



## 해석적 풀이법 (Analytic Solution)

$J(\boldsymbol{w})$를 $j$번째 파라미터 $w_j$에 대해 편미분하면 다음과 같다. ($j = 0, 1$)



$$
\begin{align}
\frac {\partial}{\partial w_j} J(\boldsymbol{w}) 
&= \frac {\partial}{\partial w_j} \left( \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( \frac {\partial}{\partial w_j} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} f(\boldsymbol{x}_i) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} (w_0x_{i0} + w_1x_{i1}) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} )\\
\end{align}
$$



파라미터를 최적화한다는 것은 $J(\boldsymbol{w})$를 최소로 만드는 $\boldsymbol{w}$를 찾는 것이다. 이 문제는 그라디언트의 성질에 의해 모든 $j=0,\, 1$에 대해 동시에 $\partial J / \partial w_j$를 0으로 만드는(최소화하는) $\boldsymbol{w}$을 찾는 문제로 바꿀 수 있다. 즉,



$$\frac {\partial}{\partial w_j} J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} ) = 0$$



을 최소화하는 $\boldsymbol{w} = [w_0, w_1]$을 찾으면 된다.

이 식을 변형하면



$$\sum _{i=1} ^{n} f(\boldsymbol{x}_i) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$





$$\sum _{i=1} ^{n} (w_0 x_{i0} + w_1 x_{i1}) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$





$$w_0 \sum _{i=1} ^{n} x_{i0} x_{ij} + w_1 \sum _{i=1} ^{n} x_{i1} x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$



이므로, 우리는 $j$에 0과 1을 대입하여 다음 연립방정식을 얻을 수 있다.



$$\begin{cases}
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i0} = \sum _{i=1} ^{n} y_i \cdot x_{i0}\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i1} = \sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{cases}$$



이를 행렬의 곱 형태로 표현하면



$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1}\\
\end{array} \right]

\cdot

\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]

=

\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{array} \right]
$$



가 된다.

이때, 



$$X = \left[ \begin{array}\\
x_{10} & x_{11}\\
x_{20} & x_{21}\\
\vdots & \vdots\\
x_{n0} & x_{n1}\\
\end{array} \right],\quad Y = \left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]$$



라 하면



$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1}\\
\end{array} \right]
=
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\end{array} \right]
\cdot
\left[ \begin{array}\\
x_{10} & x_{11}\\
x_{20} & x_{21}\\
\vdots & \vdots\\
x_{n0} & x_{n1}\\
\end{array} \right]
= X^\intercal \cdot X
$$





$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{array} \right]
=
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\end{array} \right]
\cdot
\left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]
= X^\intercal \cdot Y
$$



이므로, 다음과 같이 정리할 수 있다.



$$(X^\intercal \cdot X) \cdot
\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]
=(X^\intercal \cdot Y)$$



이를 풀면



$$
\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]
= (X^\intercal \cdot X)^{-1} \cdot (X^\intercal \cdot Y)
$$



가 된다.

이제 주어진 데이터에 대해 이 식을 적용하면 $J(\boldsymbol{w})$를 최소로 만드는 $\boldsymbol{w} = [w_0, w_1]$를 구할 수 있다.

## 수치 계산법 (Numerical Solution)

[해석적 풀이법](#kramdown_해석적-풀이법-analytic-solution)에서 구했다시피 오차 함수 $J(\boldsymbol{w})$의 그라디언트는 다음과 같다.



$$
\begin{align}
\nabla J(\boldsymbol{w})

&= \left[ \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i0} ), \, \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i1} ) \right]\\[5pt]


&= -\frac {2}{n}
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\end{array} \right]

\cdot

\left[ \begin{array}\\
y_1 - f(\boldsymbol{x}_1)\\
y_2 - f(\boldsymbol{x}_2)\\
\vdots\\
y_n - f(\boldsymbol{x}_n)\\
\end{array} \right]\\[5pt]

&= -\frac {2}{n} \, X^\intercal \cdot (Y - f(X))

\end{align}
$$



이 식을 바탕으로 다음 업데이트 식을 얻을 수 있다.



$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} - \eta \, \left[ -\frac {2}{n} \, X^\intercal \cdot (Y - f(X)) \right]$$




# 문제 풀이

## 해석적 풀이법

{% highlight python linenos %}

import numpy as np
import matplotlib.pyplot as plt

def draw(title, w):
    plt.scatter(x, y, c='b')
    plt.xlim(0, 10)
    plt.ylim(-0.1, 10.1)
    plt.title(title)
    plt.plot(range(-1, 11), [(w[0, 0] + w[1, 0] * x) for x in range(-1, 11)], c='r')
    plt.show()
    print("w = [ %f, %f ]" % (w[0, 0], w[1, 0]))

# data
x = np.array([
    2.96, 8.00, 1.59, 4.22, 5.18, 5.57, 1.18, 4.75, 1.69, 7.96, 
    1.80, 4.47, 3.04, 2.67, 3.11, 5.48, 0.80, 8.77, 4.77, 2.72, 
    2.81, 2.97, 9.74, 7.77, 4.10, 5.85, 2.50, 7.01, 6.36, 2.05, 
    0.02, 8.19, 0.08, 8.80, 9.76, 9.47, 1.55, 3.73, 3.60, 2.50, 
])

y = np.array([
    4.12, 6.28, 3.05, 5.02, 5.01, 5.03, 3.19, 4.75, 3.40, 6.43,
    3.72, 4.24, 4.21, 3.75, 3.76, 5.55, 2.58, 6.95, 4.46, 4.35, 
    3.77, 3.99, 7.58, 6.60, 4.48, 5.70, 3.67, 6.07, 5.69, 3.47, 
    2.57, 7.00, 2.27, 7.22, 7.40, 7.09, 3.10, 4.84, 4.18, 3.63, 
])

# reformat the data
x_ = np.concatenate((np.ones((x.shape[0], 1)), np.expand_dims(x, axis=1)), axis=1) # x0 = 1인 열 추가
y_ = np.expand_dims(y, axis=1)

# calculate w
w = np.linalg.inv(x_.T @ x_) @ (x_.T @ y_)

# result
draw("Analytic Solution", w)

{% endhighlight %}

### 실행 결과

{% include caption-img.html src="simple-linear-regression-analytic-sol-result.png" title="Fig.02 해석적 풀이법 시행 결과" description="빨간 직선은 추정된 $\boldsymbol{w}$로 그린 것이다. 직선이 데이터(파란 점)들의 분포를 잘 묘사하고 있는 것을 볼 수 있다." %}

## 수치 계산법

### SGD

에폭의 수가 많아짐에 따라 실제로 w가 정답에 가까워지고 있는 것을 보여주기 위해, 매 에폭마다 계산된 $\boldsymbol{w}$의 값으로 그래프를 그려보았다.

{% highlight python linenos %}

import numpy as np
import matplotlib.pyplot as plt

def draw(title, w):
    plt.scatter(x, y, c='b')
    plt.xlim(0, 10)
    plt.ylim(-0.1, 10.1)
    plt.title(title)
    plt.plot(range(-1, 11), [(w[0, 0] + w[1, 0] * x) for x in range(-1, 11)], c='r')
    plt.figtext(0.5, 0, "w = [ %f, %f ]" % (w[0, 0], w[1, 0]), wrap=True, horizontalalignment='center', fontsize=11)
    plt.show()

np.random.seed(2020) # for consistency

# data
x = np.array([
    2.96, 8.00, 1.59, 4.22, 5.18, 5.57, 1.18, 4.75, 1.69, 7.96, 
    1.80, 4.47, 3.04, 2.67, 3.11, 5.48, 0.80, 8.77, 4.77, 2.72, 
    2.81, 2.97, 9.74, 7.77, 4.10, 5.85, 2.50, 7.01, 6.36, 2.05, 
    0.02, 8.19, 0.08, 8.80, 9.76, 9.47, 1.55, 3.73, 3.60, 2.50, 
])

y = np.array([
    4.12, 6.28, 3.05, 5.02, 5.01, 5.03, 3.19, 4.75, 3.40, 6.43,
    3.72, 4.24, 4.21, 3.75, 3.76, 5.55, 2.58, 6.95, 4.46, 4.35, 
    3.77, 3.99, 7.58, 6.60, 4.48, 5.70, 3.67, 6.07, 5.69, 3.47, 
    2.57, 7.00, 2.27, 7.22, 7.40, 7.09, 3.10, 4.84, 4.18, 3.63, 
])

x_ = np.concatenate((np.ones((x.shape[0], 1)), np.expand_dims(x, axis=1)), axis=1)
y_ = np.expand_dims(y, axis=1)

# initialize w
w = np.random.rand(x_.shape[1], 1)
draw("Initial", w)

# hyperparameter
epochs = 20
lr = 0.01

# training
loss = 0
for epoch in range(epochs):
    data_num = x_.shape[0]
    for i in range(data_num):
        sample = x_[i:i + 1, :]
        true_y = y_[i:i + 1, :]
        
        predict_y = sample @ w
        
        # calculate loss
        loss += np.sum(np.square(true_y - predict_y))
        
        # calculate gradient
        gradient = -2 * sample.T @ (true_y - predict_y)
        
        # update w
        w = w - lr * gradient
    loss /= data_num
    
    draw("epoch %d : loss = %f" % (epoch + 1, loss), w)

{% endhighlight %}

#### 실행 결과

{% include caption-img.html src="simple-linear-regression-numeric-sol-sgd-result.png" class="no-max-height" title="Fig.03 확률적 경사 하강법 시행 결과" description="각 에폭(epoch) 별 추정된 $\boldsymbol{w}$를 빨간 직선으로 그렸다. 에폭의 수가 커질수록 직선이 점점 데이터(파란 점)들의 분포를 잘 묘사하게 되는 것을 볼 수 있다." %}

### Minibatch GD

일반적으로 경사 하강법에서 학습의 진행 상황을 알고자 할 때, 위의 [확률적 경사 하강법](#kramdown_sgd)에서 했던 것처럼 추정된 $\boldsymbol{w}$ 값을 보기보다는 오차(loss)값이 어떻게 변하고 있는지를 주로 많이 본다. 매 에폭마다 오차 값이 어떻게 변하는지를 그래프로 그려보았다.

{% highlight python linenos %}

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(2020) # for consistency

# data
x = np.array([
    2.96, 8.00, 1.59, 4.22, 5.18, 5.57, 1.18, 4.75, 1.69, 7.96, 
    1.80, 4.47, 3.04, 2.67, 3.11, 5.48, 0.80, 8.77, 4.77, 2.72, 
    2.81, 2.97, 9.74, 7.77, 4.10, 5.85, 2.50, 7.01, 6.36, 2.05, 
    0.02, 8.19, 0.08, 8.80, 9.76, 9.47, 1.55, 3.73, 3.60, 2.50, 
])

y = np.array([
    4.12, 6.28, 3.05, 5.02, 5.01, 5.03, 3.19, 4.75, 3.40, 6.43,
    3.72, 4.24, 4.21, 3.75, 3.76, 5.55, 2.58, 6.95, 4.46, 4.35, 
    3.77, 3.99, 7.58, 6.60, 4.48, 5.70, 3.67, 6.07, 5.69, 3.47, 
    2.57, 7.00, 2.27, 7.22, 7.40, 7.09, 3.10, 4.84, 4.18, 3.63, 
])

x_ = np.concatenate((np.ones((x.shape[0], 1)), np.expand_dims(x, axis=1)), axis=1)
y_ = np.expand_dims(y, axis=1)

# initialize w
w = np.random.rand(x_.shape[1], 1)
#draw("Initial", w)

# hyperparameter
epochs = 500
lr = 0.01
batch_size = 10

# training
data_num = x_.shape[0]
batch_num = int(np.ceil(data_num / batch_size))
losses = []
for epoch in range(epochs):
    loss = 0
    
    for i in range(batch_num):
        sample = x_[i * batch_size:(i + 1) * batch_size, :]
        true_y = y_[i * batch_size:(i + 1) * batch_size, :]
        predict_y = sample @ w
        
        # calculate loss
        loss += np.sum(np.square(true_y - predict_y))
        
        # calculate gradient
        gradient = -(2 / sample.shape[0]) * sample.T @ (true_y - predict_y)
        
        # update w
        w = w - lr * gradient
    loss /= data_num
    losses.append(loss)
    print("epoch %03d : loss = %f, w = [ %f, %f ]" % (epoch + 1, loss, w[0, 0], w[1, 0]))

plt.plot([(x + 1) for x in range(len(losses))], losses)
plt.title("Mini-batch Losses")
plt.show()

{% endhighlight %}

#### 실행 결과

{% include folder.html filename="simple-linear-regression-minibatch-gd-result.txt" mode="raw-txt" show_msg="출력 결과 보기" hide_msg="출력 결과 숨기기" %}

{% include caption-img.html src="simple-linear-regression-numeric-sol-minibatch-gd-result.png" title="Fig.04 미니배치 경사 하강법 시행 결과" description="각 에폭(epoch) 별 오차(loss)값을 그래프로 나타내었다.<br/>에폭의 수가 커질수록 오차가 감소하지만, 그 감소폭이 점점 작아지는 것을 볼 수 있다. 그 감소폭이 0이 되었을 때가 바로 그라디언트가 0이 되었을 때로, 가장 작은 오차를 만드는(최적화된) 파라미터를 찾은 것이다.<br/>그래프에서 볼 수 있다시피 확률적 경사 하강법과 비교했을 때 초기에 오차 하락율이 크다. 미니배치 경사 하강법은 확률적 경사 하강법에 비해 안정성이 크기 때문이다." %}

### BGD

{% highlight python linenos %}

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(2020) # for consistency

# data
x = np.array([
    2.96, 8.00, 1.59, 4.22, 5.18, 5.57, 1.18, 4.75, 1.69, 7.96, 
    1.80, 4.47, 3.04, 2.67, 3.11, 5.48, 0.80, 8.77, 4.77, 2.72, 
    2.81, 2.97, 9.74, 7.77, 4.10, 5.85, 2.50, 7.01, 6.36, 2.05, 
    0.02, 8.19, 0.08, 8.80, 9.76, 9.47, 1.55, 3.73, 3.60, 2.50, 
])

y = np.array([
    4.12, 6.28, 3.05, 5.02, 5.01, 5.03, 3.19, 4.75, 3.40, 6.43,
    3.72, 4.24, 4.21, 3.75, 3.76, 5.55, 2.58, 6.95, 4.46, 4.35, 
    3.77, 3.99, 7.58, 6.60, 4.48, 5.70, 3.67, 6.07, 5.69, 3.47, 
    2.57, 7.00, 2.27, 7.22, 7.40, 7.09, 3.10, 4.84, 4.18, 3.63, 
])

x_ = np.concatenate((np.ones((x.shape[0], 1)), np.expand_dims(x, axis=1)), axis=1)
y_ = np.expand_dims(y, axis=1)

# initialize w
w = np.random.rand(x_.shape[1], 1)
#draw("Initial", w)

# hyperparameter
epochs = 500
lr = 0.01
batch_size = 10

# training
losses = []
for epoch in range(epochs):
    sample = x_
    true_y = y_
    predict_y = sample @ w

    # calculate loss
    loss = np.mean(np.square(true_y - predict_y))
    losses.append(loss)
    
    # calculate gradient
    gradient = -(2 / sample.shape[0]) * sample.T @ (true_y - predict_y)

    # update w
    w = w - lr * gradient
    
    print("epoch %03d : loss = %f, w = [ %f, %f ]\t\t" % (epoch + 1, loss, w[0, 0], w[1, 0]), end=("" if epoch % 2 == 0 else "\n"))

plt.plot([(x + 1) for x in range(len(losses))], losses)
plt.title("Mini-batch Losses")
plt.show()

{% endhighlight %}

#### 실행 결과

{% include folder.html filename="simple-linear-regression-bgd-result.txt" mode="raw-txt" show_msg="출력 결과 보기" hide_msg="출력 결과 숨기기" %}

{% include caption-img.html src="simple-linear-regression-numeric-sol-bgd-result.png" title="Fig.05 배치 경사 하강법 시행 결과" description="각 에폭(epoch) 별 오차(loss)값을 그래프로 나타내었다.<br/>위 두 방법에 비해서 최적화시키는데 시간이 많이 요구된다.<br/>그래프에서 볼 수 있다시피 확률적 경사 하강법과 비교했을 때 초기에 오차 하락율이 매우 크다. 배치 경사 하강법은 확률적 경사 하강법에 비해 안정성이 크기 때문이다." %}