---
title: "다중 선형 회귀 (Multiple Linear Regression)"
order: 9
date: "2020-05-21"
isWriting: true
---

다중 선형 회귀는 [단순 선형 회귀]({{ site.url }}{{ site.baseurl }}/machine_learning/08-simple-linear-regression)와 크게 다르지 않다. 단순 선형 회귀에서의 과정을 잘 이해했다면 다중 선형 회귀로의 확장은 그리 어렵지 않을 것이다.

# 문제 상황

다음 데이터는 다음 데이터는 [보스턴 주택 가격 데이터 셋(Boston House-price Dataset)](http://lib.stat.cmu.edu/datasets/boston)으로 불리는 유명한 데이터 셋의 일부이다.

<div class="table-wrapper" markdown="block">

|  CRIM   |   ZN    |  INDUS  |  CHAS   |   NOX   |   RM    |   AGE   |   DIS   |   RAD   |   TAX   | PTRATIO |    B    |  LSTAT  |  MEDV   |
| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |
| 0.00632 |   18    |  2.31   |    0    |  0.538  |  6.575  |  65.2   |  4.09   |    1    |   296   |  15.3   |  396.9  |  4.98   |   24    |
| 0.02731 |    0    |  7.07   |    0    |  0.469  |  6.421  |  78.9   | 4.9671  |    2    |   242   |  17.8   |  396.9  |  9.14   |  21.6   |
| 0.02729 |    0    |  7.07   |    0    |  0.469  |  7.185  |  61.1   | 4.9671  |    2    |   242   |  17.8   | 392.83  |  4.03   |  34.7   |
| 0.03237 |    0    |  2.18   |    0    |  0.458  |  6.998  |  45.8   | 6.0622  |    3    |   222   |  18.7   | 394.63  |  2.94   |  33.4   |
| 0.06905 |    0    |  2.18   |    0    |  0.458  |  7.147  |  54.2   | 6.0622  |    3    |   222   |  18.7   |  396.9  |  5.33   |  36.2   |
| 0.02985 |    0    |  2.18   |    0    |  0.458  |  6.43   |  58.7   | 6.0622  |    3    |   222   |  18.7   | 394.12  |  5.21   |  28.7   |
| 0.08829 |  12.5   |  7.87   |    0    |  0.524  |  6.012  |  66.6   | 5.5605  |    5    |   311   |  15.2   |  395.6  |  12.43  |  22.9   |
| 0.14455 |  12.5   |  7.87   |    0    |  0.524  |  6.172  |  96.1   | 5.9505  |    5    |   311   |  15.2   |  396.9  |  19.15  |  27.1   |
| 0.21124 |  12.5   |  7.87   |    0    |  0.524  |  5.631  |   100   | 6.0821  |    5    |   311   |  15.2   | 386.63  |  29.93  |  16.5   |
| 0.17004 |  12.5   |  7.87   |    0    |  0.524  |  6.004  |  85.9   | 6.5921  |    5    |   311   |  15.2   | 386.71  |  17.1   |  18.9   |
| &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; | &#8942; |

</div>

이 데이터는 1978년 Harrison D.와 Rubinfeld, D.L.이 작성한 논문 <Hedonic prices and the demand for clean air>에 등장하는 데이터 셋으로, 1978년 보스턴 시의 506개 마을(town)의 여러 요건(feature)들과 주택 가격 중앙값(MEDV) 정보를 가지고 있다. 다음은 데이터 셋에 대한 자세한 정보이다.

- 레코드 수 : 506개
- 필드 개수 : 14개
  - CRIM : 마을별 1인당 범죄율
  - ZN : 25,000 평방피트를 초과하는 거주지역의 비율
  - INDUS : 비소매상업지역이 점유하고 있는 토지의 비율
  - CHAS : 찰스 강의 경계에 위치한 경우 1, 아니면 0
  - NOX : 10ppm 당 농축 일산화질소
  - RM : 주택 1가구 당 평균 방의 개수
  - AGE : 1940년 이전에 건축된 소유주택의 비율
  - DIS : 5개의 보스턴 직업센터까지의 접근성 지수
  - RAD : 방사형 도로까지의 접근성 지수
  - TAX : $10,000 당 재산세율
  - PTRATIO : 마을별 학생/교사 비율 
  - B : 1000(Bk - 0.63)<sup>2</sup>, Bk는 마을별 흑인 비율
  - LSTAT : 모집단의 하위계층의 비율(%)
  - MEDV : 본인 소유의 주택가격(중앙값) (단위: $1,000)
  - (CAT.MEDV) : MEDV가 $30,000을 넘으면 1, 아니면 0. 데이터 셋에 따라 이 필드가 있는 경우도 있다. (원본 데이터에는 없다.)

이 데이터 셋은 너무 유명하기에 다양한 기계학습 라이브러리에서 기본적으로 제공하고 있다. 이 글에서는 가장 유명한 파이썬 기계학습 라이브러리 중 하나인 [사이킷런(Scikit-Learn)](https://scikit-learn.org/stable/)에서 제공하는 데이터 셋을 사용하도록 하겠다.

사이킷런은 다음 pip 명령어로 쉽게 설치할 수 있다.

{% highlight python %}
pip install scikit-learn
{% endhighlight %}

보스턴 주택 가격 데이터 셋은 다음 명령어로 받을 수 있다.

{% highlight python %}
from sklearn import datasets

dataset = datasets.load_boston()
{% endhighlight %}

이렇게 받은 보스턴 주택 가격 데이터 셋은 다음과 같은 구조로 되어 있다.

- `dataset` [sklearn.utils.Bunch]
  - `data` [numpy.ndarray] : (506, 13) 506개의 각 행은 한 레코드를 나타냄. 13개의 각 열은 순서대로 각각 CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT를 의미함.
  - `target` [numpy.ndarray] : (506, ) 506개의 각 데이터는 `data`의 각 행에 상응하는 MEDV를 의미함.
  - `feature_names` [numpy.ndarray] : (13, ) 13개의 각 데이터는 `data`의 각 열의 이름을 의미함. 즉, array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']).
  - `filename` [str] : 데이터 셋 파일(.csv)이 저장되어 있는 경로를 나타냄.
  - `DESCR` [str] : 데이터 셋에 대한 설명서

이 데이터를 바탕으로 마을의 요인들과 주택 가격 사이의 관계를 다중 선형 회귀 모델을 이용하여 나타내보자.

# 모델 설계 : 다중 선형 회귀 모델 (Multiple Linear Regression Model)

$d$개의 변수(variable)[^1]로 이루어진 입력 데이터 $\boldsymbol{x}[x_1, x_2, \cdots, x_d]$와 출력 데이터 $y$에 대해, 다중 선형 회귀 모델 $f$는 다음과 같이 입력 데이터와 출력 데이터 간 관계를 추정한다.

[^1]: 특징(feature), 속성(attribute), 열(column) 등으로도 쓴다.

<div class="mathjax-wrapper" markdown="block">

$$y = f(\boldsymbol{x}) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_d x_d$$

</div>

$\boldsymbol{x}[x_1, x_2, \cdots, x_d]$에 항상 1인 더미 변수(dummy variable) $x_0$를 추가하면($\boldsymbol{x}[x_0, x_1, x_2, \cdots, x_d]$), 모델 $f$를 파라미터 $\boldsymbol{w}[w_0, w_1, w_2, \cdots, w_d]$에 대해 다음과 같이 나타낼 수도 있다.

<div class="mathjax-wrapper" markdown="block">

$$y = f(\boldsymbol{x}) = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = w_0 x_0 + w_1 x_1 + w_2 x_2 + \cdots + w_d x_d$$

</div>

# 파라미터 최적화

다중 선형 회귀 모델은 본질적으로 단순 선형 회귀 모델의 열의 개수가 늘어난 것이다. 따라서 다중 선형 회귀 모델의 파라미터 최적화 과정은 단순 선형 회귀 모델의 그것과 같다.[^2]

[^2]: 입력 데이터의 행렬 크기가 커진 것(($n$, 2) → ($n$, $d$)) 빼고는 식의 형태가 같다.

## 오차 함수 : 평균 제곱 오차 (MSE, Mean of Squared Error)

단순 선형 회귀 모델에서처럼, 다중 선형 회귀 모델의 파라미터를 최적화하기 위해서는 [평균 제곱 오차(MSE, Mean of Squared Error)]({{ site.url }}{{ site.baseurl }}/machine_learning/03-supervised-learning#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용한다.

총 $n$쌍의 입력 데이터-출력 데이터(레이블) 쌍 ($\boldsymbol{x}_i$, $y_i$) ($i=1, 2, \cdots, n)$가 있는 데이터 셋과 모델 $f$가 주어졌을 때, 모델에 대한 평균 제곱 오차 $J(\boldsymbol{w})$는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(x_i))^2$$

</div>

## 분석적 풀이법 (Analytic Solution)

$J(\boldsymbol{w})$를 $j$번째 파라미터 $w_j$에 대해 편미분하면 다음과 같다. ($j = 0, 1, \cdots, d$)[^3]

[^3]: 단순 선형 회귀 모델에서는 $j = 0, 1$이었다.

<div class="mathjax-wrapper" markdown="block">

$$
\begin{align}
\frac {\partial}{\partial w_j} J(\boldsymbol{w}) 
&= \frac {\partial}{\partial w_j} \left( \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( \frac {\partial}{\partial w_j} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} f(\boldsymbol{x}_i) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} (w_0x_{i0} + w_1x_{i1} + \cdots + w_d x_{id}) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} )\\
\end{align}
$$

</div>

이제 모든 $j = 0, 1, \cdots, d$에 대해 $\partial J / \partial w_j$를 동시에 0으로 만드는(최소화하는) $\boldsymbol{w}[w_0, w_1, \cdots, w_d]$를 계산해보자.

<div class="mathjax-wrapper" markdown="block">

$$X = \left[ \begin{array}\\
x_{10} & x_{11} & \cdots & x_{1d}\\
x_{20} & x_{21} & \cdots & x_{2d}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n0} & x_{n1} & \cdots & x_{nd}\\
\end{array} \right],\quad Y = \left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]$$

</div>

라 놓고, 단순 선형 회귀 모델에서와 같은 방법으로 식을 변형, 계산하자.

<div class="mathjax-wrapper" markdown="block">

$$\frac {\partial}{\partial w_j} J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} ) = 0$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$\sum _{i=1} ^{n} f(\boldsymbol{x}_i) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$\sum _{i=1} ^{n} (w_0 x_{i0} + w_1 x_{i1} + \cdots + w_d x_{id}) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$w_0 \sum _{i=1} ^{n} x_{i0} x_{ij} + w_1 \sum _{i=1} ^{n} x_{i1} x_{ij} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$\begin{cases}
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i0} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{i0} = \sum _{i=1} ^{n} y_i \cdot x_{i0}\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i1} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{i1} = \sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad \vdots\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{id} + w_1 \sum _{i=1} ^{n} x_{i1} x_{id} + \cdots + w_d \sum _{i=1} ^{n} x_{id} x_{id} = \sum _{i=1} ^{n} y_i \cdot x_{id}\\
\end{cases}$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{i0} \\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{i1} \\
\vdots & \vdots & \ddots & \vdots\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{id} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{id} & \cdots & \displaystyle\sum _{i=1} ^{n} x_{id} x_{id} \\
\end{array} \right]

\cdot

\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]

=

\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\vdots\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{id}\\
\end{array} \right]
$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$(X^\intercal \cdot X) \cdot
\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]
=(X^\intercal \cdot Y)$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$
∴\,\left[ \begin{array}\\
w_0\\
w_1\\
\vdots\\
w_d\\
\end{array} \right]
= (X^\intercal \cdot X)^{-1} \cdot (X^\intercal \cdot Y)
$$

</div>

상술했던 대로, 단순 선형 회귀 모델의 결과값과 동일하다.

## 수치 계산법

분석적 풀이법이 그랬듯이, 다중 선형 회귀 모델의 수치 계산법도 단순 선형 회귀 모델의 것과 같다.

오차 함수 $J(\boldsymbol{w})$의 그라디언트를 계산하면

<div class="mathjax-wrapper" markdown="block">

$$
\begin{align}
\nabla J(\boldsymbol{w})

&= \left[ \begin{array}\\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i0} ) \\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i1} ) \\
\vdots\\
\displaystyle\frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{id} )
\end{array} \right]\\[5pt]


&= -\frac {2}{n}
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1d} & x_{2d} & \cdots & x_{nd} \\
\end{array} \right]

\cdot

\left[ \begin{array}\\
y_1 - f(\boldsymbol{x}_1)\\
y_2 - f(\boldsymbol{x}_2)\\
\vdots\\
y_n - f(\boldsymbol{x}_n)\\
\end{array} \right]\\[5pt]

&= -\frac {2}{n} \, X^\intercal \cdot (Y - f(X))

\end{align}
$$

</div>

따라서 다음 업데이트 식을 얻을 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} - \eta \, \left[ -\frac {2}{n} \, X^\intercal \cdot (Y - f(X)) \right]$$

</div>

# 문제 풀이

주어진 데이터가 한정적이므로, 전체 506개 데이터를 모두 학습에 사용하기보다는 그 중 일부를 떼어 학습용 데이터(training data)와 검증용 데이터(validation data)로 분리해서 사용하는 것이 좋다. 이번 문제풀이에서는 406개 데이터를 학습용 데이터로 사용하고, 100개 데이터를 검증용 데이터로 사용하기로 하였다.

## 분석적 풀이법

{% highlight python linenos %}

from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

def accuracy(predict_y, y):
    return np.mean(np.square((y - predict_y)))

def status(w):
    w_ = np.squeeze(w)
    print("w = [", end="")
    for i in range(14):
        if(i == 13):
            print(w_[i], end="]")
        else:
            print(w_[i], end=", ") 
    print()
    
    training_predict_y = training_x @ w
    validation_predict_y = validation_x @ w
    
    print("accuracy [training] : " + str(accuracy(training_predict_y, training_y)))
    print("accuracy [validation] : " + str(accuracy(validation_predict_y, validation_y)))
    
    print("{:^40s}|{:^40s}".format("training", "validation"))
    print("{:^20s}{:^20s}|{:^20s}{:^20s}".format("predict_y", "true_y", "predict_y", "true_y"))
    for i in range(10):
        print("{:^20f}{:^20f}|{:^20f}{:^20f}".format(float(training_predict_y[i]), float(training_y[i]), float(validation_predict_y[i]), float(validation_y[i])))
    print()
    
np.random.seed(2020)
    
# data
dataset = datasets.load_boston()
x = dataset.data
y = dataset.target

# reformat the data
x_ = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1) # x0 = 1인 열 추가
y_ = np.expand_dims(y, axis=1)

# divide data into training set and validation set
training_x = x_[0:406, :]
training_y = y_[0:406, :]

validation_x = x_[406:506, :]
validation_y = y_[406:506, :]

# random w
w = np.random.rand(x_.shape[1], 1) * 100
print("Before Training...")
status(w)

# calculate w
w = np.linalg.inv(training_x.T @ training_x) @ (training_x.T @ training_y)

# validating
print("After Training...")
status(w)

{% endhighlight %}

### 실행 결과

{% include folder.html filename="multiple-linear-regression-analytic-result.txt" mode="raw-txt" show_msg="출력 결과 보기" hide_msg="출력 결과 숨기기" %}

## 수치 계산법

### SGD


#### 실행 결과

### Mini-Batch GD

#### 실행 결과

### BGD

#### 실행 결과