---
title: "정규화 (Normalization)"
order: 13
date: "2020-06-01"
---

# 정규화의 필요성

## 비정규화된 데이터 (Unnormalized Data)

다음 데이터 셋을 보자.

| $x_1$ | $x_2$  |       |  $y$  |
| :---: | :----: | :---: | :---: |
| 4000  | 0.0005 |       |   5   |
| 4000  | 0.0009 |       |   9   |
| 8000  | 0.0004 |       |   4   |
| 8000  | 0.0001 |       |   1   |
| 7000  | 0.0006 |       |   6   |
| 6000  | 0.0005 |       |   5   |
| 8000  | 0.0002 |       |   2   |
| 2000  | 0.0004 |       |   4   |
| 3000  | 0.0005 |       |   5   |
| 4000  | 0.0005 |       |   5   |

이 데이터 셋은 독립변수 $x_1$, $x_2$와 종속변수 $y$에 대해,

- $y = 0 \cdot x_1 + 10000 \cdot x_2$
- $x_1 \in [1000, 9000]$ 
- $x_2 \in [0.0001, 0.0009]$ 

의 관계가 성립하도록 만든 데이터이다. $x_1$과 $x_2$의 값의 범위 차이가 큰 것을 볼 수 있는데, 이렇게 데이터의 독립 변수(feature) 간 값의 범위의 차이가 큰 경우 데이터가 **비정규화되었다(unnormalized)**고 표현한다.

이제 이 데이터에 대해 다중 선형 회귀 모델을 만드는 상황을 생각해보자. 편의상 상수항(bias)은 없다고 하면 다음과 같이 $\boldsymbol{w} [w_1, w_2]$를 파라미터로 가지는 모델을 세울 수 있다.



$$y = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = w_1 x_1 + w_2 x_2 $$



모델이 위 데이터에 대해 정상적으로 학습된다면 $\boldsymbol{w} = [0, 10000]$을 학습할 것이다. 과연 그런지 알아보자.

우선 해석적 풀이법을 적용하면 다음과 같다.

{% highlight python linenos %}
import numpy as np

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.linalg.inv(x.T @ x) @ (x.T @ y)
print("[w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-analytic-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

실행 결과 우리가 원하는 대로 $\boldsymbol{w} = [0, 10000]$를 학습함을 확인할 수 있다.

이젠 경사 하강법을 적용해보자. BGD를 이용하여 학습률 0.001로 100에폭을 학습시켜 보았다.

{% highlight python linenos %}
import numpy as np

np.random.seed(2020)

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.random.randn(2, 1)

epochs = 100
lr = 0.001

data_num = x.shape[0]
print("Initial : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))

for epoch in range(epochs):
    predict_y = x @ w

    # calculate gradient
    gradient = -(2 / data_num) * x.T @ (y - predict_y)

    # update w
    w = w - lr * gradient

    print("Epoch %03d : [w1, w2] = [%f, %f]" % (epoch + 1, w[0, 0], w[1, 0]))

print("Result : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-bgd-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

보이는 바와 같이 파라미터들이 수렴하지 않고 발산해 버린다. 발산하지 않는 작은 학습률을 찾아 학습시켜보면 학습이 진행되지 않는다.

해석적 풀이법은 수학적으로 최적해를 찾는 풀이법이기에 비정규화된 데이터에 대해서도 잘 작동한다. 하지만 위에서 보였듯이 경사 하강법은 비정규화된 데이터에 대해 잘 작동하지 않는다.[^1] 왜 이런 현상이 일어나는 것일까?

[^1]: 그라디언트의 크기가 너무 커져 발산하거나(gradient exploding), 그라디언트의 크기가 너무 작아져 학습이 진행되지 않는다(gradient vanishing).

위 경사 하강법에서는 오차 함수로 [평균 제곱 오차](/machine_learning/03-supervised-learning#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용하고 있다. 평균 제곱 오차의 그라디언트는 다음과 같다.



$$
\nabla J(\boldsymbol{w}) = \left[ \begin{array}\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{array} \right]
$$



이로부터 $w_1$, $w_2$의 변화량 $\Delta w_1$, $\Delta w_2$를 다음과 같이 표현할 수 있다.[^2]

[^2]: 경사 하강법에서의 파라미터 업데이트 식 참고 : $\boldsymbol{w_{new}} = \boldsymbol{w_{old}} - \eta \nabla \boldsymbol{w_{old}}$



$$
\begin{cases}\\
\Delta w_1 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\Delta w_2 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{cases}
$$



주어진 데이터에서 $x_1$은 $x_2$에 비해 값이 약 10,000,000배 크므로, $\Delta w_1$이 $\Delta w_2$에 비해 10,000,000배 크다. 이런 $\Delta w_1$, $\Delta w_2$를 가지고 파리미터 업데이트를 한다면 어떤 일이 일어날까? $w_1$이 잘 학습되도록 학습률 $\eta$를 작게 하면 $\Delta w_2$가 너무 작아져 $w_2$는 거의 학습이 되지 않을 것이고, $w_2$가 잘 학습되게 $\eta$를 크게 하면 $\Delta w_1$이 너무 커져 $w_1$은 발산해버릴 것이다.

## 정규화 (Normalization)

사실 비정규화된 데이터가 잘 학습되지 않는 이 문제는 오차 함수의 종류나 모델의 종류에 상관없이 발생한다. 이 문제는 경사 하강법이 가지고 있는 본질적인 한계라 볼 수 있다. 따라서 경사 하강법을 적용하기 전 데이터들이 비슷한 범위의 값을 가지게끔 조정해 줄 필요가 있는데, 이를 **정규화(Normalization)**[^3]라 한다.

[^3]: feature scaling이라고도 한다.

정규화를 잘 수행하면 학습이 안정적이면서도 빠르게 진행된다. 다만 조심할 것은 정규화 과정에서 각 데이터간 차이, 즉 데이터의 분포 상태는 왜곡하면 안된다는 것이다.

모든 데이터 셋에 정규화를 적용할 필요는 없다. 주어진 데이터 셋을 보고 상황에 맞춰 판단하면 되는 것이다.

# 정규화의 종류

정규화 방법으로는 최대-최소 정규화(Min-Max Normalization)과 z-점수 정규화(z-score Normalization)가 주로 많이 사용된다.

## 최대-최소 정규화 (Min-Max Normalization)[^4]

[^4]: 최대-최소 스케일링(Min-Max Scaling), 0-1 변환이라고도 한다.

최대-최소 정규화는 모든 데이터들을 [0, 1] 범위 안으로 만드는 정규화 방법으로, 가장 많이 사용되는 정규화 방법이다. 데이터 $\boldsymbol{x} = [x_1, x_2, \cdots, x_i, \cdots, x_n]$이 주어졌을 때, 최대-최소 정규화를 적용한 결과 $\boldsymbol{x}'$의 한 요소 $x_i'$은 다음과 같이 계산할 수 있다.



$$x_i' = \frac {x_i - \textrm{min}(\boldsymbol{x})} {\textrm{max}(\boldsymbol{x}) - \textrm{min}(\boldsymbol{x})}$$



최대-최소 정규화는 구현이 간단하고 연산이 빠른 장점이 있지만, 데이터의 이상치(Outlier, Noise)에 영향을 크게 받는다는 단점이 있다. 다음과 같이 이상치가 존재하는 데이터가 주어졌다고 해 보자.

{% include caption-img.html src="normalization-outlier-original.png" title="Fig.01 이상치가 존재하는 데이터" description="빨간 점은 이상치(outlier)를 나타낸다." %}

이 데이터에 최대-최소 정규화를 적용하면 다음과 같이 된다.

{% include caption-img.html src="normalization-outlier-minmax.png" title="Fig.02 Fig.01 데이터에 최대-최소 정규화를 적용한 결과" %}

범위 [0, 1] 안에 표현된 $y$축의 데이터와는 다르게 $x$축의 데이터는 단 하나의 이상치 때문에 대부분 [0, 0.4] 안에 표현되어 학습이 잘 진행되지 않는다. 이상치가 있는 데이터에서 정규화를 수행해야 한다면 이상치를 제거하고 최대-최소 정규화를 적용하던가, 아니면 아래의 [z-점수 정규화](#kramdown_z-점수-정규화-z-score-normalization)를 사용하는 것이 좋다.

## z-점수 정규화 (z-score Normalization)[^5]

[^5]: 표준화(Standardization)라고도 한다.

z-점수 정규화는 전체 데이터의 평균을 0, 표준편차를 1로 만드는 정규화 방법이다. 평균이 $m$, 표준편차가 $\sigma$인 데이터 $\boldsymbol{x} = [x_1, x_2, \cdots, x_i, \cdots, x_n]$이 주어졌을 때, z-점수 정규화를 적용한 결과 $\boldsymbol{x}'$의 한 요소 $x_i'$은 다음과 같이 계산할 수 있다.



$$x_i' = \frac {x_i - m} {\sigma}$$



z-점수 정규화는 비교적 이상치에 영향을 적게 받는다. 이상치가 존재하는 Fig.01의 데이터에 z-점수 정규화를 적용하면 다음과 같이 된다.

{% include caption-img.html src="normalization-outlier-zscore.png" title="Fig.03 Fig.01 데이터에 z-점수 정규화를 적용한 결과" %}

$x$축, $y$축의 데이터 모두 대부분 범위 [-1.5, 1.5] 안에 표현되므로 학습이 잘 진행될 수 있다.


### z-점수 정규화로 스케일링이 되는 이유 : 체비셰프 부등식 (Chebyshev's Inequality)

정규화는 데이터들을 스케일링하기 위해 사용한다. 최대-최소 정규화가 데이터를 특정 범위([0, 1]) 안으로 스케일링한다는 것은 어렵지 않게 이해된다. 하지만 평균과 표준편차를 이용하는 z-점수 정규화가 데이터들을 특정 범위 안으로 반드시 스케일링시킬 수 있을까?

만약 데이터가 정규 분포를 따른다면, 범위 [-3, 3] 안에 99.7%의 데이터가 있는 것을 표준정규분포표로부터 쉽게 알 수 있다. 하지만 만약 데이터가 정규분포를 따르지 않는다면 어떨까?

데이터가 정규분포를 따르지 않는 경우에도 체비셰프 부등식(Chebyshev's Inequality)은 범위 [-4, 4] 안에 최소 94%의 데이터가 있는 것을 보장한다. 체비셰프 부등식은 다음과 같다.

> 임의의 확률변수 $X$의 평균이 $m$이고, 표준편차가 $\sigma$일 때, 다음이 성립한다.
> 
> 
>
> $$P(|X - m| \ge k \sigma ) \le \frac{1}{k^2}$$
>
> 

이 부등식으로부터 z-점수 정규화를 적용했을 때 특정 범위 안으로 데이터들을 스케일링할 수 있는 것을 어느 정도 보장할 수 있다.