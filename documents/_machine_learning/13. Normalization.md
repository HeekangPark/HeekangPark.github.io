---
title: "정규화 (Normalization)"
order: 13
date: "2020-06-01"
isWriting: true
---

# 정규화의 필요성

## 비정규화된 데이터 (Unnormalized Data)

다음 데이터 셋을 보자.

| $x_1$ | $x_2$  |       |  $y$  |
| :---: | :----: | :---: | :---: |
| 4000  | 0.0005 |       |   5   |
| 4000  | 0.0009 |       |   9   |
| 8000  | 0.0004 |       |   4   |
| 8000  | 0.0001 |       |   1   |
| 7000  | 0.0006 |       |   6   |
| 6000  | 0.0005 |       |   5   |
| 8000  | 0.0002 |       |   2   |
| 2000  | 0.0004 |       |   4   |
| 3000  | 0.0005 |       |   5   |
| 4000  | 0.0005 |       |   5   |

이 데이터 셋은 독립변수 $x_1$, $x_2$와 종속변수 $y$에 대해,

- $y = 0 \cdot x_1 + 10000 \cdot x_2$
- $x_1 \in [1000, 9000]$ 
- $x_2 \in [0.0001, 0.0009]$ 

의 관계가 성립하도록 만든 데이터이다. $x_1$과 $x_2$의 값의 범위 차이가 큰 것을 볼 수 있는데, 이렇게 데이터의 독립 변수(feature) 간 값의 범위의 차이가 큰 경우 데이터가 **비정규화되었다(unnormalized)**고 표현한다.

이제 이 데이터에 대해 다중 선형 회귀 모델을 만드는 상황을 생각해보자. 편의상 상수항(bias)은 없다고 하면 다음과 같이 $\boldsymbol{w} [w_1, w_2]$를 파라미터로 가지는 모델을 세울 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$y = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = w_1 x_1 + w_2 x_2 $$

</div>

모델이 위 데이터에 대해 정상적으로 학습된다면 $\boldsymbol{w} = [0, 10000]$을 학습할 것이다. 과연 그런지 알아보자.

우선 해석적 풀이법을 적용하면 다음과 같다.

{% highlight python linenos %}
import numpy as np

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.linalg.inv(x.T @ x) @ (x.T @ y)
print("[w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-analytic-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

실행 결과 우리가 원하는 대로 $\boldsymbol{w} = [0, 10000]$를 학습함을 확인할 수 있다.

이젠 경사 하강법을 적용해보자. BGD를 이용하여 학습률 0.001로 100에폭을 학습시켜 보았다.

{% highlight python linenos %}
import numpy as np

np.random.seed(2020)

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.random.randn(2, 1)

epochs = 100
lr = 0.001

data_num = x.shape[0]
print("Initial : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))

for epoch in range(epochs):
    predict_y = x @ w

    # calculate gradient
    gradient = -(2 / data_num) * x.T @ (y - predict_y)

    # update w
    w = w - lr * gradient

    print("Epoch %03d : [w1, w2] = [%f, %f]" % (epoch + 1, w[0, 0], w[1, 0]))

print("Result : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-bgd-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

보이는 바와 같이 파라미터들이 수렴하지 않고 발산해 버린다. 발산하지 않는 작은 학습률을 찾아 학습시켜보면 학습이 진행되지 않는다.

해석적 풀이법은 수학적으로 최적해를 찾는 풀이법이기에 비정규화된 데이터에 대해서도 잘 작동한다. 하지만 위에서 보였듯이 경사 하강법은 비정규화된 데이터에 대해 잘 작동하지 않는다.[^1] 왜 이런 현상이 일어나는 것일까?

[^1]: 그라디언트의 크기가 너무 커져 발산하거나(gradient exploding), 그라디언트의 크기가 너무 작아져 학습이 진행되지 않는다(gradient vanishing).

위 경사 하강법에서는 오차 함수로 [평균 제곱 오차](/machine_learning/03-supervised-learning#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용하고 있다. 평균 제곱 오차의 그라디언트는 다음과 같다.

<div class="mathjax-wrapper" markdown="block">

$$
\nabla J(\boldsymbol{w}) = \left[ \begin{array}\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{array} \right]
$$

</div>

이로부터 $w_1$, $w_2$의 변화량 $\Delta w_1$, $\Delta w_2$를 다음과 같이 표현할 수 있다.[^2]

[^2]: 경사 하강법에서의 파라미터 업데이트 식 참고 : $\boldsymbol{w_{new}} = \boldsymbol{w_{old}} - \eta \nabla \boldsymbol{w_{old}}$

<div class="mathjax-wrapper" markdown="block">

$$
\begin{cases}\\
\Delta w_1 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\Delta w_2 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{cases}
$$

</div>

주어진 데이터에서 $x_1$은 $x_2$에 비해 값이 약 10,000,000배 크므로, $\Delta w_1$이 $\Delta w_2$에 비해 10,000,000배 크다. 이런 $\Delta w_1$, $\Delta w_2$를 가지고 파리미터 업데이트를 한다면 어떤 일이 일어날까? $w_1$이 잘 학습되도록 학습률 $\eta$를 작게 하면 $\Delta w_2$가 너무 작아져 $w_2$는 거의 학습이 되지 않을 것이고, $w_2$가 잘 학습되게 $\eta$를 크게 하면 $\Delta w_1$이 너무 커져 $w_1$은 발산해버릴 것이다.

## 정규화 (Normalization)

사실 이 문제는 오차 함수의 종류나 모델의 종류에 상관없이 발생한다. 어떤 오차 함수를 사용하는지, 어떤 모델을 사용하는지에 따라 경중이 달라질 수는 있겠지만, 본질적으로 학습이 잘 되지 않는다는 점은 동일하다. 이 문제는 경사 하강법이 가지고 있는 본질적인 한계라 볼 수 있다.

따라서 경사 하강법을 적용하기 전 데이터들이 비슷한 범위의 값을 가지게끔 조정해 줄 필요가 있다. 이를 **정규화(Normalization)**[^3]라 하는데, 중요한 것은 각 데이터간 차이, 즉 데이터의 분포 상태는 왜곡하면 안된다는 것이다.

[^3]: feature scaling이라고도 한다.

정규화를 잘 수행하면 학습이 안정적이면서도 빠르게 되게 할 수 있다. 이때 모든 데이터 셋에 정규화를 적용할 필요는 없다. 주어진 데이터 셋을 보고 상황에 맞춰 판단하면 되는 것이다.

# 정규화의 종류

일반적으로 다음 두 가지가 정규화 방법으로 많이 사용된다.

## 