---
title: "단순 선형 회귀 (Simple Linear Regression)"
order: 8
date: "2020-05-14"
---

지도학습으로 풀 수 있는 문제 중 하나인 단순 선형 회귀(Simple Linear Regression Model) 문제를 풀어보자.

# 문제 상황

독립변수 $x$와 종속변수 $y$에 대해 다음과 같이 데이터가 주어졌다고 하자.

|  $x$  |  $y$  |       |  $x$  |  $y$  |
| :---: | :---: | :---: | :---: | :---: |
| 2.96  | 4.12  |       | 8.00  | 6.28  |
| 1.59  | 3.05  |       | 4.22  | 5.02  |
| 5.18  | 5.01  |       | 5.57  | 5.03  |
| 1.18  | 3.19  |       | 4.75  | 4.75  |
| 1.69  | 3.40  |       | 7.96  | 6.43  |
| 1.80  | 3.72  |       | 4.47  | 4.24  |
| 3.04  | 4.21  |       | 2.67  | 3.75  |
| 3.11  | 3.76  |       | 5.48  | 5.55  |
| 0.80  | 2.58  |       | 8.77  | 6.95  |
| 4.77  | 4.46  |       | 2.72  | 4.35  |
| 2.81  | 3.77  |       | 2.97  | 3.99  |
| 9.74  | 7.58  |       | 7.77  | 6.60  |
| 4.10  | 4.48  |       | 5.85  | 5.70  |
| 2.50  | 3.67  |       | 7.01  | 6.07  |
| 6.36  | 5.69  |       | 2.05  | 3.47  |
| 0.02  | 2.57  |       | 8.19  | 7.00  |
| 0.08  | 2.27  |       | 8.80  | 7.22  |
| 9.76  | 7.40  |       | 9.47  | 7.09  |
| 1.55  | 3.10  |       | 3.73  | 4.84  |
| 3.60  | 4.18  |       | 2.50  | 3.63  |

{% include caption-img.html src="simple-linear-regression-data.png" title="Fig.01 산점도" description="독립변수 $x$와 종속변수 $y$에 대한 산점도" %}

# 모델 설계 : 단순 선형 회귀 모델 (Simple Linear Regression Model)

산점도에서 볼 수 있듯이 $x$, $y$ 간에는 아주 강한 선형 상관관계가 있다. 이를 바탕으로 다음과 같이 모델을 세울 수 있다.

$$y = f(x) = w_1 x + w_0$$

이 모델을 단순 선형 회귀 모델(Simple Linear Regression Model)이라 한다. 단순 선형 회귀 모델은 독립 변수와 종속 변수가 1개씩 있고, 모델의 차수가 일차식인 모델이다.

위 모델을 다음과 같이 쓸 수도 있다.

$$y = f(x) = w_1 x_1 + w_0 x_0$$

위와 같이 $x$를 $x_1$으로 (이름을) 바꾸고, 항상 1인 변수 $x_0$를 추가한다. 이렇게 하면 이 모델을 다음과 같이 이해할 수 있다.

$$y = f(x) = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = \begin{bmatrix} w_0\\w_1\end{bmatrix} ^\intercal  \cdot \begin{bmatrix} x_0\\x_1\end{bmatrix}$$

# 파리미터 최적화

## 오차 함수 : 평균 제곱 오차(MSE, Mean of Squared Error)

단순 선형 회귀 모델의 파라미터를 최적화하기 위해서는 [평균 제곱 오차(MSE, Mean of Squared Error)]({{ site.url }}{{ site.baseurl }}/machine_learning/03-supervised-learning#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용한다.

총 $n$쌍의 입력 데이터-출력 데이터(레이블) 쌍 ($\boldsymbol{x}_i$, $y_i$) ($i=1, 2, \cdots, n)$가 있는 데이터 셋과 모델 $f$가 주어졌을 때, 모델에 대한 평균 제곱 오차 $J(\boldsymbol{w})$는 다음과 같이 정의된다.

$$J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(x_i))^2$$

## 분석적 풀이법 (Analytic Solution)

$J(\boldsymbol{w})$를 $j$번째 파라미터 $w_j$에 대해 편미분하면 다음과 같다. ($j = 0, 1$)

$$
\begin{align}
\frac {\partial}{\partial w_j} J(\boldsymbol{w}) 
&= \frac {\partial}{\partial w_j} \left( \frac {1}{n} \sum _{i=1} ^{n} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( \frac {\partial}{\partial w_j} (y_i - f(\boldsymbol{x}_i))^2 \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} f(\boldsymbol{x}_i) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} \left( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -\frac {\partial}{\partial w_j} (w_0x_{i0} + w_1x_{i1}) \right) \\[5pt]
&= \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} )\\
\end{align}
$$

파라미터를 최적화한다는 것은 $J(\boldsymbol{w})$를 최소로 만드는 $\boldsymbol{w}$를 찾는 것이다. 이 문제는 그라디언트의 성질에 의해 모든 $j=0,\, 1$에 대해 $\partial J / \partial w_j$를 0으로 만드는(최소화하는) $\boldsymbol{w}$을 찾는 문제로 바꿀 수 있다. 즉,

$$\frac {\partial}{\partial w_j} J(\boldsymbol{w}) = \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{ij} ) = 0$$

를 최소화하는 $\boldsymbol{w} = [w_0, w_1]$을 찾으면 된다.

이 식을 변형하면

$$\sum _{i=1} ^{n} f(\boldsymbol{x}_i) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

$$\sum _{i=1} ^{n} (w_0 x_{i0} + w_1 x_{i1}) \cdot x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

$$w_0 \sum _{i=1} ^{n} x_{i0} x_{ij} + w_1 \sum _{i=1} ^{n} x_{i1} x_{ij} = \sum _{i=1} ^{n} y_i \cdot x_{ij}$$

이므로, 우리는 $j$에 0과 1을 대입하여 다음 연립방정식을 얻을 수 있다.

$$\begin{cases}
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i0} = \sum _{i=1} ^{n} y_i \cdot x_{i0}\\
w_0 \displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} + w_1 \sum _{i=1} ^{n} x_{i1} x_{i1} = \sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{cases}$$

이를 행렬의 곱 형태로 표현하면

$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1}\\
\end{array} \right]

\cdot

\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]

=

\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{array} \right]
$$

가 된다.

이때, 

$$X = \left[ \begin{array}\\
x_{10} & x_{11}\\
x_{20} & x_{21}\\
\vdots & \vdots\\
x_{n0} & x_{n1}\\
\end{array} \right],\quad Y = \left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]$$

라 하면

$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i0} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i0}\\
\displaystyle\sum _{i=1} ^{n} x_{i0} x_{i1} & \displaystyle\sum _{i=1} ^{n} x_{i1} x_{i1}\\
\end{array} \right]
=
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\end{array} \right]
\cdot
\left[ \begin{array}\\
x_{10} & x_{11}\\
x_{20} & x_{21}\\
\vdots & \vdots\\
x_{n0} & x_{n1}\\
\end{array} \right]
= X^\intercal \cdot X
$$

$$
\left[ \begin{array}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i0}\\
\displaystyle\sum _{i=1} ^{n} y_i \cdot x_{i1}\\
\end{array} \right]
=
\left[ \begin{array}\\
x_{10} & x_{20} & \cdots & x_{n0} \\
x_{11} & x_{21} & \cdots & x_{n1} \\
\end{array} \right]
\cdot
\left[ \begin{array}\\
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{array} \right]
= X^\intercal \cdot Y
$$

이므로, 다음과 같이 정리할 수 있다.

$$(X^\intercal \cdot X) \cdot
\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]
=(X^\intercal \cdot Y)$$

이를 풀면

$$
\left[ \begin{array}\\
w_0\\
w_1\\
\end{array} \right]
= (X^\intercal \cdot X)^{-1} \cdot (X^\intercal \cdot Y)
$$

가 된다.

이제 주어진 데이터에 대해 이 식을 적용하면 $J(\boldsymbol{w})$를 최소로 만드는 $\boldsymbol{w} = [w_0, w_1]$를 구할 수 있다.

## 수치 계산법 (Numerical Solution)

[분석적 풀이법](#kramdown-분석적-풀이법-analytic-solution)에서 구했다시피 오차 함수 $J(\boldsymbol{w})$의 그라디언트는 다음과 같다.

$$\nabla J(\boldsymbol{w}) = \left[ \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i0} ), \, \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i1} ) \right]$$

이 식을 바탕으로 다음 업데이트 식을 얻을 수 있다.

$$\boldsymbol{w}_{new} = \boldsymbol{w}_{old} - \eta \left[ \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i0} ), \, \frac {1}{n} \sum _{i=1} ^{n} ( 2 (y_i - f(\boldsymbol{x}_i)) \cdot -x_{i1} ) \right] $$

### 미니배치 경사 하강법 (Mini-Batch Gradient Descent Method)

지난번 [단순분류]({{ site.url }}{{ site.baseurl }}/machine_learning/05-simple-classification#kramdown_배치-경사-하강법-bgd-batch-gradient-descent-method-확률적-경사-하강법-sgd-stochastic-gradient-descent-method) 문서에서 언급했다시피, 데이터 전체를 살펴보면서 데이터 전체에 대해 한 번 그라디언트를 계산해 파라미터를 업데이트하는 배치 경사 하강법(BGD, Batch Gradient Descent Method)과, 데이터 각각에 대해 그라디언트를 계산해 파라미터를 여러 번 업데이트하는 확률적 경사 하강법(SGD, Stochastic Gradient Descent Method)이 있다. 배치 경사 하강법은 학습이 안정적으로 진행되지만, 데이터가 너무 클 경우 물리적으로 시행 불가능할 수도 있고, 또 느리다. 반면 확률적 경사 하강법은 빠르지만 학습 안정성이 떨어진다. 그렇다면 이들을 절충할 수는 없을까? 이 둘을 절충하여 만들어진 방법이 미니배치 경사 하강법(Mini-Batch Gradient Descent Method)이다.

배치 경사 하강법은 전체 데이터에 대해 그라디언트를 계산하고 파라미터 업데이트를 진행하고, 확률적 경사 하강법은 데이터 각각에 대해 그라디언트 계산 후 파라미터 업데이트를 진행한다면, 미니배치 경사 하강법은 전체 데이터를 몇 개의 뭉치로 나눈 후 그 뭉치에 대해 그라디언트를 계산하고 파라미터 업데이트를 진행한다. 이때 데이터 뭉치의 크기(한 데이터 뭉치 안의 데이터 수)를 배치 크기(batch size)라 부른다. 배치 크기가 1이면 미니배치 경사 하강법은 확률적 경사 하강법과 같아지고, 배치 크기가 전체 데이터의 크기이면 미니배치 경사 하강법은 배치 경사 하강법과 같아진다.

참고로 헷갈릴 수 있는 몇 가지 용어를 정리하면 다음과 같다.

- 에폭(에포크, epoch) : 한 에폭을 돌았다는 것은 모델이 데이터 전체를 한 번 훑었다는 것이다. 한 에폭 동안 파라미터 업데이트가 몇 번 일어날지는 어떤 경사 하강법을 사용하는지에 따라 달라진다.[^1] 일반적으로 모델의 학습은 정해진 횟수[^2]만큼 에폭을 돌리는 식으로 진행된다. 한 에폭에는 한 번 이상 배치가 수행된다.
- 배치(batch) : 한 번 그라디언트를 계산하고 파라미터를 업데이트하는 작업, 또는 그 때 사용하는 데이터를 의미한다. 그 데이터의 수가 배치 크기(batch size)이다.
- 미니배치(mini-batch) : 미니배치 경사 하강법에서 한 번 그라디언트를 계산하고 파라미터를 업데이트하기 위해 사용하는 데이터를 의미한다. 배치(batch)라는 단어를 사용하는 것이 맞지만, 많은 경우 배치라 하면 전체 데이터를 의미하고, 미니배치 경사 하강법에서의 배치는 미니배치라 불린다.

[^1]: 배치 경사 하강법에서는 1번, 확률적 경사 하강법에서는 데이터의 총 개수만큼, 미니배치 경사 하강법에서는 그 사이 어딘가...
[^2]: 하이퍼파라미터 중 하나가 된다.

# 문제 풀이

## 분석적 풀이법

{% highlight python linenos %}

import numpy as np

def draw(w):
    plt.scatter(x, y, c='b')
    plt.xlim(0, 10)
    plt.ylim(-0.1, 10.1)
    plt.plot(range(-1, 11), [(w[0, 0] + w[1, 0] * x) for x in range(-1, 11)], c='r')
    plt.show()
    print(w)

# data
x = np.array([
    2.96, 8.00, 1.59, 4.22, 5.18, 5.57, 1.18, 4.75, 1.69, 7.96, 
    1.80, 4.47, 3.04, 2.67, 3.11, 5.48, 0.80, 8.77, 4.77, 2.72, 
    2.81, 2.97, 9.74, 7.77, 4.10, 5.85, 2.50, 7.01, 6.36, 2.05, 
    0.02, 8.19, 0.08, 8.80, 9.76, 9.47, 1.55, 3.73, 3.60, 2.50, 
])

y = np.array([
    4.12, 6.28, 3.05, 5.02, 5.01, 5.03, 3.19, 4.75, 3.40, 6.43,
    3.72, 4.24, 4.21, 3.75, 3.76, 5.55, 2.58, 6.95, 4.46, 4.35, 
    3.77, 3.99, 7.58, 6.60, 4.48, 5.70, 3.67, 6.07, 5.69, 3.47, 
    2.57, 7.00, 2.27, 7.22, 7.40, 7.09, 3.10, 4.84, 4.18, 3.63, 
])

# reformat the data
x_ = np.concatenate((np.ones((x.shape[0], 1)), np.expand_dims(x, axis=1)), axis=1) # x0 = 1인 열 추가
y_ = np.expand_dims(y, axis=1)

# calculate w
w = np.linalg.inv(x_.T @ x_) @ (x_.T @ y_)

# result
draw(w)

{% endhighlight %}

### 실행 결과

{% include caption-img.html src="simple-linear-regression-analytic-sol-result.png" title="Fig.02 분석적 풀이법 시행 결과" description="빨간 직선이 분석 결과. 직선이 데이터(파란 점)들의 분포를 잘 묘사하고 있는 것을 볼 수 있다. 아래 숫자는 추정된 $\boldsymbol{w}=[w_0, w_1]$을 나타낸다." %}

## 수치 계산법

### SGD

{% highlight python linenos %}

{% endhighlight %}

### 미니배치 경사 하강법

### BGD


### 실행 결과