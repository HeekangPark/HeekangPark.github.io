---
title: "오차 함수 (Error Functions)"
order: 4
date: "2020-06-13"
isWriting: true
---

오차 함수는 지도학습 과정에서 오차의 크기를 계산하는데 사용되는 함수로, 비용 함수(Cost Function), 손실 함수(Loss Function), 목표 함수(Objective Function)라고도 불린다. 풀고자 하는 지도학습 문제에 따라 적당한 오차 함수를 선택하면 된다.

# 회귀 문제에서의 오차 함수

총 $n$개의 입력-출력 데이터(레이블) 쌍 ($\boldsymbol{x}_i$, $y_i$)가 주어지고,($i = 1, 2, ..., n$) 이 데이터 셋에 대해 모델 $f$를 학습하는 상황이라 하자.

## 평균절대오차 (MAE, Mean of Absolute Error)

평균제곱오차는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - f(\boldsymbol{x}_i)|$$

</div>

이름 그대로 오차($y_i - f(\boldsymbol{x}_i)$)에 절대값을 씌워 모두 양수로 만든 후 평균을 구하는 함수이다.

## 평균제곱오차 (MSE, Mean of Squared Error)

평균제곱오차는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$ \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\boldsymbol{x}_i))^2 $$

</div>

이름 그대로 오차($y_i - f(\boldsymbol{x}_i)$)를 제곱하여 모두 양수로 만든 후 평균을 구하는 함수이다.

일반적으로 오차 함수로 평균절대오차보타 평균제곱오차를 더 많이 사용하는데, 그 이유는 다음과 같다.

- 해석적 풀이법에서는 오차 함수를 미분해야 한다. 평균절대오차는 전구간 미분 가능하지 않다.
- 수치 계산법에서 오차 함수는 현재 단계에서 문제 상황을 알려주는 역할을 한다. $y_i$와 $f(\boldsymbol{x}_i)$의 값이 차이가 많이 날수록 문제가 큰 상황이라 할 수 있는데, 두 값 간의 오차 $y_i - f(\boldsymbol{x}_i)$의 크기가 커질수록 값을 과장하는 평균제곱오차가 평균절대오차에 비해 문제 상황을 더 잘 알려준다. 문제를 잘 알려줄수록 문제가 해결될 가능성이 커지기에, 평균제곱오차를 사용할 때가 평균절대오차를 사용할 때에 비해서 일반적으로 학습이 더 잘 된다.

# 분류 문제에서의 오차 함수

## 크로스 엔트로피 (Cross Entropy)

크로스 엔트로피를 이해하려면 정보 이론(Information Theory)을 조금 알아야 한다.

### 정보량 (Information Quantitation)

일상적으로 사용하는 정보(Information)는 추상적인 개념이지만, 공학적인 접근을 위해서는 이를 정량화해야 한다.

다음 두 사건이 주어졌다고 해 보자.

1. 페이커랑 초등학생이 롤 1:1 대결을 했는데, 페이커가 이겼다.
2. 페이커랑 초등학생이 롤 1:1 대결을 했는데, 초등학생이 이겼다.

1번 사건은 일어날 확률이 매우 높은 사건이지만, 2번 사건은 일어날 확률이 매우 낮은 사건이다. 그 결과 우리는 1번 사건을 볼 때는 "흠, 그럴 수 있지." 하며 볼 테지만, 2번 사건을 볼 때는 "엥? 진짜?" 하며 볼 것이다. 2번 사건이 1번 사건에 비해 더 놀라운(surprising), 정보를 더 많이 가지고 있는(informative) 사건인 것이다. 즉, 잘 일어나지 않는 사건(unlikely event)은 더 많은 정보를 가지고 있다(informative).

정보 이론은 위 아이디어를 바탕으로 발생 확률이 $p(x)$인 사건 $x$의 정보량(Information Quantitation) $I(x)$를 다음과 같이 정의한다. 

<div class="mathjax-wrapper" markdown="block">

$$ I(x) = - \log_2 p(x) $$

</div>

정보량의 단위는 bit(Binary Information Unit)[^1][^2]를 사용한다. 이는 컴퓨터 저장 공간의 단위인 bit(Binary Digit)과는 다른 단위이다.[^3] 이 문서에서 이후 사용되는 bit는 Binary Information Unit의 bit이다.

[^1]: 정보 이론의 창시자 섀년(Claude E. Shannon)의 이름을 따 섀년(Shannon)이라 하기도 한다. (1Shannon == 1bit)
[^2]: 정보량을 밑이 $e$인 자연로그 $\ln$을 사용하여 정의할 수도 있는데($I(x) = - \ln p(x)$), 이 경우 단위를 nat라 한다. (1nat == 약 1.443bit)
[^3]: 사실 완전 관련없지는 않은데, 1 Binary Information Unit의 정보를 저장하기 위해서는 최소 1 Binary Digit만큼의 저장 공간이 필요하다.

1bit의 정보량은 일어날 확률이 0.5인 사건이 가지는 정보량이다. 또한 무조건 일어나지 않는 사건(발생 확률 0)의 정보량은 ∞bit이고, 무조건 일어나는 사건(발생 확률 1)인 사건의 정보량은 0bit이다.

정보량을 이용하면 두 사건 간 정보의 양을 정량적으로 비교할 수 있다. 예를 들어, 사건 "동전 4개를 던질 때, 이 중 1개가 앞면이다."(발생 확률 $\frac{1}{4}$)의 정보량은 2bit이다. 또 사건 "주사위 1개를 던질 때, 1이 나온다."(발생 확률 $\frac{1}{6}$)의 정보량은 약 2.58bit이다. 이로부터 사건 "주사위 1개를 던질 때, 1이 나온다."는 사건 "동전 4개를 던질 때, 이 중 1개가 앞면이다."보다 약 1.29배 정도 놀라운(정보량이 많은) 사건이라 말할 수 있다.

정보량의 중요한 성질 중 하나는 여러 독립 사건(independent event)들의 결합 사건(joint event)의 정보량이 각각의 독립 사건들의 정보량의 합과 같다는 점이다. 사건 "동전 4개와 주사위 1개를 던질 때, 동전 1개가 앞면이 나오고, 주사위는 1이 나온다"(발생 확률 $\frac{1}{24}$)을 보자. 이 사건의 정보량은 약 4.58bit로, 발생 확률로부터 직접 계산할 수도 있고($- \log_2 \frac{1}{24}$) 두 독립 사건의 정보량의 합으로 계산할 수도 있다(2bit + 2.58bit). 이게 가능한 이유는 정보량이 로그를 이용해 정의되었기 때문이다. 독립 사건들의 결합 사건의 발생 확률은 각 독립 사건들의 발생 확률의 곱과 같다. 그리고 로그는 곱을 합으로 바꾸기 때문에 이런 성질이 생긴다.

### 엔트로피 (Entropy)

엔트로피(Entropy)[^5]란 발생할 수 있는 모든 사건들에 대한 정보량의 기대값이다. 사건 $e_1$, $e_2$, …, $e_n$에 대해 각 사건 $e_i$($i = 1, 2, \cdots, n$)의 발생 확률이 $p(e_i)$라 할 때, 엔트로피 $H$는 다음과 같이 정의된다.

[^5]: 섀년 엔트로피(Shannon Entropy)라 하기도 한다.

<div class="mathjax-wrapper" markdown="block">

$$ H = \sum _{i=1} ^{n} p(e_i) I(e_i) = -\sum _{i=1} ^{n} p(e_i) \log_2 p(e_i)$$

</div>

예를 들어, 앞면과 뒷면이 나올 확률이 각각 $\frac{1}{2}$인 동전 2개를 던져 앞면이 나온 개수를 조사한다고 해 보자. 이때 발생 가능한 모든 사건의 종류와 그 확률은 다음과 같다.

- 앞면이 0개 나온 경우 : $\frac{1}{4}$
- 앞면이 1개 나온 경우 : $\frac{1}{2}$
- 앞면이 2개 나온 경우 : $\frac{1}{4}$

이 사건들의 엔트로피는 다음과 같이 계산된다.

<div class="mathjax-wrapper" markdown="block">

$$ H = (-\frac{1}{4} \log_2 \frac{1}{4}) + (-\frac{1}{2} \log_2 \frac{1}{2}) + (-\frac{1}{4} \log_2 \frac{1}{4}) = 1.5$$

</div>

엔트로피가 높다는 것은 사건들의 평균 정보량이 많다는 것으로, 예측이 어렵다(사건들 중 어떤 사건이 일어날 지 짐작하기 어렵다)는 뜻이다. 즉, 각 사건들의 발생 확률이 비슷할수록[^6] 엔트로피는 커진다. 반대로 엔트로피가 낮다는 것은 사건들의 평균 정보량이 적다는 것으로, 예측이 쉽다(사건들 중 어떤 사건이 일어날 지 짐작하기 쉽다)는 뜻이다. 즉, 특정 사건의 발생 확률이 높으면[^7] 엔트로피는 작아진다.

[^6]: 사건(의 확률) 분포가 균등할수록(uniform)
[^7]: 사건(의 확률) 분포가 결정적일수록(deterministic)

### 쿨백-라이블러 발산 (KLD, Kullback-Leibler Divergence)

