---
title: "오차 함수 (Error Functions)"
order: 4
date: "2020-06-13"
---

오차 함수는 지도학습 과정에서 오차의 크기를 계산하는데 사용되는 함수로, 비용 함수(Cost Function), 손실 함수(Loss Function), 목표 함수(Objective Function)라고도 불린다. 풀고자 하는 지도학습 문제에 따라 적당한 오차 함수를 선택하면 된다.

# 회귀 문제에서의 오차 함수

총 $n$개의 입력-출력 데이터(레이블) 쌍 ($\boldsymbol{x}_i$, $y_i$)가 주어지고,($i = 1, 2, ..., n$) 이 데이터 셋에 대해 모델 $f$를 학습하는 상황이라 하자.

## 평균절대오차 (MAE, Mean of Absolute Error)

평균제곱오차는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$ \textrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - f(\boldsymbol{x}_i)|$$

</div>

이름 그대로 오차($y_i - f(\boldsymbol{x}_i)$)에 절대값을 씌워 모두 양수로 만든 후 평균을 구하는 함수이다.

## 평균제곱오차 (MSE, Mean of Squared Error)

평균제곱오차는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$ \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\boldsymbol{x}_i))^2 $$

</div>

이름 그대로 오차($y_i - f(\boldsymbol{x}_i)$)를 제곱하여 모두 양수로 만든 후 평균을 구하는 함수이다.

일반적으로 오차 함수로 평균절대오차보타 평균제곱오차를 더 많이 사용하는데, 그 이유는 다음과 같다.

- 해석적 풀이법에서는 오차 함수를 미분해야 한다. 평균절대오차는 전구간 미분 가능하지 않다.
- 수치 계산법에서 오차 함수는 현재 단계에서 문제 상황을 알려주는 역할을 한다. $y_i$와 $f(\boldsymbol{x}_i)$의 값이 차이가 많이 날수록 문제가 큰 상황이라 할 수 있는데, 두 값 간의 오차 $y_i - f(\boldsymbol{x}_i)$의 크기가 커질수록 값을 과장하는 평균제곱오차가 평균절대오차에 비해 문제 상황을 더 잘 알려준다. 문제를 잘 알려줄수록 문제가 해결될 가능성이 커지기에, 평균제곱오차를 사용할 때가 평균절대오차를 사용할 때에 비해서 일반적으로 학습이 더 잘 된다.

# 분류 문제에서의 오차 함수

## 크로스 엔트로피 (Cross Entropy)

입력 데이터를 총 $n$개의 클래스 중 하나로 분류하는 문제에서, 정답 레이블의 원-핫 벡터(one-hot vector) $p$와 모델이 추정한 각 클래스별 확률 벡터 $q$에 대해, 크로스 엔트로피 $H(p, q)$는 다음과 같이 정의된다.

<div class="mathjax-wrapper" markdown="block">

$$H(p, q) = -\sum_{i=1} ^{n} p_i \log q_i$$

</div>

분류 모델은 일반적으로 이 크로스 엔트로피를 최소화하는 방향으로 학습하게 된다.

크로스 엔트로피를 이해하려면 정보 이론(Information Theory)을 조금 알아야 한다.

### 정보량 (Information Quantitation)

일상적으로 사용하는 정보(Information)는 추상적인 개념이지만, 공학적인 접근을 위해서는 이를 정량화해야 한다.

다음 두 사건이 주어졌다고 해 보자.

1. 페이커랑 초등학생이 롤 1:1 대결을 했는데, 페이커가 이겼다.
2. 페이커랑 초등학생이 롤 1:1 대결을 했는데, 초등학생이 이겼다.

1번 사건은 일어날 확률이 매우 높은 사건이지만, 2번 사건은 일어날 확률이 매우 낮은 사건이다. 그 결과 우리는 1번 사건을 볼 때는 "흠, 그럴 수 있지." 하며 볼 테지만, 2번 사건을 볼 때는 "엥? 진짜?" 하며 볼 것이다. 2번 사건이 1번 사건에 비해 더 놀라운(surprising), 정보를 더 많이 가지고 있는(informative) 사건인 것이다. 즉, 잘 일어나지 않는 사건(unlikely event)은 더 많은 정보를 가지고 있다(informative).

정보 이론은 위 아이디어를 바탕으로 발생 확률이 $p(x)$인 사건 $x$의 정보량(Information Quantitation) $I(x)$를 다음과 같이 정의한다. 

<div class="mathjax-wrapper" markdown="block">

$$ I(x) = - \log_2 p(x) $$

</div>

정보량의 단위는 bit(Binary Information Unit)[^1][^2]를 사용한다. 이는 컴퓨터 저장 공간의 단위인 bit(Binary Digit)과는 다른 단위이다.[^3] 이 문서에서 이후 사용되는 bit는 Binary Information Unit의 bit이다.

[^1]: 정보 이론의 창시자 섀년(Claude E. Shannon)의 이름을 따 섀년(Shannon)이라 하기도 한다. (1Shannon == 1bit)
[^2]: 정보량을 밑이 $e$인 자연로그 $\ln$을 사용하여 정의할 수도 있는데($I(x) = - \ln p(x)$), 이 경우 단위를 nat라 한다. (1nat == 약 1.443bit)
[^3]: 사실 완전 관련없지는 않은데, 1 Binary Information Unit의 정보를 저장하기 위해서는 최소 1 Binary Digit만큼의 저장 공간이 필요하다.

1bit의 정보량은 일어날 확률이 0.5인 사건이 가지는 정보량이다. 또한 무조건 일어나지 않는 사건(발생 확률 0)의 정보량은 ∞bit이고, 무조건 일어나는 사건(발생 확률 1)인 사건의 정보량은 0bit이다.

정보량을 이용하면 두 사건 간 정보의 양을 정량적으로 비교할 수 있다. 예를 들어, 사건 "동전 4개를 던질 때, 이 중 1개가 앞면이다."(발생 확률 $\frac{1}{4}$)의 정보량은 2bit이다. 또 사건 "주사위 1개를 던질 때, 1이 나온다."(발생 확률 $\frac{1}{6}$)의 정보량은 약 2.58bit이다. 이로부터 사건 "주사위 1개를 던질 때, 1이 나온다."는 사건 "동전 4개를 던질 때, 이 중 1개가 앞면이다."보다 약 1.29배 정도 놀라운(정보량이 많은) 사건이라 말할 수 있다.

정보량의 중요한 성질 중 하나는 여러 독립 사건(independent event)들의 결합 사건(joint event)의 정보량이 각각의 독립 사건들의 정보량의 합과 같다는 점이다. 사건 "동전 4개와 주사위 1개를 던질 때, 동전 1개가 앞면이 나오고, 주사위는 1이 나온다"(발생 확률 $\frac{1}{24}$)을 보자. 이 사건의 정보량은 약 4.58bit로, 발생 확률로부터 직접 계산할 수도 있고($- \log_2 \frac{1}{24}$) 두 독립 사건의 정보량의 합으로 계산할 수도 있다(2bit + 2.58bit). 이게 가능한 이유는 정보량이 로그를 이용해 정의되었기 때문이다. 독립 사건들의 결합 사건의 발생 확률은 각 독립 사건들의 발생 확률의 곱과 같다. 그리고 로그는 곱을 합으로 바꾸기 때문에 이런 성질이 생긴다.

### 엔트로피 (Entropy)

엔트로피(Entropy)[^5]란 발생할 수 있는 모든 사건들에 대한 정보량의 기대값이다. 사건 $e_1$, $e_2$, …, $e_n$에 대해 각 사건 $e_i$($i = 1, 2, \cdots, n$)의 발생 확률이 $p(e_i)$라 할 때, 엔트로피 $H$는 다음과 같이 정의된다.

[^5]: 섀년 엔트로피(Shannon Entropy)라 하기도 한다.

<div class="mathjax-wrapper" markdown="block">

$$ H = \sum _{i=1} ^{n} p(e_i) I(e_i) = -\sum _{i=1} ^{n} p(e_i) \log_2 p(e_i)$$

</div>

예를 들어, 앞면과 뒷면이 나올 확률이 각각 $\frac{1}{2}$인 동전 2개를 던져 앞면이 나온 개수를 조사한다고 해 보자. 이때 발생 가능한 모든 사건의 종류와 그 확률은 다음과 같다.

- 앞면이 0개 나온 경우 : $\frac{1}{4}$
- 앞면이 1개 나온 경우 : $\frac{1}{2}$
- 앞면이 2개 나온 경우 : $\frac{1}{4}$

이 사건들의 엔트로피는 다음과 같이 계산된다.

<div class="mathjax-wrapper" markdown="block">

$$ H = (-\frac{1}{4} \log_2 \frac{1}{4}) + (-\frac{1}{2} \log_2 \frac{1}{2}) + (-\frac{1}{4} \log_2 \frac{1}{4}) = 1.5$$

</div>

엔트로피가 높다는 것은 사건들의 평균 정보량이 많다는 것으로, 예측이 어렵다(사건들 중 어떤 사건이 일어날 지 짐작하기 어렵다)는 뜻이다. 즉, 각 사건들의 발생 확률이 비슷할수록[^6] 엔트로피는 커진다. 반대로 엔트로피가 낮다는 것은 사건들의 평균 정보량이 적다는 것으로, 예측이 쉽다(사건들 중 어떤 사건이 일어날 지 짐작하기 쉽다)는 뜻이다. 즉, 특정 사건의 발생 확률이 높으면[^7] 엔트로피는 작아진다.

[^6]: 사건(의 확률) 분포가 균등할수록(uniform)
[^7]: 사건(의 확률) 분포가 결정적일수록(deterministic)

### 쿨백-라이블러 발산 (KLD, KL Divergence, Kullback-Leibler Divergence)

주어진 사진을 4개의 클래스 `Bird`, `Cat`, `Dog`, `Cow` 중 하나로 분류하는 분류기(Classifier)를 생각해보자. 이 분류기는 입력 데이터가 4개의 클래스 각각에 속할 확률(confidence)을 출력한다. 그리고 이 중 가장 확률이 높은 클래스를 최종 분류 결과로 내놓는다.

{% include caption-img.html src="classifier-prediction.png" title="Fig.01 분류기" description="입력 데이터가 각 클래스에 속할 확률을 계산한 후, 가장 높은 확률을 가진 클래스로 예측(prediction)한다." %}

모델의 학습은 오차 함수(Error Function)을 최소화하는 식으로 이루어진다. 성공적인 학습을 위해서는 현재 모델의 예측값과 참값 사이의 차이를 잘 보여줄 수 있는 오차 함수를 사용해야 한다. 현재 Fig.01에서 분류기는 `Bird` 클래스의 사진을 `Cat`으로 잘못 분류하고 있는데, 이런 분류기에 적용할 수 있는 적절한 오차 함수로는 어떤 것이 있을까?

불행히도 분류 문제에서는 회귀 모델에서 했던 것처럼 출력된 최종 결과값과 참값 사이의 직접적인 차이를 계산할 수 없다.[^8] Fig.01의 모델은 입력 데이터가 `Bird`, `Cat`, `Dog`, `Cow` 클래스에 속할 확률을 각각 0.3, 0.6, 0.09, 0.01로 계산했다. 이를 [0.3, 0.6, 0.09, 0.01]과 같이 확률분포로 표현하자. 이때 참값은 [1, 0, 0, 0]이라 표현할 수 있다.[^9] 만약 두 확률분포 [0.3, 0.6, 0.09, 0.01]과 [1, 0, 0, 0] 간의 차이를 계산할 수 있는 함수가 있다면, 그 함수를 오차 함수로 사용할 수 있을 것이다.

[^8]: `Bird` - `Cat`을 계산할 수는 없지 않은가.
[^9]: 이런 식의 표기법을 원-핫 인코딩(one-hot encoding)라 하고, 이렇게 만들어진 벡터를 원-핫 벡터(one-hot vector)라 한다. $n$개의 클래스가 있을 때, 원-핫 인코딩은 크기 $n$짜리 벡터의 인덱스 $1$ ~ $n$에 클래스를 하나씩 매칭시킨 다음, 해당하는 클래스의 인덱스에는 1을, 나머지에는 0을 넣어 만들 수 있다.

쿨백-라이블러 발산(KLD, KL Divergence, Kullback-Leibler Divergence)[^10]이 바로 그런 함수이다. 쿨백-라이블러 발산을 이용하면 두 확률분포 간의 차이를 구할 수 있다. 두 확률분포 $p[p_1, p_2, \cdots, p_n]$, $q[q_1, q_2, \cdots, q_n]$에 대해 쿨백-라이블러 발산 $KL(p \Vert q)$, $KL(q \Vert p)$[^11]는 각각 다음과 같이 계산된다.

[^10]: 상대 엔트로피(Relative Entropy), Information Gain, Information Divergence 등으로도 불린다.
[^11]: $D_{KL}(p \Vert q)$로도 표기한다.

<div class="mathjax-wrapper" markdown="block">

$$KL(p \Vert q) = \sum _{i=1} ^{n} p_i \log \frac{p_i}{q_i}$$

</div>

<div class="mathjax-wrapper" markdown="block">

$$ KL(q \Vert p) = \sum _{i=1} ^{n} q_i \log \frac{q_i}{p_i}$$

</div>

쿨백-라이블러 발산은 다음과 같은 특징이 있다.

- 쿨백-라이블러 발산은 항상 0 이상이다[^12] : 만약 확률분포 $p$와 확률분포 $q$가 동일하다면 두 확률분포 간 쿨백-라이블러 발산은 0으로 최소값이 된다.
- 비대칭적(asymmetric)이다 : $KL(p \Vert q) \neq KL(q \Vert p)$이다. 쿨백-라이블러 발산을 두 확률분포 간 거리 개념으로 이해하기 쉬운데, 쿨백-라이블러 발산은 비대칭적이기에 거리 개념으로 사용할 수 없다.[^13]

[^12]: 증명은 젠슨 부등식(Jensen’s Inequality)을 이용해서 한다.
[^13]: 그래서 고안된 것이 젠슨-섀넌 발산(JSD, JS Divergence, Jensen-Shannon Divergence)이다. 두 확률분포 두 확률분포 $p[p_1, p_2, \cdots, p_n]$, $q[q_1, q_2, \cdots, q_n]$에 대해, 젠슨-섀넌 발산 $JSD(p, \, q)$는 $JSD(p, \, q) = \frac{1}{2} KL(p \Vert \frac{p + q}{2}) + \frac{1}{2} KL(q \Vert \frac{p + q}{2})$으로 정의된다. 젠슨-섀넌 발산은 대칭적(symmetric)이어서 거리 개념으로 사용해도 된다. 하지만 계산의 복잡성 때문인지 쿨백-라이블러 발산보다 잘 사용되지 않는다.

### 크로스 엔트로피 (Cross Entropy)

쿨백-라이블러 발산 식은 다음과 같이 두 개의 항으로 분리할 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$KL(p \Vert q) = \sum _{i=1} ^{n} p_i \log \frac{p_i}{q_i} = (-\sum_{i=1} ^{n} p_i \log q_i) - (-\sum_{i=1} ^{n} p_i \log p_i)$$

</div>

식을 자세히 보면 뒤의 항 $(-\displaystyle\sum_{i=1} ^{n} p_i \log p_i)$은 확률분포 $p$에 대한 엔트로피 $H(p)$이고, 앞의 항 $(-\displaystyle\sum_{i=1} ^{n} p_i \log q_i)$도 엔트로피와 식의 형태가 유사하나 로그 안의 값이 $p_i$가 아닌 $q_i$임을 확인할 수 있다.

이때, 앞의 항 $(-\displaystyle\sum_{i=1} ^{n} p_i \log q_i)$를 크로스 엔트로피(Cross Entropy)라 정의하고, $H(p, \, q)$와 같이 표기한다. 즉, 

<div class="mathjax-wrapper" markdown="block">

$$H(p, q) = -\sum_{i=1} ^{n} p_i \log q_i$$

</div>

이고, 쿨백-라이블러 발산은 다음과 같이 표현할 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$KL(p \Vert q) = H(p, q) - H(p)$$

</div>

Fig.01의 분류기 문제로 돌아가보자. $p$를 [1, 0, 0, 0]이라 하고 $q$를 [0.3, 0.6, 0.09, 0.01]라 하자. 두 확률분포 간의 차이를 계산하기 위해 쿨백-라이블러 발산을 이용한다. 이론적으로 두 확률분포 간의 차이를 계산하기 위해 $KL(p \Vert q)$과 $KL(q \Vert p)$ 둘 중 어느 것을 사용해도 상관없으나, $p$가 대부분의 항이 0인 원-핫 벡터이기 때문에 $KL(q \Vert p)$은 계산이 불가능하므로[^14] $KL(p \Vert q)$를 이용하여 두 확률분포 간 차이를 계산하자.

[^14]: 분모에 0이 들어가 계산이 불가능해진다. 

이때 $p$는 참값으로서, 바뀌지 않는다. 즉 $H(p)$의 값은 상수이다. 따라서 두 확률분포 간의 차이를 구할 때 크로스 엔트로피 $H(p, q)$만을 고려해도 충분하다.

결론적으로, 분류 문제의 오차 함수로 크로스 엔트로피를 사용할 수 있다.