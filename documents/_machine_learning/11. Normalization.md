---
title: "정규화 (Normalization)"
order: 11
date: "2020-06-01"
isWriting: true
---

# 정규화의 필요성

다음과 같은 데이터가 주어졌다고 해 보자.

| $x_1$ | $x_2$  |       |  $y$  |
| :---: | :----: | :---: | :---: |
| 4000  | 0.0005 |       |   5   |
| 4000  | 0.0009 |       |   9   |
| 8000  | 0.0004 |       |   4   |
| 8000  | 0.0001 |       |   1   |
| 7000  | 0.0006 |       |   6   |
| 6000  | 0.0005 |       |   5   |
| 8000  | 0.0002 |       |   2   |
| 2000  | 0.0004 |       |   4   |
| 3000  | 0.0005 |       |   5   |
| 4000  | 0.0005 |       |   5   |

이 데이터 셋의 독립변수 $x_1$, $x_2$와 종속변수 $y$ 사이에 $y = 0 \cdot x_1 + 10000 \cdot x_2$의 관계가 성립하도록 만든 데이터이다.[^1] 이제 이 데이터에 대해 다중 선형 회귀 모델을 만드는 상황을 생각해보자. 편의상 상수항(bias)은 없다고 하면 다음과 같이 $\boldsymbol{w} [w_1, w_2]$를 파라미터로 가지는 모델을 세울 수 있다.

[^1]: 즉 $x_1$은 아무런 역할도 하지 않는다.

<div class="mathjax-wrapper" markdown="block">

$$y = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = w_1 x_1 + w_2 x_2 $$

</div>

이 모델에 대해 위 데이터로 파라미터 최적화를 수행해보자. 모델이 정상적으로 학습된다면 $\boldsymbol{w} = [0, 10000]$을 학습할 것이다. 과연 그런지 알아보자.

## 해석적 풀이법

해석적 풀이법을 적용하면 다음과 같다.

{% highlight python linenos %}
import numpy as np

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.linalg.inv(x.T @ x) @ (x.T @ y)
print("[w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-analytic-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

실행 결과 우리가 원하는 대로 $\boldsymbol{w} = [0, 10000]$를 학습함을 확인할 수 있다.

## 수치 계산법

BGD를 이용하여 학습률 0.001로 100에폭을 학습시켜 보았다.

{% highlight python linenos %}
import numpy as np

np.random.seed(2020)

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.random.randn(2, 1)

epochs = 100
lr = 0.001

data_num = x.shape[0]
print("Initial : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))

for epoch in range(epochs):
    predict_y = x @ w

    # calculate gradient
    gradient = -(2 / data_num) * x.T @ (y - predict_y)

    # update w
    w = w - lr * gradient

    print("Epoch %03d : [w1, w2] = [%f, %f]" % (epoch + 1, w[0, 0], w[1, 0]))

print("Result : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-bgd-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

보이는 바와 같이 파라미터들이 수렴하지 않고 발산해 버린다. 발산하지 않는 작은 학습률을 찾아 학습시켜보면 학습이 진행되지 않는다.

왜 이런 일이 일어나는 것일까?

## 비정규화된 데이터 (Unnormalizaed Data)

주어진 데이터에서 $x_1$은 1000 ~ 9000 사이의 값을 가지는 값이고, $x_2$는 0.0001 ~ 0.0009 사이의 값을 가진다. 이렇게 데이터의 독립 변수(feature) 간 값의 범위의 차이가 큰 경우 데이터가 **비정규화되었다(unnormalized)**고 표현한다.

수학적으로 최적해를 찾는 해석적 풀이법은 비정규화된 데이터에 대해서도 잘 작동한다. 하지만 위에서 보였듯이 수치 계산법은 비정규화된 데이터에 대해 잘 작동하지 않는다.[^2] 왜 이런 현상이 일어나는 것일까?

[^2]: 그라디언트의 크기가 너무 커져 발산하거나(gradient exploding), 그라디언트의 크기가 너무 작아져 학습이 진행되지 않는다(gradient vanishing).

위 수치 계산법에서는 오차 함수로 [평균 제곱 오차](/machine_learning/03-supervised-learning#kramdown_평균제곱오차-mse-mean-of-squared-error)를 사용하고 있다. 평균 제곱 오차의 그라디언트는 다음과 같다.

<div class="mathjax-wrapper" markdown="block">

$$
\nabla J(\boldsymbol{w}) = \left[ \begin{array}\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{array} \right]
$$

</div>

이로부터 $w_1$, $w_2$의 변화량을 다음과 같이 표현할 수 있다.[^3]

[^3]: 경사 하강법에서의 파라미터 업데이트 식 : \boldsymbol{w_{new}} = \boldsymbol{w_{old}} - \eta \nabla \boldsymbol{w_{old}}

<div class="mathjax-wrapper" markdown="block">

$$
\begin{cases}\\
\Delta w_1 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i1} }\\
\Delta w_2 = -\eta \cdot \displaystyle\frac{1}{n} \sum _{i=1} ^{n} { 2(y_i - (w_1 x_{i1} + w_2 x_{i2})) \cdot -x_{i2} }\\
\end{cases}
$$

</div>

주어진 데이터에서 $x_1$은 $x_2$에 비해 값이 약 10,000,000배 크므로, $\Delta w_1$이 $\Delta w_2$에 비해 10,000,000배 크다. 이런 $\Delta w_1$, $\Delta w_2$를 가지고 파리미터 업데이트를 한다면 어떤 일이 일어날까? $w_1$이 잘 학습되도록 학습률 $\eta$를 작게 하면 $\Delta w_2$가 너무 작아져 $w_2$는 거의 학습이 되지 않을 것이고, $w_2$가 잘 학습되게 $\eta$를 크게 하면 $\Delta w_1$이 너무 커져 $w_1$은 발산해버릴 것이다.

이처럼 비정규화된 데이터에 경사 하강법을 적용할 경우 각각의 파라미터들의 학습 속도에 차이가 커 올바르게 학습되지 않는다. 따라서 경사 하강법을 적용하기 전 데이터들이 비슷한 범위의 값을 가지게끔 조정해 줄 필요가 있는데 이를 **정규화(Normalization)**라 한다. 이때 중요한 것은 각 데이터간 차이, 즉 데이터의 분포 상태는 왜곡하면 안된다는 것이다.

모든 데이터 셋에 정규화를 적용할 필요는 없다. 주어진 데이터 셋을 보고 상황에 맞춰 판단하면 되는 것이다.[^4]

[^4]: 물론 현실적으로 대부분의 데이터 셋의 독립 변수들은 다른 범위의 값을 가지기에, 학습 전에 정규화를 해야 한다.

# 정규화의 종류

다양한 정규화 방법 중 가장 많이 쓰이는 두 가지 방법이 있다.

