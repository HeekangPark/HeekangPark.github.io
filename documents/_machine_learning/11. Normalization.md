---
title: "정규화 (Normalization)"
order: 11
date: "2020-06-01"
isWriting: true
---

# 정규화의 필요성

다음과 같은 데이터가 주어졌다고 해 보자.

| $x_1$ | $x_2$  |       |  $y$  |
| :---: | :----: | :---: | :---: |
| 4000  | 0.0005 |       |   5   |
| 4000  | 0.0009 |       |   9   |
| 8000  | 0.0004 |       |   4   |
| 8000  | 0.0001 |       |   1   |
| 7000  | 0.0006 |       |   6   |
| 6000  | 0.0005 |       |   5   |
| 8000  | 0.0002 |       |   2   |
| 2000  | 0.0004 |       |   4   |
| 3000  | 0.0005 |       |   5   |
| 4000  | 0.0005 |       |   5   |

사실 이 데이터는 $y = 0 \cdot x_1 + 10000 \cdot x_2$의 관계식이 성립하도록 만든 데이터 셋이다. 이제 이 데이터에 대해 다중 선형 회귀 모델을 만드는 상황을 생각해보자. 편의상 상수항(bias)은 없다고 하면 다음과 같이 $\boldsymbol{w} [w_1, w_2]$를 파라미터로 가지는 모델을 세울 수 있다.

<div class="mathjax-wrapper" markdown="block">

$$y = \boldsymbol{w}^\intercal \cdot \boldsymbol{x} = w_1 x_1 + w_2 x_2 $$

</div>

이 모델에 대해 위 데이터로 파라미터 최적화를 수행하면 $\boldsymbol{w} = [0, 10000]$을 학습할 것이다. 과연 그런지 알아보자.

## 해석적 풀이법

해석적 풀이법을 적용하면 다음과 같다.

{% highlight python linenos %}
import numpy as np

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.linalg.inv(x.T @ x) @ (x.T @ y)
print("[w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-analytic-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

실행 결과 우리가 원하는 대로 $\boldsymbol{w} = [0, 10000]$를 학습함을 확인할 수 있다.

## 수치 계산법

BGD를 이용하여 학습률 0.001로 100에폭을 학습시켜 보았다.

{% highlight python linenos %}
import numpy as np

np.random.seed(2020)

x = np.array([
    [4000, 0.0005],
    [4000, 0.0009],
    [8000, 0.0004],
    [8000, 0.0001],
    [7000, 0.0006],
    [6000, 0.0005],
    [8000, 0.0002],
    [2000, 0.0004],
    [3000, 0.0005],
    [4000, 0.0005]
])

y = np.array([
    [5],
    [9],
    [4],
    [1],
    [6],
    [5],
    [2],
    [4],
    [5],
    [5]
])

w = np.random.randn(2, 1)

epochs = 100
lr = 0.001

data_num = x.shape[0]
print("Initial : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))

for epoch in range(epochs):
    predict_y = x @ w

    # calculate gradient
    gradient = -(2 / data_num) * x.T @ (y - predict_y)

    # update w
    w = w - lr * gradient

    print("Epoch %03d : [w1, w2] = [%f, %f]" % (epoch + 1, w[0, 0], w[1, 0]))

print("Result : [w1, w2] = [%f, %f]" % (w[0, 0], w[1, 0]))
{% endhighlight %}

{% include folder.html filename="normalization-biased-data-bgd-result.txt" mode="raw-txt" show_msg="결과 보기" hide_msg="결과 숨기기" %}

보이는 바와 같이 파라미터들이 수렴하지 않고 발산해 버린다. 작은 학습률을 사용하면 발산하진 않아도 학습이 진행되지 않는다.

왜 이런 일이 일어나는 것일까?

## 비정규화된 데이터 (Unnormalizaed Data)

주어진 데이터에서 $x_1$은 1000 ~ 9000 사이의 값을 가지는 값이고, $x_2$는 0.0001 ~ 0.0009 사이의 값을 가진다. 이렇게 데이터의 독립 변수(feature) 간 값의 범위의 차이가 큰 경우 데이터가 **비정규화되었다(unnormalized)**고 표현한다.

해석적 풀이법은 비정규화된 데이터에 대해서도 잘 작동한다. 해석적 풀이법은 수학적으로 최적의 해를 찾는 방법이기 때문이다. 하지만 위에서 보였듯이 비정규화된 데이터에 대해 수치 계산법을 적용할 경우 학습이 잘 되지 않는다.[^1] 왜 이런 현상이 일어나는 것일까?

[^1]: 발산하거나, 학습이 진행되지 않는다.

경사 하강법은 다음과 같은 순서로 작동한다.

1. 랜덤한 $\boldsymbol{w}$를 잡는다.
2. 오차 함수 $J$의 그라디언트 $\nabla J_\boldsymbol{w}$를 구한다.
3. 그라디언트로 반대 방향으로 $\boldsymbol{w}$를 업데이트한다 : $\boldsymbol{w} = \boldsymbol{w} - \eta \nabla J_\boldsymbol{w}$
4. 3을 필요한 만큼 반복한다.

한 번에 파라미터가 업데이트되는 양은 $|\eta \nabla J_\boldsymbol{w}|$이다. 


비정규화되지 않은 데이터에 대해 경사 하강법을 적용하는 과정을 다음과 같이 그래프로 표현할 수 있다.

{% include caption-img.html src="normalization-normal-parameter-updates.png" title="Fig.01 경사 하강법" description="경사 하강법은 랜덤한 위치에서 시작하여 점진적으로 오차 함수의 값이 가장 작은 진한 노란색 영역으로 다가가는 것이다." %}

그래프의 각 축은 각 독립 변수에 상응하는 파라미터($w_1$, $w_2$)를 나타내고 검은 색 점은 $\boldsymbol{w}$를 나타낸다. 화살표는 $\boldsymbol{w}$가 업데이트되는 과정을 도식화한 것이다. 그림에서는 총 5번의 파라미터 업데이트가 있었다. 오차 함수 $J$에서 같은 값이 나오는 파라미터 값들은 같은 색으로 표현되어 있다. 색이 진할수록 오차 함수의 값이 작다. 그림에서 보다시피 경사 하강법이 진행됨에 따라 랜덤한 위치에서 시작하여 점진적으로 오차 함수의 값이 가장 작은 진한 노란색 영역으로 다가가고 있다.

이때 화살표의 길이는 파라미터 업데이트 식($\boldsymbol{w} = \boldsymbol{w} - \eta \nabla J_\boldsymbol{w}$)의 $\eta \nabla J_\boldsymbol{w}$

그렇다면 우리의 데이터처럼 속성 간 범위 차이가 큰 데이터에 대한 경사 하강법은 어떻게 표현될까?

{% include caption-img.html src="normalization-unnormalized.png" title="Fig.02 범위가 다른 속성을 가진 데이터에서의 경사 하강법" description="파라미터 범위가 큰 속성($w_1$)에 " %}