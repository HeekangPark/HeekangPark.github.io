---
title: "지도학습 (Supervised Learning)"
order: 3
date: "2020-05-01"
---

# 지도학습의 과정

일반적으로 지도학습은 다음의 과정을 거치며 진행된다.

1. 데이터 수집 및 정제 : 학습을 위해 라벨링된 데이터를 (최대한 많이) 수집한다. 이후 수집한 데이터를 필요에 따라 가공[^1]한다.
2. 모델 결정 : 해결하고자 하는 과제 및 데이터의 성격 등을 토대로 사용하고자 하는 기계학습 모델을 결정한다. 지도학습 모델에는 로지스틱 회귀(Logistic Regression), SVM(Support Vector Machine), 의사결정나무(Decision Tree), 랜덤 포레스트(Random Forest), NN(Nearest Neighbor), 인공신경망(Neural Network), 다중 레이어 퍼셉트론(Multilayer Perceptron, MLP) 등이 있다.
3. 파라미터 최적화(Parameter Optimization) : 데이터에 맞게 모델의 파라미터를 학습시킨다.

[^1]: 통계적 처리, 노이즈 제거, 특징점(feature) 추출 등

# 파라미터 최적화(Parameter Optimization)

위 과정 중 파라미터 최적화에 대해서 조금 더 알아보자.

지도학습을 위해 데이터를 수집해 다음과 같이 총 $n$개의 입력값 $\overrightarrow{x}(x_1, x_2, \dots, x_d)$와 정답(label) $y$의 쌍으로 이루어진 데이터 셋을 구성했다고 하자.

|   idx    |  $x_1$   |  $x_2$   | $\dots$  |  $x_d$   |       |   $y$    |
| :------: | :------: | :------: | :------: | :------: | :---: | :------: |
|    1     |    2     |    16    | $\dots$  |    7     |       |    6     |
|    2     |    3     |    6     | $\dots$  |    7     |       |    9     |
| $\vdots$ | $\vdots$ | $\vdots$ | $\ddots$ | $\vdots$ |       | $\vdots$ |
|   $n$    |    8     |    96    | $\dots$  |    21    |       |    2     |

이 데이터 셋에 대해 $p$개의 파라미터 $\overrightarrow{w}(w_1, w_2, \dots, w_p )$로 이루어진 모델 $f(\overrightarrow{w})$를 학습시키기로 하였다.

만약 이 모델이 잘 학습되었다면, 다음 식을 만족할 것이다.

$$f(\overrightarrow{x}\,;\,\overrightarrow{w}) \approx y$$

이 말을 조금 다르게 표현하면, 잘 학습된 모델에서는 $f(\overrightarrow{x}\,;\,\overrightarrow{w})$와 $y$ 간의 오차가 작다. 즉 수학적으로 **모델을 학습시킨다는 것은 $f(\overrightarrow{x}\,;\,\overrightarrow{w})$와 $y$ 간의 오차 $e$를 작게 만드는 과정**이다.

이제 우리는 "모델을 학습시킨다"는 추상적인 문제 대신, "오차를 최소화한다"라는 명확하고 구체적인 문제를 가지게 되었다. 그리고 오차를 최소화하는 문제는 전형적인 최적화 문제(Optimization Problem) 중 하나이다.(이 때문에 모델 학습 과정을 파라미터 최적화라 하는 것이다.)

오차 $e$는 $\overrightarrow{w}$로 이루어진 함수이다. 이제 우리의 목표는 주어진 함수 $e(\overrightarrow{w})$가 언제 최소가 되는지, 즉 최소점에서의 $\overrightarrow{w}$를 찾는 것이다. 이 문제를 푸는 방법은 크게 두 가지가 있다. 첫 번째는 분석적 풀이법(Analytic Solution)이고, 두 번째는 수치 계산법(Numerical Solution)이다.

## 분석적 풀이법(Analytic Solution)

분석적 풀이법은 주어진 함수식이 언제 최소값을 갖는지 수학적으로 푸는 것이다.

수학적으로 어떤 함수가 언제 최솟값을 가지는지 어떻게 알 수 있을까? 예를 들어, 다음 이차함수를 생각해 보자.

$$f(x) = x^2 + 2x + 3 $$

중학교 시간에 배운 수학을 이용하면

$$f(x) = (x+1)^2 + 2$$

이므로 $x = -1$일 때 $f$는 최솟값을 가짐을 알 수 있다.

$f$가 $x = -1$에서 최솟값을 가진다는 것을 미적분을 이용해서도 보일 수 있다. $f$는 볼록함수(convex function)이므로, 최솟값은 이 함수의 도함수(derivative)가 0이 되는 점에서 발생한다. 즉,

$$\frac{df}{dx} = 2x + 2 = 0$$

을 만족시키는 $x = -1$에서 최솟값을 가짐을 알 수 있다.

다변수 함수에서는 [그라디언트(gradient)]({{ site.url }}/{{ site.baseurl }}calculus/그라디언트-gradient)를 이용하면 같은 풀이를 할 수 있다.

함수

$$f(x, y) = x^2 +xy - 2x - y^2$$

에 대해, 그라디언트는 

$$\nabla f(x, y) = [2x + y - 2, x - 2y] $$

이다. 이때 $f$의 최소점은 그라디언트의 각 원소가 0이 되는 점이다.

$$ \begin{cases}
2x + y - 2 = 0\\
x - 2y = 0\\
\end{cases} $$

위 연립방정식을 풀면

$$ x = \frac {4}{5},\quad y = \frac{2}{5} $$

이므로, 우리는 함수 $f(x, y) = x^2 +xy - 2x - y^2$가 (4/5, 2/5)에서 최솟값을 가짐을 알 수 있다.


## 수치 계산법(Numerical Solution)

수치 계산법은 정확한 해가 아닌 근사해를 구하는 것을 목적으로 하는 풀이이다. 분석적 풀이법이 수학적으로 정확한 답을 준다는 장점이 있지만, 다음의 상황에서는 적용하기 어려운 단점이 있다.

- 변수의 수가 너무 많은 경우 : 변수의 수가 너무 많은 경우 수학적으로 해를 구하는 과정에 너무 많은 컴퓨팅 파워가 요구된다.
- 최소점이 존재하지 않거나, 계산 불가능한 경우 : 함수가 볼록 함수가 아니거나, 불연속 등의 이유로 수학적으로 최소점을 계산할수 없는 경우가 있다.
- 그라디언트의 원소를 0으로 만드는 근을 구할 수 없거나 구하기 어려운 경우 : 연립방정식이 비선형연립방정식이어서 근을 수학적으로 구할 수 없거나, 선형연립방정식이어도 근이 존재하지 않는 경우, 근이 무수히 많은 경우 등이 있을 수 있다.

이 경우 수치 계산법이 유용한 방법이 될 수 있다.

수치 계산법에는 다양한 방법이 있다.

- 경사 하강법(Gradient Descent Method)
- 뉴턴법(Newton Method)
- 가우스-뉴턴법(Gauss-Newton Method)
- Levenberg-Marquardt Method
- BFGS
- Conjugate Gradient Method
- 등등...

이 중 경사하강법이 가장 흔하게 쓰이는 방법이다. 경사하강법은 방법 특성상 최소값에 도달하지 못할 수도 있으나, 속도가 빠르고 연산량이 적어 현제 널리 쓰이고 있다.

### 경사 하강법 (Gradient Descent Method)

경사 하강법은 분석적 풀이법에서처럼 그라디언트를 사용한다. 다만 경사 하강법에서는 분석적 풀이법과는 다르게 한 번에 해를 구하는 것이 아니라, 단계를 거듭하며 점점 해에 가까운 근사값을 구하는 방법이다.



