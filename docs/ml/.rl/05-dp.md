---
title: "L05. Dynamic Programming"
order: 5
date_created: "2021-04-08"
date_modified: "2021-10-14"
---

# DP(Dynamic Programming)란?

알고리즘을 공부해 보았다면 DP(Dynamic Programming, 동적 계획법)에 대해 들어보았을 것이다. 알고리즘에서의 DP는 다음 두 가지 속성을 가지고 있는 문제에 적용 가능한 풀이 방법이다.

- Overlapping Subproblem : 문제는 여러 개의 하위 문제(subproblem)으로 쪼갤 수 있다. 이때 하위 문제들은 서로 겹치는(overlapping) 부분들이 있어, 한 하위 문제를 풀 때 사용했던 결과를 적당한 곳에 저장해 놨다가[^1] 재활용할 수 있다.[^2]
- Optimal Substructure : 하위 문제의 최적 해(optimal solution)를 이용해 상위 문제의 최적 해를 구할 수 있다.[^3]

[^1]: 이를 메모이제이션(Memoization)이라 한다.
[^2]: 예를 들어, "$n$번째 피보나치 수를 구하는 문제"는 "$n-1$번째 피보나치 수를 구하는 하위 문제"와 "$n-2$번째 피보나치 수를 구하는 하위 문제"로 쪼갤 수 있다. 다시 "$n-1$번째 피보다치 수를 구하는 문제"는 "$n-2$번째 피보나치 수를 구하는 하위 문제"와 "$n-3$번째 피보나치 수를 구하는 하위 문제"로 쪼갤 수 있다. 이런 식으로 계속 하위 문제로 쪼개어 나갈 때, 몇몇 부분들이 겹치는 것을 볼 수 있다. 예를 들어, "$n-2$번째 피보나치 수를 구하는 하위 문제"는 "$n$번째 피보나치 수를 구하는 문제"의 하위 문제이면서 동시에 "$n-1$번째 피보나치 수를 구하는 문제"의 하위 문제이다. 이런 경우 동적 계획법에서는 $n-2$번째 피보나치 수를 한 번만 계산해 (어딘가에 저장해 뒀다가(메모이제이션)) 이를 다시 재활용하여 사용한다.
[^3]: 예를 들어, A 도시에서 B 도시까지 가는 길찾기 문제가 있다고 해 보자. 만약 A 도시와 B 도시 사이 C 도시가 있다면, A 도시에서 B 도시까지 가는 최적 경로에는 A 도시에서 C 도시까지 가는 최적 경로와 C 도시에서 B 도시까지 가는 최적 경로들의 조합이 반드시 포함되어 있을 것이다. 이런 식의 성질을 Optimal Substructure라 한다.

Finite MDP 상황에서, **환경에 대해 완벽한 지식이 주어진 경우** DP를 이용하면 최적 정책(optimal policy)을 정확히 구할 수 있다.

그러나 안타깝게도 DP는 한계가 분명하다.

- Finite MDP가 아닐 경우 DP를 사용할 수 없다.
- Finite MDP이더라도 환경에 대해 완벽한 지식을 모른다면 DP를 사용할 수 없다.
- 계산 시간이 엄청나게 많이 필요하다.

이런 이유들 때문에 RL 문제에 DP를 범용적으로 적용하긴 어렵다. 그러나 많은 RL 풀이법들이 DP의 아이디어를 사용하기에 DP를 잘 이해하는 것이 중요하다.

# Policy Iteration

첫 번째로 배울 DP는 **Policy Iteration**이라는 방법이다. Policy Iteration을 이용하면 최적 정책을 찾을 수 있다. 참고로 이렇게 최적 정책을 찾는 문제를 **Control 문제**라 한다.

Policy Iteration을 간단히 설명하면 다음과 같다.

1. Initialization : 정책 $\pi$, 상태-가치 함수 $v$를 적당한 값으로 초기화한다.
2. **Policy Evaluation** : 현재 정책 $\pi$를 적용했을 때 각 상태 $s$의 가치 $v_{\pi}(s)$를 업데이트한다.
3. **Policy Improvement** : 업데이트된 가치 함수 $v_{\pi}$를 바탕으로 현재 정책 $\pi$보다 조금 더 개선된 새로운 정책 $\pi'$를 만든다. 이를 현재 정책으로 삼는다.
4. 2, 3번 과정을 여러 번 반복한다.

Policy Evaluation 과정과 Policy Improvement 과정을 번갈아가며 수행하면 가치 함수와 정책이 점점 최적 가치 함수 $v_{*}$(또는 $q_{*}$)와 최적 정책 $\pi_{*}$에 다가가게 된다. 즉 다음과 같은 시퀸스가 형성된다.

$$\pi_0 \textcolor{red}{\rightarrow} v_{\pi_0} \textcolor{blue}{\rightarrow}
\pi_1 \textcolor{red}{\rightarrow} v_{\pi_1} \textcolor{blue}{\rightarrow}
\pi_2 \textcolor{red}{\rightarrow} v_{\pi_2} \textcolor{blue}{\rightarrow}
\cdots \textcolor{blue}{\rightarrow}
\pi_* \textcolor{red}{\rightarrow} v_{*}
$$

여기서 빨간 화살표는 Policy Evaluation을, 파란 화살표는 Policy Improvement를 의미한다. 각 과정을 자세히 살펴보자.

## Policy Evaluation

**Policy Evaluation**은 임의의 정책 $\pi$가 주어졌을 때 상태-가치 함수(state-value function) $v_{\pi}$를 구하는 방법이다. 참고로 이렇게 주어진 정책에 대해 가치 함수를 계산하는 문제를 **Prediction 문제**라 한다.

우린 환경에 대한 모든 정보를 알고 있으므로, [이전 글](/ml/rl/04-mdp#bellman-equation-가치-함수의-재귀성)에서 살펴본 Bellman equation을 세울 수 있다. 각 상태에 대해 Bellman equation을 만들면, 우리는 $\|\mathcal{S}\|$개의 변수들[^4]로 이루어진 $\|\mathcal{S}\|$개의 연립 선형방정식을 얻을 수 있다(단, $\mathcal{S}$는 전체 상태들의 집합). 그리고 이 연립방정식의 해를 구하면 주어진 정책에 대한 상태-가치 함수를 구할 수 있다.

[^4]: $v_{\pi}(s)$들 (단, $s \in \mathcal{S}$)

선형대수학 등에서 학습한 연립 선형방정식을 푸는 다양한 기법들을 활용하면 이 연립방정식을 풀 수 있다. 하지만 이 방법은 상태의 수가 많아지면 많이질수록 계산 난이도가 매우 올라간다는 단점이 있다. 그래서 일반적으론 이 방법보다는 **Iterative policy evaluation**이라는 방법을 이용해 연립방정식을 푼다. Iterative policy evaluation은 Bellman equation의 재귀성을 이용해 수치해석적으로(numerically) Bellman equation을 푸는 방법이다. Iterative policy evaluation의 알고리즘은 다음과 같다.

1. 각 상태 $s$에 대해, $v_0 (s)$를 임의의 값으로 초기화한다.

2. 다음 업데이트 식을 이용해 반복적으로(iterative) 각 상태 $s$마다 $v_k (s)$를 업데이트한다.[^5] {#eq-policy-evaluation-update}

    $$v_{k+1}(s) = \sum_a \pi(a | s) \sum_{s',\,r} p(s',r | s, a)[r + \gamma v_k (s')]$$

3. 2를 적당한 횟수만큼 반복한다.

[^5]: 잘 보면 상태 $s$의 가치 $v_{k+1}(s)$는 가능한 모든 다음 상태 $s'$들의 가치 $v_k (s')$들을 이용해 업데이트되고 있다. 이렇게 가능한 모든 다음 값들을 이용해 현재 값을 업데이트하는 것을 **expected update**라 부른다. "expected"라는 표현은 이 업데이트 과정이 샘플링(sampling)된 다음 상태 하나만을 보고 업데이트하는 것이 아닌, 가능한 모든 다음 상태들의 기댓값(expectation)을 구한다는 것에서 나온 표현이다. DP에서는 환경에 대한 모든 정보를 알고 있으므로 expected update가 가능하다. 참고로 다음 글에서 살펴볼 [MC Method](/ml/rl/06-mc)나 [TD Method](/ml/rl/07-td)에서는 샘플링된 다음 상태 하나만을 이용해 업데이트가 진행된다(이를 **sample update**라 부른다).

Iterative policy evaluation을 수행하면 수열 $\\{ \,v_k (s) \, \\}$가 생성된다. $k$가 커지면 커질수록 $v_k (s)$는 $v_{\pi}(s)$에 점점 수렴한다.

### 여담: Iterative policy evaluation 구현하기

Iterative policy evaluation을 구현하는 가장 직관적은 방법은 두 개의 배열을 사용해, 각각 옛 값($v_k (s)$)와 최신 값($v_{k+1}(s)$)를 번갈아가며 담는 방법일 것이다.

그런데 배열을 하나만 쓰는 방법도 있다. 업데이트 할 때, 최신 값이 옛 값을 그냥 덮어씌우게 하는 것이다.[^6] 얼핏 생각해 보면 이렇게 할 경우 몇몇 $v_{k+1} (s)$는 업데이트될 때 $v_{k}(s')$이 아닌 $v_{k+1}(s')$을 사용하게 되어 문제가 있을 것 같다. 하지만 $v_{k} (s)$는 참값이 아닌 근사값임을 기억하자. 사실 $v_{k+1}(s')$를 이용해 업데이트를 하면 오차가 작은 최신 값을 사용해 업데이트를 한 셈이 되어, 배열을 두 개 사용할 때보다 오히려 더 빨리 수렴하게 된다. 대신 이 방법은 각 상태를 업데이트하는 순서에 따라 수렴 속도가 크게 바뀐다. 일반적으론 배열을 두 개 사용하는 방법보다 이 방법으로 Iterative policy evaluation을 구현한다.

[^6]: 이 방법에서의 매 업데이트는 전체 상태 공간(state space)을 한 번 쭉 훑으며(sweep) 진행되기에, 업데이트 한 번을 **sweep**이라 부르기도 한다. 즉 다음과 같이 말할 수 있다 : "Policy Evaluation은 여러 번의 sweep을 통해 현재 정책에 대한 가치 함수를 추정한다".

그리고 또 하나 생각해 봐야 할 것이, 반복을 멈추는 시점이다. 수학적으로는 $k \rightarrow \infty$일 때 $v_k = v_\pi$가 되므로 무한 번 반복을 해야겠지만, 당연히 실제 구현할 땐 이렇게 하지 않는다. 반복을 멈추는 시점을 설정하는 다양한 방법 중 하나를 소개하면 다음과 같다.

1. $v_k (s)$를 업데이트할 때 각 상태별로 최신 값과 옛 값의 차 $\| v_{k+1} (s) - v_k (s) \|$를 계산해, 그 중 최댓값을 찾는다.
2. 만약 그 최댓값이 특정 값(threshold) 이하로 떨어지면 그만둔다.

## Policy Improvement

**Policy Improvement**는 Policy Evaluation으로 계산된 최신 가치 함수 $v_{\pi}$를 토대로 최선의 정책을 업데이트하는 과정을 뜻한다.

우리는 [이전 글](/ml/rl/04-mdp#정책-policy-과-가치-함수-value-function)에서 $v_{\pi}$가 상태 $s$에서 현재 정책 $\pi$를 따르는 것이 얼마나 좋은지를 나타내는 값임을 배웠다. 어떤 임의의 결정론적 정책(deterministic policy) $\pi$가 있을 때, 어떤 상태 $s$에서 특정 행동 $a \neq \pi(s)$에 대해

$$q_{\pi}(s, a) > v_{\pi}(s)$$

가 성립한다고 해 보자.[^7] 그렇다면 상태 $s$에서는 행동 $a$를 선택하고 나머지 상태에 대해서는 $\pi$와 똑같은 선택을 하는, 새로운 정책 $\pi'$는 기존 정책 $\pi$보다 더 나은 정책이라 할 수 있다.

[^7]: 이를 말로 풀어 쓰면 다음과 같다: 상태 $s$에서 ($\pi(s)$가 아닌) 행동 $a$를 시행하고 이후 $\pi$를 따르는 것이, 계속 $\pi$를 따르는 것보다 낫다.

이를 일반화하면 다음과 같이 된다.


:::: info Policy Improvement Theorem {#policy-improvement-theorem}

결정론적 정책 $\pi$와, $\pi$와 동일하지만 특정 상태 $s$에서만 다른(즉, $\pi'(s) \neq \pi(s)$) 결정론적 정책 $\pi'$가 있다고 하자. 모든 상태 $s \in \mathcal{S}$에 대해

$$q_{\pi} (s,\,\pi'(s)) \ge v_{\pi}(s)$$

가 성립하면, 정책 $\pi'$는 정책 $\pi$보다 낫거나 같은 정책이다. 다시 말해, 모든 상태 $s \in \mathcal{S}$에 대해 다음이 성립한다.

$$v_{\pi'}(s) \ge v_{\pi}$$

::: details 증명 : Policy Improvement Theorem

$$\begin{align}
v_{\pi}(s) 
&\le q_{\pi}(s,\,\pi'(s))\\[0.5em]
&=\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s,\,A_t = \pi'(s)]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1},\,\pi'(S_{t+1}))\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) \,|\,S_{t+1},\,A_{t+1} = \pi'(S_{t+1})]\,|\,S_t = s]\\[0.5em]
&=\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+3})\,|\,S_t = s]\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+4})\,|\,S_t = s]\\[0.5em]
&\qquad\qquad\qquad\qquad\qquad\vdots\\[0.5em]
&\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots \,|\,S_t = s]\\[0.5em]
&= v_{\pi'}(s)
\end{align}
$$

:::

::::

이제 얘기를 조금 더 확장해 하나의 상태에서만 개선된 정책이 아닌, 전체 상태에 대해 더 개선된 정책 $\pi''(s)$을 생각해 보자. 즉, 모든 상태에 대해 $q_{\pi} (s,\,a)$를 참고해 가장 최고의 행동을 탐욕적으로(greedy) 뽑는 것이다. 이를 수식으로 나타내면 다음과 같다.

$$\begin{align}
\pi''(s)
&= \underset{a}{\operatorname{arg max}} q_{\pi} (s,\,a)\\[0.5em]
&= \underset{a}{\operatorname{arg max}} \mathbb{E}[R_{t+1} + \gamma v_{\pi} (S_{t+1}) \,|\,S_t = s,\,A_t = a]\\[0.5em]
&= \underset{a}{\operatorname{arg max}} \sum_{s'\,r} p(s',\,r\,|\,s,\,a) [r + \gamma v_{\pi} (s')]\\[0.5em]
\end{align}
$$

$\pi''(s)$는 Policy Improvement Theorem을 만족하므로 기존 정책 $\pi(s)$보다 항상 좋거나 같은 정책이 된다. 이처럼 **가치 함수를 토대로 기존 정책보다 더 개선된 정책 $\pi''(s)$를 탐욕적인 방법으로 찾는 과정을 Policy Improvement라 한다.**

만약 Policy Improvement로 만든 새로운 정책 $\pi''(s)$가 기존 정책 $\pi(s)$와 같은 정도로 좋다고 해 보자. 즉, $v_{\pi} = v_{\pi''}$라 해 보자. 이 경우 $v_{\pi''}$을 다음과 같이 전개할 수 있다.

$$\begin{align}
v_{\pi''}(s)
&= \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{\pi''}(S_{t+1})\,|\,S_t = s,\, A_t = a]\\[0.5em]
&= \max_{a} \sum_{s'\,r} p(s',\,r\,|\,s,\,a) [r + \gamma v_{\pi''} (s')]\\[0.5em]
\end{align}
$$

이때, 위 식의 형태는 [이전 글](/ml/rl/04-mdp#bellman-optimality-equation)에서 다룬 Bellman optimality equation과 같음을 볼 수 있다. Finite MDP 문제에서는 Bellman optimality equation의 해가 유일하므로, $v_{\pi} = v_{\pi''} = v_{*}$가 된다. 다시 말해, Policy Improvement는 기존 정책이 최적 정책이 아닌 한 무조건 기존 정책보다 개선된 정책을 만들어 낸다.

이때까지의 우리의 논의는 모두 주어진 정책이 결정론적 정책일 때의 얘기였다. 그런데 Policy Improvement는 확률론적 정책(stochastic policy)으로도 쉽게 확장 가능하다. Policy Improvement Theorem은 확률론적 정책에 대해서도 적용된다. 만약 탐욕적인 행동($q_{\pi}$를 최대로 만드는 행동)이 여러 개 있을 경우[^8] 확률론적 정책에서는 이들에 모두 0이 아닌 확률을 부여하기만 하면 된다.

[^8]: 결정론적 정책에서는 이런 경우 여러 행동 중 하나를 랜덤하게 고른다(어떤 것을 골라도 최적 행동이 된다).

## 구현

Policy Iteration에 대한 의사 코드(pseudo code)는 다음과 같다.

::: info Policy Iteration (Iterative Policy Evaluation 사용)

<div class="pseudo-code" markdown="block">

1\. Initialization

<span class="indent-1"/>모든 $s \in \mathcal{S}$에 대해 $V(s) \in \mathbb{R}$과 $\pi(s) \in \mathcal{A}(s)$를 임의의 값으로 초기화한다.

{:.mt-1}
2\. (Iterative) Policy Evaluation

<span class="indent-1"/><span class="keyword-highlight">Loop</span>:

<span class="indent-2"/>$\Delta \leftarrow 0$

<span class="indent-2"/><span class="keyword-highlight">Loop</span> for each $s \in \mathcal{S}$:

<span class="indent-3"/>$v \leftarrow V(s)$

<span class="indent-3"/>$V(s) \leftarrow \sum_{s',\,r}p(s',\,r\,\|\,s,\,\pi(s))[r + \gamma V(s')]$

<span class="indent-3"/>$\Delta \leftarrow \max(\Delta, \|v - V(s)\|)$

<span class="indent-1"/><span class="keyword-highlight">until</span> $\Delta < \theta$ (단, $\theta$는 추정(estimation)의 정확도를 결정하는 작은 양의 실수)

{:.mt-1}
3\. Policy Improvement

<span class="indent-1"/>policy_stable $\leftarrow$ true

<span class="indent-1"/><span class="keyword-highlight">Loop</span> for each $s \in \mathcal{S}$:

<span class="indent-2"/>old_action $\leftarrow \pi(s)$

<span class="indent-2"/>$\pi(s) \leftarrow \underset{a}{\operatorname{arg max}} \sum_{s'\,r} p(s',\,r\,\|\,s,\,a) [r + \gamma V(s')]$

<span class="indent-2"/><span class="keyword-highlight">If</span> old_action $\neq \pi(s)$:

<span class="indent-3"/>policy_stable $\leftarrow$ false

<span class="indent-1"/><span class="keyword-highlight">If</span> policy_stable:

<span class="indent-2"/><span class="keyword-highlight">Return</span> $V$ ($V \approx v_{*}$)

<span class="indent-1"/><span class="keyword-highlight">else</span>:

<span class="indent-2"/><span class="keyword-highlight">Goto</span> 2. Policy Evaluation

</div>
:::