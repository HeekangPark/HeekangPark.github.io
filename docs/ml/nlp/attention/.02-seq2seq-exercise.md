
# 예제

Google Colab에서 pytorch를 이용해 Seq2Seq 모델 기반 한국어 → 영어 번역기를 직접 만들어 보자.

### 사전준비

다음 라이브러리들을 설치한다.[^4-1]

```python
!pip install torch transformers Korpora tqdm
```

[^4-1]: 2024년 8월 현재 Google Colab에서 아래 명령어를 실행하면 pytorch==2.3.1+cu121, transformers==4.42.4, Korpora==0.2.0, tqdm==4.66.5 버전이 설치된다.

다음 항목들을 import한다.

```python
import os
import math
import json
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from torch.nn.utils import clip_grad_norm_

from Korpora import Korpora

from transformers import AutoTokenizer

from tqdm.notebook import tqdm
```

## 데이터

영어-한국어 번역기를 만드는데 사용할 데이터는 Jungyeul Park 님이 만드신 한국어-영어 뉴스 말뭉치이다.

::: info 데이터 정보

- author: Jungyeul Park
- repository: <https://github.com/jungyeul/korean-parallel-corpora>
- size:
  - train: 94,123 pairs
  - dev: 1,000 pairs
  - test: 2,000 pairs

:::

Korpora를 이용하면 이 말뭉치를 손쉽게 받을 수 있다(Korpora에 대한 자세한 사용법은 [다음 문서](https://ko-nlp.github.io/Korpora/)를 참고하자). 이를 이용해 train, dev, test dataset 및 dataloader를 다음과 같이 만들 수 있다.

```python:line-numbers
# Dataset, Dataloader

class KoEnDataset(Dataset):
    def __init__(self, split):
        assert split in ["train", "dev", "test"], f"Invalid param <split> : {split}"
        
        full_dataset = Korpora.load("korean_parallel_koen_news", root_dir=".")
        if split == "train":
            self.dataset = full_dataset.train
        elif split == "dev":
            self.dataset = full_dataset.dev
        else: # test
            self.dataset = full_dataset.test
    
    def __len__(self):
        return len(self.dataset) 
    
    def __getitem__(self, idx):
        kor = self.dataset[idx].text
        eng = self.dataset[idx].pair
        return kor, eng

def collate_fn(pairs):
    batch_size = len(pairs)
    
    kors = [x[0] for x in pairs]
    engs = [x[1] for x in pairs]
    
    # tokenize
    kors_input_ids = [
        torch.LongTensor(
            [kor_tokenizer.bos_token_id] + x + [kor_tokenizer.eos_token_id]
        ) for x in kor_tokenizer(kors, add_special_tokens=False).input_ids
    ]
    
    kors_lengths = [x.shape[0] for x in kors_input_ids]
    
    engs_input_ids = [
        torch.LongTensor(
            [eng_tokenizer.bos_token_id] + x + [eng_tokenizer.eos_token_id]
        ) for x in eng_tokenizer(engs, add_special_tokens=False).input_ids
    ]
    
    engs_lengths = [x.shape[0] for x in engs_input_ids]
    
    # sort
    sorted_order = [
        (original_idx, sorted_idx) for sorted_idx, original_idx
        in enumerate(
            sorted(range(len(kors_lengths)), key=lambda i: kors_lengths[i], reverse=True)
        )
    ]
    
    kors_input_ids = [kors_input_ids[i] for i, _ in sorted_order]
    kors_lengths = [kors_lengths[i] for i, _ in sorted_order]
    engs_input_ids = [engs_input_ids[i] for i, _ in sorted_order]
    engs_lengths = [engs_lengths[i] for i, _ in sorted_order]
    
    # padding
    kors_input_ids = pad_sequence(
        kors_input_ids,
        batch_first=True,
        padding_value=kor_tokenizer.pad_token_id
    )
    
    engs_input_ids = pad_sequence(
        engs_input_ids,
        batch_first=True,
        padding_value=eng_tokenizer.pad_token_id
    )
    
    # labels
    labels = engs_input_ids.clone()

    indices = torch.LongTensor(engs_lengths) - 1 # index of <eos> tokens
    labels[torch.arange(batch_size), indices] = eng_tokenizer.pad_token_id # substitute <eos> tokens to <pad> tokens

    labels = labels[:, 1:] # remove <sos> tokens
    # labels : tensor[batch_size, max_sequence_len - 1]

    labels = labels.reshape(-1)
    # labels : tensor[batch_size * (max_sequence_len - 1)]

    return kors_input_ids, kors_lengths, engs_input_ids, engs_lengths, labels

num_workers = 8
batch_size = 128

train_dataset = KoEnDataset("train")
dev_dataset = KoEnDataset("dev")
test_dataset = KoEnDataset("test")

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)
```

- line 28-32, 36-40 : tokenization
  - 각 시퀸스의 앞과 뒤에 `<sos>`, `<eos>`을 붙임
  - tokenizer가 자동으로 붙여주는 special token들(ex. `[CLS]`, `[SEP]`)은 본 예제에서는 필요없기에 `add_special_tokens`에 `False`를 줌
- line 44-55 : sorting
  - packing을 위해 한국어 시퀸스(인코더 입력 시퀸스)의 길이 순으로 내림차순 정렬
  - 영어 시퀸스(디코더 입력 시퀸스)는 길이 순으로 정렬되지 않겠지만, 디코더에서는 packing을 사용하지 않기 때문에 상관 없다.
- line 70-80 : label 만들기
  - 디코더는 `<eos>` 토큰이 입력되기 직전 시점의 토큰을 입력받아 `<eos>` 토큰을 출력함(정확히는, 그러려고 노력함). 즉 `<eos>` 토큰이 입력되었을 때는 아무런 의미없는 토큰이 출력됨. 이에 label을 만들 때는 `<eos>` 토큰을 모두 `<pad>` 토큰으로 대체함. `<pad>` 토큰이 위치한 인덱스는 loss 계산 과정에서 무시할 수 있음(아래 [[Knowledge Base/nlp/seq2seq#training\|#training]] 문단 참조).
  - 디코더는 `<sos>` 토큰을 입력받아 그 다음 토큰을 연쇄적으로 생성함. 즉 디코더의 출력에는 `<sos>` 토큰이 포함되어 있지 않음. 이에 label을 만들 때는 디코더 입력 시퀸스에서 첫 번째 토큰(`<sos>`)을 제거함.

## Seq2Seq 모델 구현

### 인코더

우선 인코더는 다음과 같은 역할을 해야 한다.

1. 원문 문자열(영어)을 받아들여,
2. tokenization : tokenizer를 이용해 tokenization을 수행하고,
3. embedding : 각 token을 임베딩 벡터로 변환하고,
4. feeding : 하나씩 RNN 모듈에 넣어 은닉 상태를 업데이트한다.
5. 마지막 token까지 모두 입력한 후, 최종 은닉 상태를 반환한다.

2번 tokenization 과정에서는 spacy, nltk 등의 rule-based tokenizer를 사용해도 되지만, 우린 huggingface의 transformers 라이브러리에서 제공하는 `bert-base-uncased` tokenizer를 사용하자. 이 tokenizer는 데이터 기반 tokenizer(WordPiece)로, BERT가 학습할 때 사용했던 tokenizer이다. 또 3번 embedding 과정에서는 one-hot encoding을 사용한다.

```python:line-numbers
# Encoder

class Encoder(nn.Module):
    def __init__(self, hidden_size=256, num_layers=2, dropout=0.5):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.rnn = nn.LSTM(
            input_size=self.tokenizer.vocab_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
    
    def one_hot_encoding(self, input_ids):
        # sequence 하나를 one hot embedding으로 변환해주는 함수
        # input_ids : LongTensor(seq_len)
        # return : tensor(seq_len, vocab_size)
        return torch.zeros(len(input_ids), self.tokenizer.vocab_size).scatter(1, input_ids.unsqueeze(1), 1)
    
    def forward(self, sentences):
        # 만약 단일 문장이 입력되었다면, 문장을 리스트로 감싸 batch 입력으로 바꿔준다.
        if isinstance(sentences, str):
            sentences = [sentences]
        
        # huggingface tokenizer를 이용해 tokenize
        input_ids = self.tokenizer(
            sentences,
            return_attention_mask=False,
            return_token_type_ids=False
        ).input_ids
        
        # one-hot encoding
        input_ids = [
            self.one_hot_encoding(torch.LongTensor(x)) for x in input_ids
        ]
        
        # 길이 내림차순으로 정렬
        sorted_order = [
            (original_idx, sorted_idx)
            for sorted_idx, original_idx
            in enumerate(
                sorted(range(len(input_ids)), key=lambda i: input_ids[i].shape[0], reverse=True)
            )
        ]
        input_ids = [input_ids[i] for i, _ in sorted_order]
        lengths = [x.shape[0] for x in input_ids]
        
        # padded sequence 생성
        padded_input_ids = pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id
        )
        
        # packed sequence 변환
        packed_input_ids = pack_padded_sequence(
            padded_input_ids,
            lengths=lengths,
            batch_first=True
        )
        
        outputs, (hidden_states, cell_states) = self.rnn(packed_input_ids)
        
        # 원래 순서대로 복구
        sorted_order = sorted(sorted_order, key=lambda x: x[0])
        hidden_states = torch.stack(
            [hidden_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        cell_states = torch.stack(
            [cell_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        
        return hidden_states, cell_states
```

======================================================================================================================

# 예제

pytorch를 이용해 영어를 한국어로 번역하는 Seq2Seq 모델을 직접 만들어 보자.

## 사전준비

다음 패키지들을 설치한다.

- pytorch==1.13.0
- transformers==4.24.0
- korpora==0.2.0

다음 항목들을 import한다.

```python
from Korpora import Korpora
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
import torch.nn as nn
from transformers import AutoTokenizer
```

## 데이터

영어-한국어 번역기를 만드는데 사용할 데이터는 Jungyeul Park 님이 만드신 한국어-영어 뉴스 말뭉치이다.

::: info 데이터 정보

- author: Jungyeul Park
- repository: <https://github.com/jungyeul/korean-parallel-corpora>
- size:
  - train: 94,123 pairs
  - dev: 1,000 pairs
  - test: 2,000 pairs

:::

Korpora를 이용하면 이 말뭉치를 손쉽게 받을 수 있다(Korpora에 대한 자세한 사용법은 [다음 문서](https://ko-nlp.github.io/Korpora/)를 참고하자). 이를 이용해 train, dev, test dataset 및 dataloader를 다음과 같이 만들 수 있다.

```python:line-numbers
# Dataset, Dataloader

class KoEnDataset(Dataset):
    def __init__(self, split):
        assert split in ["train", "dev", "test"], f"Invalid param <split> : {split}"
        
        full_dataset = Korpora.load("korean_parallel_koen_news", root_dir=".")
        if split == "train":
            self.dataset = full_dataset.train
        elif split == "dev":
            self.dataset = full_dataset.dev
        else: # test
            self.dataset = full_dataset.test
    
    def __len__(self):
        return len(self.dataset) 
    
    def __getitem__(self, idx):
        return self.dataset[idx].text, self.dataset[idx].pair

train_dataset = KoEnDataset("train")
dev_dataset = KoEnDataset("dev")
test_dataset = KoEnDataset("test")

batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

## Seq2Seq 모델 구현

우선 인코더는 다음과 같은 역할을 해야 한다.

1. 원문 문자열(영어)을 받아들여,
2. tokenization : tokenizer를 이용해 tokenization을 수행하고,
3. embedding : 각 token을 임베딩 벡터로 변환하고,
4. feeding : 하나씩 RNN 모듈에 넣어 은닉 상태를 업데이트한다.
5. 마지막 token까지 모두 입력한 후, 최종 은닉 상태를 반환한다.

2번 tokenization 과정에서는 spacy, nltk 등의 rule-based tokenizer를 사용해도 되지만, 우린 huggingface의 transformers 라이브러리에서 제공하는 `bert-base-uncased` tokenizer를 사용하자. 이 tokenizer는 데이터 기반 tokenizer(WordPiece)로, BERT가 학습할 때 사용했던 tokenizer이다. 또 3번 embedding 과정에서는 one-hot encoding을 사용한다.

```python:line-numbers
# Encoder

class Encoder(nn.Module):
    def __init__(self, hidden_size=256, num_layers=2, dropout=0.5):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.rnn = nn.LSTM(
            input_size=self.tokenizer.vocab_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
    
    def one_hot_encoding(self, input_ids):
        # sequence 하나를 one hot embedding으로 변환해주는 함수
        # input_ids : LongTensor(seq_len)
        # return : tensor(seq_len, vocab_size)
        return torch.zeros(len(input_ids), self.tokenizer.vocab_size).scatter(1, input_ids.unsqueeze(1), 1)
    
    def forward(self, sentences):
        # 만약 단일 문장이 입력되었다면, 문장을 리스트로 감싸 batch 입력으로 바꿔준다.
        if isinstance(sentences, str):
            sentences = [sentences]
        
        # huggingface tokenizer를 이용해 tokenize
        input_ids = self.tokenizer(
            sentences,
            return_attention_mask=False,
            return_token_type_ids=False
        ).input_ids
        
        # one-hot encoding
        input_ids = [
            self.one_hot_encoding(torch.LongTensor(x)) for x in input_ids
        ]
        
        # 길이 내림차순으로 정렬
        sorted_order = [
            (original_idx, sorted_idx)
            for sorted_idx, original_idx
            in enumerate(
                sorted(range(len(input_ids)), key=lambda i: input_ids[i].shape[0], reverse=True)
            )
        ]
        input_ids = [input_ids[i] for i, _ in sorted_order]
        lengths = [x.shape[0] for x in input_ids]
        
        # padded sequence 생성
        padded_input_ids = pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id
        )
        
        # packed sequence 변환
        packed_input_ids = pack_padded_sequence(
            padded_input_ids,
            lengths=lengths,
            batch_first=True
        )
        
        outputs, (hidden_states, cell_states) = self.rnn(packed_input_ids)
        
        # 원래 순서대로 복구
        sorted_order = sorted(sorted_order, key=lambda x: x[0])
        hidden_states = torch.stack(
            [hidden_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        cell_states = torch.stack(
            [cell_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        
        return hidden_states, cell_states
```






### tokenizer

다음과 같이 tokenizer들을 설정한다.

```python title:"tokenizer 설정" linenos
kor_tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
eng_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

kor_tokenizer.add_special_tokens({
    "bos_token": "<bos>",
    "eos_token": "<eos>",
    "pad_token": "<pad>"
})

eng_tokenizer.add_special_tokens({
    "bos_token": "<bos>",
    "eos_token": "<eos>",
    "pad_token": "<pad>"
})
```

- line 1, 2 : huggingface의 pretrained tokenizer
    - `kor_tokenizer` : [한국어 BERT](https://huggingface.co/klue/bert-base)에 사용된 tokenizer
    - `eng_tokenizer` : [영어 BERT](https://huggingface.co/bert-base-uncased)에 사용된 tokenizer
- line 4-8, 10-14 : 특수 토큰(special token) 추가
    - Seq2Seq 모델의 학습/사용을 위해서는 `<sos>`, `<eos>`, `<pad>`, 이렇게 세 종류의 특수 토큰들이 필요
    - `add_special_tokens()` 메소드를 사용해 이들을 명시적으로 추가
    - huggingface의 tokenizer에서는 `<sos>`(start of sequence)라는 이름 대신 `<bos>`(begin of sequence)라는 이름을 사용한다. 단순히 이름의 차이일 뿐이니 혼용해도 상관없다.

### 모델 구현

#### ModelConfig

- model의 설정값을 담아두기 위한 클래스

```python title:"ModelConfig" linenos
class ModelConfig:
    def __init__(self, config_dict):
        self.config_dict = config_dict
        for key, value in config_dict.items():
            if isinstance(value, dict):
                value = ModelConfig(value)
                
            setattr(self, key, value)
    
    def __getattr__(self, key):
        return None
    
    def __str__(self):
        return str(self.config_dict)
    
    def __repr__(self):
        return self.__str__()
```

#### Embedding

- (tokenize된) 시퀸스를 임베딩 형태로 바꿔주는 모듈

```python title:"Embedding" linenos
class Embedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        
        self.embedding = nn.Embedding(
            num_embeddings=self.config.vocab_size,
            embedding_dim=self.config.embedding_dim,
            padding_idx=self.config.pad_token_id
        )
        
        self.dropout = nn.Dropout(self.config.dropout_rate)
        
    def forward(self, x):
        # x : tensor[batch_size, max_sequence_len]
        
        output = self.dropout(self.embedding(x))
        # output : tensor[batch_size, max_sequence_len, embedding_dim]
        
        return output
```

#### Encoder

```python title:"Encoder" linenos
class Encoder(nn.Module):
    rnn_models = {
        "vanilla": nn.RNN,
        "lstm": nn.LSTM,
        "gru": nn.GRU
    }
    
    def __init__(self, encoder_config):
        super().__init__()
            
        self.config = encoder_config
        
        self.rnn = self.rnn_models[self.config.rnn_name](
            input_size=self.config.embedding_dim,
            hidden_size=(self.config.hidden_size // 2) if self.config.bidirectional else self.config.hidden_size, # half hidden size for bidirectional RNN,
            num_layers=self.config.num_layers,
            batch_first=True,
            dropout=self.config.dropout_rate if self.config.num_layers > 1 else 0,
            bidirectional=self.config.bidirectional
        )
        
    def forward(self, input_embs, lengths):
        # input_embs : tensor[batch_size, max_sequene_len, embedding_dim]
        # lengths : list
        
        batch_size = input_embs.shape[0]
        
        # packing
        input_embs_packed = pack_padded_sequence(
            input_embs,
            lengths=lengths,
            batch_first=True,
        )
        
        # rnn
        outputs_packed, hidden_states = self.rnn(input_embs_packed)
        
        # unpacking
        outputs, _ = pad_packed_sequence(
            outputs_packed,
            batch_first=True
        )
        
        # convert : tensor[num_layers * 2, batch_size, hidden_size // 2] -> tensor[num_layers, batch_size, hidden_size]
        if self.config.bidirectional:    
            if self.config.rnn_name == "lstm":
                hidden_states_list = list(hidden_states)
            else: # vanilla, gru
                hidden_states_list = [hidden_states]
                
            for i in range(len(hidden_states_list)):
                hidden_states_list[i] = hidden_states_list[i].transpose(0, 1).contiguous() # tensor[batch_size, num_layers * 2, hidden_size // 2]
                hidden_states_list[i] = hidden_states_list[i].view(batch_size, -1, self.config.hidden_size) # tensor[batch_size, num_layers, hidden_size]
                hidden_states_list[i] = hidden_states_list[i].transpose(0, 1).contiguous() # tensor[num_layers, batch_size, hidden_size]
                
            if self.config.rnn_name == "lstm":
                hidden_states = tuple(hidden_states_list)
            else:
                hidden_states = hidden_states_list[0]
                    
        # outputs : tensor[batch_size, max_sequence_len, hidden_size]
        # hidden_states : tensor[num_layers, batch_size, hidden_size]
        return outputs, hidden_states
```

- line 2-6, 13 : Vanilla RNN, LSTM, GRU 중 원하는 RNN으로 쉽게 전환할 수 있도록 설계
- line 15, 19, 44-59 : bidirectional 설정
    - 인코더는 마지막 시점까지의 입력을 한 번에 받을 수 있음 → bidirectional RNN 사용 가능
    - 반면 디코더는 이전 시점의 출력이 다음 시점의 입력으로 들어가는 구조 상 마지막 시점까지의 입력을 한 번에 받을 수 없음 → bidirectional RNN 사용 불가능
    - 따라서 인코더에서 bidirectional RNN을 사용한 경우, 디코더와의 호환을 위해 차원을 맞춰줘야 함
    - bidirectional RNN 인코더의 경우, half-size hidden state를 사용하고, 연산이 모두 끝난 후 순방향, 역방향 은닉 상태를 합쳐(concatenate) full-size hidden state를 만든다.
- line 28-33, 38-42 : packing
    - 인코더는 마지막 시점까지의 입력을 한 번에 받을 수 있음
    - 메모리 효율적이고 padding에 대한 추가 처리를 할 필요 없는 packing을 사용

#### Decoder

```python title:"Decoder" linenos
class Decoder(nn.Module):
    rnn_models = {
        "vanilla": nn.RNN,
        "lstm": nn.LSTM,
        "gru": nn.GRU
    }
    
    def __init__(self, decoder_config):
        super().__init__()
        
        self.config = decoder_config
        
        self.rnn = self.rnn_models[self.config.rnn_name](
            input_size=self.config.embedding_dim,
            hidden_size=self.config.hidden_size,
            num_layers=self.config.num_layers,
            batch_first=True,
            dropout=self.config.dropout_rate if self.config.num_layers > 1 else 0,
            bidirectional=False  # decoder cannot be bidirectional
        )
        
    def forward(self, input_embs, hidden_states):
        # input_embs : tensor[batch_size, 1, embedding_dim]
        # hidden_states : tensor[num_layers, batch_size, hidden_size]
        
        outputs, hidden_states = self.rnn(input_embs, hidden_states)
        
        # outputs : tensor[batch_size, 1, hidden_size]
        # hidden_states : tensor[num_layers, batch_size, hidden_size]
        return outputs, hidden_states
```

- line 19 : 디코더는 bidirectional RNN을 사용할 수 없다.
- line 22 : 모든 시점의 입력을 받는 인코더와 다르게, 디코더는 한 시점의 입력만 받음에 유의

#### LMHead

- 디코더 출력값을 각 단어에 대한 확률값[^4-2]으로 변환하는 모듈

```python title:"LMHead" linenos
class LMHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        
        self.fc = nn.Linear(
            self.config.hidden_size,
            self.config.vocab_size
        )
        self.softmax = nn.LogSoftmax(dim=-1)
        
    def forward(self, x):
        # x : tensor[batch_size, max_sequence_len, hidden_size]
        
        output = self.softmax(self.fc(x))
        # output : tensor[batch_size, max_sequence_len, vocab_size]
        
        return output
```

[^4-2]: 정확히는, 확률값에 log를 씌운 값(LogSoftmax)

#### Seq2Seq 모델

```python title:"Seq2Seq" linenos
class Seq2Seq(nn.Module):
    def __init__(self, encoder_tokenizer, decoder_tokenizer, encoder_embedding_dim, decoder_embedding_dim, encoder_bidirectional, hidden_size, num_layers, dropout_rate, rnn_name="lstm"):
        super().__init__()
        
        self.save_dir = None
        
        self.config = ModelConfig({
            "encoder": {
                "vocab_size": len(encoder_tokenizer),
                "embedding_dim": encoder_embedding_dim,
                "hidden_size": hidden_size,
                "num_layers": num_layers,
                "bidirectional": encoder_bidirectional,
                "dropout_rate": dropout_rate,
                "rnn_name": rnn_name,
                "pad_token_id": encoder_tokenizer.pad_token_id,
                "bos_token_id": encoder_tokenizer.bos_token_id,
                "eos_token_id": encoder_tokenizer.eos_token_id
            },
            "decoder": {
                "vocab_size": len(decoder_tokenizer),
                "embedding_dim": decoder_embedding_dim,
                "hidden_size": hidden_size,
                "num_layers": num_layers,
                "bidirectional": False,
                "dropout_rate": dropout_rate,
                "rnn_name": rnn_name,
                "pad_token_id": decoder_tokenizer.pad_token_id,
                "bos_token_id": decoder_tokenizer.bos_token_id,
                "eos_token_id": decoder_tokenizer.eos_token_id
            }
        })
        
        self.encoder_embedding = Embedding(self.config.encoder)
        
        self.encoder = Encoder(self.config.encoder)
        
        self.decoder_embedding = Embedding(self.config.decoder)
        
        self.decoder = Decoder(self.config.decoder)
        
        self.head = LMHead(self.config.decoder)
    
    def forward(self, encoder_input_ids, encoder_lengths, decoder_input_ids, decoder_lengths):
        # encoder_input_ids : tensor[batch_size, max_seq_len]
        # decoder_input_ids : tensor[batch_size, max_seq_len]
        
        batch_size = encoder_input_ids.shape[0]
        
        # 1. encoding
        encoder_input_embs = self.encoder_embedding(encoder_input_ids)
        # encoder_input_embs : tensor[batch_size, max_seq_len, encoder_embedding_dim]
        
        encoder_outputs, encoder_hidden_states = self.encoder(encoder_input_embs, encoder_lengths)
        # encoder_outputs : tensor[batch_size, max_seq_len, hidden_size]
        # encoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
        
        # 2. decoding
        decoder_max_sequence_len = decoder_input_ids.shape[1]
        
        decoder_input_embs = self.decoder_embedding(decoder_input_ids)
        # decoder_input_embs : tensor[batch_size, max_seq_len, decoder_embedding_dim]
        
        decoder_outputs = torch.zeros(
            (batch_size, decoder_max_sequence_len, self.config.decoder.hidden_size)
        ).to(decoder_input_embs.device) # decoder_outputs : tensor[batch_size, decoder_max_sequence_len, hidden_size]
        
        decoder_hidden_states = encoder_hidden_states
        # decoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
        
        for t in range(decoder_max_sequence_len):
            decoder_input_embs_single = decoder_input_embs[:, t, :].unsqueeze(1)
            
            decoder_outputs_single, decoder_hidden_states = self.decoder(
                decoder_input_embs_single,
                decoder_hidden_states
            )
            # decoder_outputs_single : tensor[batch_size, 1, hidden_size]
            # decoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
            
            decoder_outputs[:, t, :] = decoder_outputs_single.squeeze(1)
        
        head_outputs = self.head(decoder_outputs)
        # head_outputs : tensor[batch_size, decoder_max_sequence_len, decoder_vocab_size]
        
        return head_outputs
    
    def greedy_search(self, encoder_input_ids, encoder_lengths, max_sequence_len=50):
        # encoder_input_ids : tensor[batch_size, max_seq_len]
        
        batch_size = encoder_input_ids.shape[0]
        
        # 1. encoding
        encoder_input_embs = self.encoder_embedding(encoder_input_ids)
        # encoder_input_embs : tensor[batch_size, max_seq_len, encoder_embedding_dim]
        
        encoder_outputs, encoder_hidden_states = self.encoder(encoder_input_embs, encoder_lengths)
        # encoder_outputs : tensor[batch_size, max_seq_len, hidden_size]
        # encoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
        
        # 2. decoding
        generated_sequences = torch.full(
            (batch_size, max_sequence_len),
            fill_value=self.config.decoder.pad_token_id
        ) # generated_sequences : tensor[batch_size, max_sequence_len]
        
        decoder_input_ids_single = torch.LongTensor([self.config.decoder.bos_token_id] * batch_size).view(batch_size, 1)
        decoder_hidden_states = encoder_hidden_states
        # decoder_input_ids_single : tensor[batch_size, 1]
        # decoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
        
        is_complete = torch.BoolTensor([False] * batch_size)
        # is_complete : tensor[batch_size]
        
        for t in range(max_sequence_len):
            decoder_input_embs_single = self.decoder_embedding(decoder_input_ids_single)
            # decoder_input_embs_single : tensor[batch_size, 1, decoder_embedding_dim]
            
            decoder_outputs_single, decoder_hidden_states = self.decoder(
                decoder_input_embs_single,
                decoder_hidden_states
            )
            # decoder_outputs_single : tensor[batch_size, 1, hidden_size]
            # decoder_hidden_states : tensor[num_layers, batch_size, hidden_size]
            
            head_outputs_single = self.head(decoder_outputs_single)
            # head_outputs_single : tensor[batch_size, 1, decoder_vocab_size]
            
            generated_tokens = head_outputs_single.argmax(dim=-1).squeeze(1)
            # generated_tokens : tensor[batch_size]
            
            # 생성이 끝난 sequence는 무시
            generated_tokens[is_complete] = self.config.decoder.pad_token_id
            
            # generated_sequences에 추가
            generated_sequences[:, t] = generated_tokens
            
            # 생성이 끝난 sequence 식별
            is_complete[generated_tokens == self.config.decoder.eos_token_id] = True
            if is_complete.sum() == batch_size:
                break
            
            # 다음 iteration을 위한 준비
            decoder_input_ids_single = generated_tokens.unsqueeze(1)
            # decoder_input_ids_single : tensor[batch_size, 1]
            
        return generated_sequences
    
    def save(self, eval_result, save_dir=None, prefix="seq2seq"):
        if save_dir is None:
            if self.save_dir is None:
                save_dir = os.path.join(os.getcwd(), f'{prefix}-{datetime.now().strftime("%y%m%d-%H%M%S")}')
                self.save_dir = save_dir
            else:
                save_dir = self.save_dir
        else:
            self.save_dir = save_dir
            
        if not os.path.isdir(save_dir):
            os.makedirs(save_dir)
        
        # model save
        torch.save(self.state_dict(), os.path.join(save_dir, "model.pt"))
        
        # model config save
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(self.config.config_dict, f, ensure_ascii=False, indent=2)
        
        # evaluation result save
        with open(os.path.join(save_dir, "eval_result.json"), "w") as f:
            json.dump(eval_result, f, ensure_ascii=False, indent=2)
            
        return save_dir
            
    def load(self, load_dir):
        self.load_state_dict(torch.load(os.path.join(load_dir, "model.pt")))
```

- line 72 : 교사 강요(teacher forcing)
- line 149-173 : 모델 저장을 위한 코드
    - 주어진 폴더 경로(아무런 값이 주어지지 않았다면 현재 시간으로부터 새로운 폴더를 만듦)에 모델의 가중치 정보를 저장한 `model.pt`, 모델의 하이퍼파라미터 정보를 저장한 `config.json`, 저장이 진행된 시점에서의 train/evaluation 결과를 저장한 `eval_result.json` 파일을 만든다.
- line 175-176 : 모델 로딩을 위한 코드

### dataset, dataloader

- Jungyeul Park 님이 공개한 "한국어-영어 뉴스 말뭉치" 데이터셋 이용

  > [!info] 데이터셋 정보
  > **한국어-영어 뉴스 말뭉치**
  >
  > - author: Jungyeul Park
  > - repository: <https://github.com/jungyeul/korean-parallel-corpora>
  > - size:
  > - train: 94,123 pairs
  > - dev: 1,000 pairs
  > - test: 2,000 pairs

- Korpora
    - Korpora를 이용하면 "한국어-영어 뉴스 말뭉치" 데이터셋을 간단히 받을 수 있음
    - Korpora의 자세한 사용법은 [documentation](https://ko-nlp.github.io/Korpora/) 참고

```python title:"Dataset, Dataloader" linenos
class KoEnDataset(Dataset):
    def __init__(self, split):
        assert split in ["train", "dev", "test"], f"Invalid param <split> : {split}"
        
        full_dataset = Korpora.load("korean_parallel_koen_news", root_dir=".")
        if split == "train":
            self.dataset = full_dataset.train
        elif split == "dev":
            self.dataset = full_dataset.dev
        else: # test
            self.dataset = full_dataset.test
    
    def __len__(self):
        return len(self.dataset) 
    
    def __getitem__(self, idx):
        kor = self.dataset[idx].text
        eng = self.dataset[idx].pair
        return kor, eng

def collate_fn(pairs):
    batch_size = len(pairs)
    
    kors = [x[0] for x in pairs]
    engs = [x[1] for x in pairs]
    
    # tokenize
    kors_input_ids = [
        torch.LongTensor(
            [kor_tokenizer.bos_token_id] + x + [kor_tokenizer.eos_token_id]
        ) for x in kor_tokenizer(kors, add_special_tokens=False).input_ids
    ]
    
    kors_lengths = [x.shape[0] for x in kors_input_ids]
    
    engs_input_ids = [
        torch.LongTensor(
            [eng_tokenizer.bos_token_id] + x + [eng_tokenizer.eos_token_id]
        ) for x in eng_tokenizer(engs, add_special_tokens=False).input_ids
    ]
    
    engs_lengths = [x.shape[0] for x in engs_input_ids]
    
    # sort
    sorted_order = [
        (original_idx, sorted_idx) for sorted_idx, original_idx
        in enumerate(
            sorted(range(len(kors_lengths)), key=lambda i: kors_lengths[i], reverse=True)
        )
    ]
    
    kors_input_ids = [kors_input_ids[i] for i, _ in sorted_order]
    kors_lengths = [kors_lengths[i] for i, _ in sorted_order]
    engs_input_ids = [engs_input_ids[i] for i, _ in sorted_order]
    engs_lengths = [engs_lengths[i] for i, _ in sorted_order]
    
    # padding
    kors_input_ids = pad_sequence(
        kors_input_ids,
        batch_first=True,
        padding_value=kor_tokenizer.pad_token_id
    )
    
    engs_input_ids = pad_sequence(
        engs_input_ids,
        batch_first=True,
        padding_value=eng_tokenizer.pad_token_id
    )
    
    # labels
    labels = engs_input_ids.clone()

    indices = torch.LongTensor(engs_lengths) - 1 # index of <eos> tokens
    labels[torch.arange(batch_size), indices] = eng_tokenizer.pad_token_id # substitute <eos> tokens to <pad> tokens

    labels = labels[:, 1:] # remove <sos> tokens
    # labels : tensor[batch_size, max_sequence_len - 1]

    labels = labels.reshape(-1)
    # labels : tensor[batch_size * (max_sequence_len - 1)]

    return kors_input_ids, kors_lengths, engs_input_ids, engs_lengths, labels

num_workers = 8
batch_size = 128

train_dataset = KoEnDataset("train")
dev_dataset = KoEnDataset("dev")
test_dataset = KoEnDataset("test")

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)
```

- line 28-32, 36-40 : tokenization
    - 각 시퀸스의 앞과 뒤에 `<sos>`, `<eos>`을 붙임
    - tokenizer가 자동으로 붙여주는 special token들(ex. `[CLS]`, `[SEP]`)은 본 예제에서는 필요없기에 `add_special_tokens`에 `False`를 줌
- line 44-55 : sorting
    - packing을 위해 한국어 시퀸스(인코더 입력 시퀸스)의 길이 순으로 내림차순 정렬
    - 영어 시퀸스(디코더 입력 시퀸스)는 길이 순으로 정렬되지 않겠지만, 디코더에서는 packing을 사용하지 않기 때문에 상관 없다.
- line 70-80 : label 만들기
    - 디코더는 `<eos>` 토큰이 입력되기 직전 시점의 토큰을 입력받아 `<eos>` 토큰을 출력함(정확히는, 그러려고 노력함). 즉 `<eos>` 토큰이 입력되었을 때는 아무런 의미없는 토큰이 출력됨. 이에 label을 만들 때는 `<eos>` 토큰을 모두 `<pad>` 토큰으로 대체함. `<pad>` 토큰이 위치한 인덱스는 loss 계산 과정에서 무시할 수 있음(아래 [[Knowledge Base/nlp/seq2seq#training\|#training]] 문단 참조).
    - 디코더는 `<sos>` 토큰을 입력받아 그 다음 토큰을 연쇄적으로 생성함. 즉 디코더의 출력에는 `<sos>` 토큰이 포함되어 있지 않음. 이에 label을 만들 때는 디코더 입력 시퀸스에서 첫 번째 토큰(`<sos>`)을 제거함.

### training

- 모델 선언

    ```python title:"model" linenos
    model = Seq2Seq(
        encoder_tokenizer=kor_tokenizer,
        decoder_tokenizer=eng_tokenizer,
        encoder_embedding_dim=256,
        decoder_embedding_dim=256,
        encoder_bidirectional=True,
        hidden_size=512,
        num_layers=2,
        dropout_rate=0.5
    )
    ```

- loss function, optimizer 선언

    ```python title:"criterion, optimizer" linenos
    criterion = nn.NLLLoss(ignore_index=eng_tokenizer.pad_token_id)
    optimizer = optim.Adam(model.parameters())
    ```

    - line 1 : loss function 설정
        - `LMHead`에서 log softmax된 결과를 반환하므로 `nn.NLLLoss`를 사용 (만약 `LMHead`에서 logit을 반환했다면 `nn.CrossEntropyLoss`를 사용해야 함)
        - `ignore_index` 인자를 주어 labels에 `<pad>` 토큰이 있는 인덱스는 loss를 계산하지 않음
- training 함수 선언

    ```python title:"train_epoch" linenos
    def train_epoch(model, iterator, optimizer, criterion, device, clip=1):
        model = model.to(device)
        
        model.train()
            
        epoch_loss = 0
        
        for i, batch in enumerate(tqdm(iterator)):
            encoder_input_ids, encoder_lengths, decoder_input_ids, decoder_lengths, labels = batch
            # encoder_input_ids : tensor[batch_size, encoder_max_sequence_len]
            # encoder_lengths : list
            # decoder_input_ids : tensor[batch_size, decoder_max_sequence_len]
            # decoder_lengths : list
            
            encoder_input_ids, decoder_input_ids, labels = encoder_input_ids.to(device), decoder_input_ids.to(device), labels.to(device)
            
            optimizer.zero_grad()
            
            output = model(encoder_input_ids, encoder_lengths, decoder_input_ids, decoder_lengths)
            # output : tensor[batch_size, decoder_max_sequence_len, decoder_vocab_size]
            
            # labels와 같은 크기로 맞춰줌
            output = output[:, :-1, :]
            # output : tensor[batch_size, decoder_max_sequence_len - 1, decoder_vocab_size]
            
            output = output.reshape(-1, len(eng_tokenizer))
            # output : tensor[batch_size * (decoder_max_sequence_len - 1), decoder_vocab_size]
            
            # loss 계산
            loss = criterion(output, labels)
            
            # 역전파
            loss.backward()
            
            clip_grad_norm_(model.parameters(), clip)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            
        return epoch_loss / len(iterator)
    ```

    - line 23 : 마지막 시점의 결과 제거
        - 가장 긴 시퀸스의 경우 `<eos>` 토큰이 입력됐을 때의 결과이고, 나머지 시퀸스에서는 `<pad>` 토큰이 입력됐을 때의 결과 → 어차피 무의미한 값이 담겨 있음
        - label과의 개수를 맞춰주기 위함 (label은 반대로 최초 시점의 토큰(`<sos>` 토큰)을 제거)
    - line 35 : gradient clipping
        - 학습의 안정성을 위해 gradient clipping 수행

    ```python title:"eval_epoch" linenos
    def eval_epoch(model, iterator, criterion, device):
        model = model.to(device)
        
        model.eval()
        
        epoch_loss = 0
        
        with torch.no_grad():
            for i, batch in enumerate(tqdm(iterator)):
                encoder_input_ids, encoder_lengths, decoder_input_ids, decoder_lengths, labels = batch
                # encoder_input_ids : tensor[batch_size, encoder_max_sequence_len]
                # encoder_lengths : list
                # decoder_input_ids : tensor[batch_size, decoder_max_sequence_len]
                # decoder_lengths : list
                # labels : tensor[batch_size * (decoder_max_sequence_len - 1)]
    
                encoder_input_ids, decoder_input_ids, labels = encoder_input_ids.to(device), decoder_input_ids.to(device), labels.to(device)
    
                output = model(encoder_input_ids, encoder_lengths, decoder_input_ids, decoder_lengths)
                # output : tensor[batch_size, decoder_max_sequence_len, decoder_vocab_size]
                
                # labels와 같은 크기로 맞춰줌
                output = output[:, :-1, :]
                # output : tensor[batch_size, decoder_max_sequence_len - 1, decoder_vocab_size]
                
                output = output.reshape(-1, len(eng_tokenizer))
                # output : tensor[batch_size * (decoder_max_sequence_len - 1), decoder_vocab_size]
    
                # loss 계산
                loss = criterion(output, labels)
    
                epoch_loss += loss.item()
                
        return epoch_loss / len(iterator)
    ```

위 코드를 이용해 다음과 같이 학습을 진행하였다.

```python title:"training" linenos
epochs = 10
device = "cuda" if torch.cuda.is_available() else "cpu"

best_valid_loss = float('inf')
for epoch in tqdm(range(epochs)):
    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)
    valid_loss = eval_epoch(model, dev_dataloader, criterion, device)
    
    is_updated = False
    if valid_loss < best_valid_loss:
        is_updated = True
        best_valid_loss = valid_loss
        model.save({
            "train": {
                "loss": train_loss,
                "ppl": math.exp(train_loss)
            },
            "valid": {
                "loss": valid_loss,
                "ppl": math.exp(valid_loss)
            }
        })
    
    print(f"Epoch {epoch + 1}{'*' if is_updated else ''} :")
    print(f'  Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'   Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
```
