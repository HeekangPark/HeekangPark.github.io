
# 예제

pytorch를 이용해 영어를 한국어로 번역하는 Seq2Seq 모델을 직접 만들어 보자.

## 사전준비

다음 패키지들을 설치한다.

- pytorch==1.13.0
- transformers==4.24.0
- korpora==0.2.0

다음 항목들을 import한다.

```python
from Korpora import Korpora
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
import torch.nn as nn
from transformers import AutoTokenizer
```

## 데이터

영어-한국어 번역기를 만드는데 사용할 데이터는 Jungyeul Park 님이 만드신 한국어-영어 뉴스 말뭉치이다.

::: info 데이터 정보

- author: Jungyeul Park
- repository: <https://github.com/jungyeul/korean-parallel-corpora>
- size:
  - train: 94,123 pairs
  - dev: 1,000 pairs
  - test: 2,000 pairs

:::

Korpora를 이용하면 이 말뭉치를 손쉽게 받을 수 있다(Korpora에 대한 자세한 사용법은 [다음 문서](https://ko-nlp.github.io/Korpora/)를 참고하자). 이를 이용해 train, dev, test dataset 및 dataloader를 다음과 같이 만들 수 있다.

```python:line-numbers
# Dataset, Dataloader

class KoEnDataset(Dataset):
    def __init__(self, split):
        assert split in ["train", "dev", "test"], f"Invalid param <split> : {split}"
        
        full_dataset = Korpora.load("korean_parallel_koen_news", root_dir=".")
        if split == "train":
            self.dataset = full_dataset.train
        elif split == "dev":
            self.dataset = full_dataset.dev
        else: # test
            self.dataset = full_dataset.test
    
    def __len__(self):
        return len(self.dataset) 
    
    def __getitem__(self, idx):
        return self.dataset[idx].text, self.dataset[idx].pair

train_dataset = KoEnDataset("train")
dev_dataset = KoEnDataset("dev")
test_dataset = KoEnDataset("test")

batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

## Seq2Seq 모델 구현

우선 인코더는 다음과 같은 역할을 해야 한다.

1. 원문 문자열(영어)을 받아들여,
2. tokenization : tokenizer를 이용해 tokenization을 수행하고,
3. embedding : 각 token을 임베딩 벡터로 변환하고,
4. feeding : 하나씩 RNN 모듈에 넣어 은닉 상태를 업데이트한다.
5. 마지막 token까지 모두 입력한 후, 최종 은닉 상태를 반환한다.

2번 tokenization 과정에서는 spacy, nltk 등의 rule-based tokenizer를 사용해도 되지만, 우린 huggingface의 transformers 라이브러리에서 제공하는 `bert-base-uncased` tokenizer를 사용하자. 이 tokenizer는 데이터 기반 tokenizer(WordPiece)로, BERT가 학습할 때 사용했던 tokenizer이다. 또 3번 embedding 과정에서는 one-hot encoding을 사용한다.

```python:line-numbers
# Encoder

class Encoder(nn.Module):
    def __init__(self, hidden_size=256, num_layers=2, dropout=0.5):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.rnn = nn.LSTM(
            input_size=self.tokenizer.vocab_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
    
    def one_hot_encoding(self, input_ids):
        # sequence 하나를 one hot embedding으로 변환해주는 함수
        # input_ids : LongTensor(seq_len)
        # return : tensor(seq_len, vocab_size)
        return torch.zeros(len(input_ids), self.tokenizer.vocab_size).scatter(1, input_ids.unsqueeze(1), 1)
    
    def forward(self, sentences):
        # 만약 단일 문장이 입력되었다면, 문장을 리스트로 감싸 batch 입력으로 바꿔준다.
        if isinstance(sentences, str):
            sentences = [sentences]
        
        # huggingface tokenizer를 이용해 tokenize
        input_ids = self.tokenizer(
            sentences,
            return_attention_mask=False,
            return_token_type_ids=False
        ).input_ids
        
        # one-hot encoding
        input_ids = [
            self.one_hot_encoding(torch.LongTensor(x)) for x in input_ids
        ]
        
        # 길이 내림차순으로 정렬
        sorted_order = [
            (original_idx, sorted_idx)
            for sorted_idx, original_idx
            in enumerate(
                sorted(range(len(input_ids)), key=lambda i: input_ids[i].shape[0], reverse=True)
            )
        ]
        input_ids = [input_ids[i] for i, _ in sorted_order]
        lengths = [x.shape[0] for x in input_ids]
        
        # padded sequence 생성
        padded_input_ids = pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id
        )
        
        # packed sequence 변환
        packed_input_ids = pack_padded_sequence(
            padded_input_ids,
            lengths=lengths,
            batch_first=True
        )
        
        outputs, (hidden_states, cell_states) = self.rnn(packed_input_ids)
        
        # 원래 순서대로 복구
        sorted_order = sorted(sorted_order, key=lambda x: x[0])
        hidden_states = torch.stack(
            [hidden_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        cell_states = torch.stack(
            [cell_states[:, i, :] for i in [x[1] for x in sorted_order]],
            dim=1
        )
        
        return hidden_states, cell_states
```
