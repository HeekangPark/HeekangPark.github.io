---
title: "지도학습 (Supervised Learning)"
order: 3
date_created: "2020-05-01"
date_modified: "2022-05-22"
---

# 지도학습의 과정

일반적으로 지도학습은 다음과 같은 순서로 진행된다.

1. 데이터 수집 및 정제
   
   학습을 위해 라벨링된 데이터를 (최대한 많이) 수집한 후, 목적에 따라 적절히 가공[^1]한다.

2. 모델 선정
   
   사용할 기계학습 모델을 선정한다. 모델이란 입력값을 받아 출력값을 내놓는 일종의 함수로,[^2] 로지스틱 회귀(Logistic Regression), SVM(Support Vector Machine), 의사결정나무(Decision Tree), 랜덤 포레스트(Random Forest), NN(Nearest Neighbor), 인공신경망(Neural Network), 다중 레이어 퍼셉트론(Multilayer Perceptron, MLP) 등 다양한 종류가 있다. 해결하고자 하는 문제 및 데이터의 종류, 성격 등을 보고 적절한 모델을 선정하면 된다.

3. 모델 학습
   
   데이터에 맞게 모델을 학습시킨다.

4. 모델 사용

   학습된 모델을 이용해 해결하고자 했던 문제를 푼다.

[^1]: 노이즈 제거, 결측치 제거, 특징점(feature) 추출 등
[^2]: 그래서 모델을 가설 함수(hypothesis function)이라고도 부른다. 과학적 방법론(데이터 수집 - 가설 수립 - 가설 검증 실험 설계 - 가설 검증 - 가설 폐기 혹은 새로운 검증 실험 설계)에 익숙한 사람이라면 왜 가설 함수라 불리는지 이해할 수 있을 것이다.

# 데이터 셋, 샘플, 예제, 레이블

{% include caption-img.html src="dataset-example-label.png" title="Fig.01 지도학습을 위한 데이터 셋" description="지도학습을 위한 데이터 셋은 예제-레이블 쌍으로 이루어져 있다. 위 데이터 셋의 크기는 $n$이고 데이터 셋의 차원은 $d$이다." %}

(학습을 위해) 데이터를 모아놓은 것을 **데이터 셋(dataset)**이라 한다.

지도학습을 위한 데이터 셋은 일반적으로 설명 변수들($x\_{ij}$)과 반응 변수 $y\_i$의 쌍으로 이루어져 있다. 이때 각 설명 변수들의 벡터 $\mathbf{x}\_i = [x\_{i1},\,x\_{i2},\,\cdots,\,x\_{id}]$를 (데이터셋의 $i$번째) **예제(Example)**라 하고, 각 반응 변수 $y\_i$를 예제 $\mathbf{x}\_i$의 **레이블, 라벨(Label)** 혹은 **정답, 참값(Ground Truth)**이라 한다. 그리고 설명 변수들과 반응 변수들의 각 쌍을 (데이터셋의 $i$번째) **샘플(Sample)**이라 한다. 즉 지도학습을 위한 데이터 셋은 샘플(예제-레이블 쌍)이 여러 개 모여있는 것이라 이해할 수 있다.[^30]

[^30]: 사실 샘플(sample)과 예제(example)는 엄밀하게 구분하지 않는 용어로 보인다. 당장 비지도학습(unsupervised learning)을 위한 데이터 셋의 경우 레이블이 없으므로 샘플이 예제와 같아진다. 따라서 어느 정도 문맥을 파악해 의미를 이해하는 것이 정확하겠다.

샘플의 개수를 **데이터 셋의 크기(size of dataset)**라 하고, 예제을 이루는 설명 변수들의 개수를 데이터 셋의 **차원(dimension)** 혹은 **특성의 크기(feature size)**라 한다.

# 파라미터 최적화(Parameter Optimization)

위 과정 중 3. 모델 학습 과정에 대해서 조금 더 알아보자.

1\. 데이터 수집 및 정제 과정을 거쳐 예제 $\mathbf{x}$와 레이블 $y$의 쌍으로 이루어진 데이터 셋을 구성하고, 2. 모델 선정 과정에서 모델 $f$를 선정해 학습시키기로 했다고 하자.

이때, 모델이 잘 학습되었다면 $f(\mathbf{x})$와 $y$의 값이 비슷할 것이다. 다시말해, 잘 학습된 모델에서는 $f(\mathbf{x})$와 $y$ 간의 오차가 작을 것이다. 즉, **모델을 학습시킨다는 것은 $f(\mathbf{x})$와 $y$ 간의 오차 $e = \|y - f(\mathbf{x}) \|$를 작게 만드는 것이다**.

이제 우리는 "모델을 학습시킨다"는 추상적인 목표 대신, "오차를 최소화한다"라는 명확하고 구체적인 목표를 가지게 되었다. 일반적으로 지도학습에서 사용하는 모델은 파라미터를 가지는데,[^3] 파라미터를 적절히 조절해 오차 $e$를 최소화하는 문제는 전형적인 최적화 문제(Optimization Problem)이다. 이 때문에 모델을 학습시키는 과정을 파라미터 최적화(Parameter Optimization)라고도 한다.

[^3]: 이렇게 파라미터를 가지는 모델을 parametric model이라 한다. 다만 모든 기계학습 모델이 parametric model인 것은 아니다. 파라미터가 없는 모델을 non-parametric model이라 한다. non-parameteric model은 파라미터 최적화가 아닌 다른 방법으로 학습을 진행한다. 이 컬렉션에서는 parametric model만 다루도록 하겠다. non-parametric model을 학습시키는 방법은 다른 글들을 참조하자.

## 오차 함수 (Error Function)

그렇다면 그 오차는 어떻게 구할까? 지도학습에서 오차는 오차 함수(Error Function)[^4]라는 함수를 이용하여 구한다. 오차 함수는 모델 $f$의 파라미터 $\mathbf{w}$에 대한 함수이다.

[^4]: 비용 함수(Cost Function), 손실 함수(Loss Function), 목표 함수(Objective Function)라고도 불린다.

오차 함수에는 다양한 종류가 있는데, 풀고자 하는 문제의 종류에 따라 적절한 오차 함수를 선택하면 된다. 예를 들어 회귀 문제에서는 [평균제곱오차(MSE, Mean of Squared Error)](/swe3050/04-error-functions#kramdown_평균제곱오차-mse-mean-of-squared-error)를, 분류 문제에서는 [크로스 엔트로피(Cross Entropy)](/swe3050/04-error-functions#kramdown_크로스-엔트로피-cross-entropy)를 많이 사용한다.

각 오차 함수에 대한 자세한 내용은 [해당 문서](/swe3050/04-error-functions)를 참조하기 바란다.

## 최적화 방법

오차 함수를 최소값으로 만드는 파라미터 $\mathbf{w}$를 찾는 문제를 푸는 방법은 크게 두 가지로 분류할 수 있다.

### 해석적 풀이법 (Analytic Solution)

해석적 풀이법은 오차 함수가 언제 최소값을 갖는지 해석적인 방법으로 푸는 것이다.

해석적으로 어떤 함수가 언제 최소값을 가지는지 어떻게 알 수 있을까? 예를 들어, 다음 이차함수를 생각해 보자.

$$f(x) = x^2 + 2x + 3 $$

$f$는 볼록함수(convex function)이므로, 최소값은 이 함수의 도함수(derivative)가 0이 되는 점에서 발생한다. 즉,

$$\frac{df}{dx} = 2x + 2 = 0$$

을 만족시키는 $x = -1$에서 최소값을 가짐을 알 수 있다.

어떤 임의의 볼록함수(convex function)가 주어졌을 때, 이 함수의 최소값(global minimum)은 함수의 [그라디언트(gradient)](/calculus/02-gradient)의 모든 원소를 0으로 만드는 점이다. 예를 들어, 함수

$$f(x, y) = x^2 +xy - 2x - y^2$$

에 대해, 이 함수의 최솟값은 함수의 그라디언트

$$\nabla f(x, y) = [2x + y - 2, x - 2y] $$

의 각 원소가 0이 되는 점이다. 이로부터 연립방정식을 세우면 다음과 같다.

$$ \begin{cases}
2x + y - 2 = 0\\
x - 2y = 0\\
\end{cases} $$

연립방정식을 풀면

$$ x = \frac {4}{5},\quad y = \frac{2}{5} $$

이므로, 우리는 함수 $f(x, y) = x^2 +xy - 2x - y^2$가 $\left(\displaystyle\frac{4}{5},\,\frac{2}{5}\right)$에서 최소값을 가짐을 알 수 있다.

해석적 풀이법의 해는 닫힌 형식(Closed Form)으로 나오므로, 매우 적은 연산 비용만으로 최적의 파라미터를 구할 수 있다는 장점이 있다. 하지만 다음의 상황에서는 적용하기 어렵다는 단점이 있다.

- 데이터의 차원이 너무 크거나, 데이터의 수가 너무 많은 경우 : 일반적으로 해석적 풀이법은 행렬 연산을 통해 해를 찾게 되는데, 만약 데이터의 차원이 너무 크거나 그 수가 너무 많으면 전체 데이터를 행렬의 형태로 메모리에 올리는 게 불가능해진다.
- 수학적으로 최소점이 존재하지 않거나, 계산 불가능한 경우 : 오차 함수가 볼록 함수가 아니거나, 미분 불가능한 등의 이유로 수학적으로 최소점을 계산할수 없는 경우 해석적 풀이법을 사용할 수 없다.

사실, 대부분의 지도학습 문제는 해석적 풀이를 적용하기 힘든 문제들이다. 이 경우, 아래 수치 계산법이 좋은 대안이 될 수 있다.

## 수치 계산법 (Numerical Solution)

한 번에 해를 구하는 해석적 풀이법과는 다르게 수치 계산법에서는 여러 번 연산을 반복하면서 점점 더 품질이 좋은 근사해를 구한다.

수치 계산법에는 다양한 방법이 있다.

- 경사 하강법(GD, Gradient Descent Method)
- 뉴턴법(Newton Method)
- 가우스-뉴턴법(Gauss-Newton Method)
- Levenberg-Marquardt Method
- BFGS
- Conjugate Gradient Method
- 등등...

이 중 경사 하강법은 방법 특성상 항상 최소값에 도달한다고 확신할 수는 없으나, 속도가 빠르고 연산량이 적어 현재 널리 쓰이고 있다.

### 경사 하강법 (Gradient Descent Method)

경사 하강법에서는 그라디언트의 반대 방향으로 가면 함수의 극소점을 찾을 수 있는 그라디언트의 성질을 사용해 함수의 최솟값을 찾는다.

일반적으로 경사 하강법은 다음과 같은 순서로 진행된다.

1. 파라미터 $\mathbf{w}$를 무작위 값으로 초기화한다.
2. 오차 함수 $\mathbb{E}$에 대해, 다음 식으로 파라미터를 업데이트한다 : $\mathbf{w}\_{new} = \mathbf{w}\_{old} - \eta \nabla \mathbb{E}(\mathbf{w}\_{old})$
3. 2를 필요한 만큼 반복한다.

오차 함수 $\mathbb{E}(\mathbf{w}\_{old})$의 값을 최소화할 수 있도록 오차 함수의 그라디언트 $\nabla \mathbb{E}(\mathbf{w}\_{old})$의 반대 방향(-)으로 $\mathbf{w}$를 점진적으로 움직인다.

이때 $\eta$는 그리스 문자 에타(eta)로서 학습률(learning rate)을 나타낸다. 학습률은 한 번에 파라미터를 얼마나 업데이트할 지를 나타내는 하이퍼파라미터이다.

학습률이 크면...

- 한 번의 업데이트로 파라미터가 크게 바뀐다.
- 최소값으로 다가가는 속도가 빠르다. 최소값에 더 적은 시간으로 도달할 수 있다.
- 최소값이 아닌 극소값(local minimum)으로 빠지더라도 탈출할 가능성이 생긴다. 
- 그러나, 최소값을 지나칠 수도 있다.

학습률이 작으면...
- 한 번의 업데이트로 파라미터가 작게 바뀐다.
- 최소값으로 다가가는 속도가 느리다. 최소값 도달에 더 많은 시간이 걸린다.
- 최소값이 아닌 극소값에 빠질 경우 탈출이 어렵다.

경사 하강법이 해석적 풀이법과 동일하게 그라디언트를 사용하기에 헷갈릴 수 있는데, 해석적 풀이법은 연립방정식을 풀어 그라디언트의 각 원소를 0으로 만드는 값(= 해)을 한 번에 찾는 방법이었다면, 경사 하강법은 그라디언트의 '방향'으로 조금씩 이동하면서 점진적으로 근사해를 찾아가는 방법이다.
